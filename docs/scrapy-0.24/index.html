

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Scrapy 0.24.0 文档</title>
  


  
  
    

  

  
  

  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  
    <link rel="top" title="None" href="index.html#document-index"/>
 
<!-- RTD Extra Head -->



<!-- 
Read the Docs is acting as the canonical URL for your project. 
If you want to change it, more info is available in our docs:
  http://docs.readthedocs.org/en/latest/canonical.html
-->

<script type="text/javascript">
  // This is included here because other places don't have access to the pagename variable.
  var READTHEDOCS_DATA = {
    project: "scrapy-chs",
    version: "0.24",
    language: "zh_CN",
    page: "index",
    theme: "sphinx_rtd_theme",
    docroot: "/",
    source_suffix: ".rst",
    api_host: "https://readthedocs.org"
  }
  // Old variables
  var doc_version = "0.24";
  var doc_slug = "scrapy-chs";
  var page_name = "index";
  var html_theme = "sphinx_rtd_theme";
</script>
<!-- RTD Analytics Code -->
<!-- Included in the header because you don't have a footer block. -->
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-17997319-1']);
  _gaq.push(['_trackPageview']);

  // User Analytics Code
  _gaq.push(['user._setAccount', 'None']);
  _gaq.push(['user._trackPageview']);
  // End User Analytics Code


  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
<!-- end RTD Analytics Code -->
<!-- end RTD <extrahead> -->


  

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        <a href="index.html#document-index" class="fa fa-home"> Scrapy</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
        
            <ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-intro/overview">初窥Scrapy</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/overview#id1">选择一个网站</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/overview#intro-overview-item">定义您想抓取的数据</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/overview#spider">编写提取数据的Spider</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/overview#id3">执行spider，获取数据</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/overview#id4">查看提取到的数据</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/overview#topics-whatelse">还有什么？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/overview#id6">接下来</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-intro/install">安装指南</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/install#id2">前期准备</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/install#scrapy">安装Scrapy</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/install#intro-install-platform-notes">平台安装指南</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-intro/tutorial">Scrapy入门教程</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/tutorial#id2">创建项目</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/tutorial#item">定义Item</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/tutorial#spider">编写第一个爬虫(Spider)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/tutorial#id9">保存爬取到的数据</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/tutorial#id10">下一步</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-intro/examples">例子</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/commands">命令行工具(Command line tools)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/commands#scrapy">默认的Scrapy项目结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/commands#id1">使用 <tt class="docutils literal"><span class="pre">scrapy</span></tt> 工具</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/commands#tool-commands">可用的工具命令(tool commands)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/commands#id4">自定义项目命令</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/items">Items</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/items#item">声明Item</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/items#item-item-fields">Item字段(Item Fields)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/items#id1">与Item配合</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/items#id7">扩展Item</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/items#id8">Item对象</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/items#field">字段(Field)对象</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/spiders">Spiders</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/spiders#spider">Spider参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/spiders#topics-spiders-ref">内置Spider参考手册</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/selectors">选择器(Selectors)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/selectors#id1">使用选择器(selectors)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/selectors#module-scrapy.selector">内建选择器的参考</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/loaders">Item Loaders</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/loaders#using-item-loaders-to-populate-items">Using Item Loaders to populate items</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/loaders#input-and-output-processors">Input and Output processors</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/loaders#declaring-item-loaders">Declaring Item Loaders</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/loaders#declaring-input-and-output-processors">Declaring Input and Output Processors</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/loaders#item-loader-context">Item Loader Context</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/loaders#itemloader-objects">ItemLoader objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/loaders#reusing-and-extending-item-loaders">Reusing and extending Item Loaders</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/loaders#module-scrapy.contrib.loader.processor">Available built-in processors</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/shell">Scrapy终端(Scrapy shell)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/shell#id1">启动终端</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/shell#id2">使用终端</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/shell#shell-session">终端会话(shell session)样例</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/shell#spidershellresponse">在spider中启动shell来查看response</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/item-pipeline">Item Pipeline</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/item-pipeline#id1">编写你自己的item pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/item-pipeline#id2">Item pipeline 样例</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/item-pipeline#id4">启用一个Item Pipeline组件</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/feed-exports">Feed exports</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/feed-exports#serialization-formats">序列化方式(Serialization formats)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/feed-exports#storages">存储(Storages)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/feed-exports#uri">存储URI参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/feed-exports#storage-backends">存储端(Storage backends)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/feed-exports#settings">设定(Settings)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/link-extractors">Link Extractors</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/link-extractors#module-scrapy.contrib.linkextractors">内置Link Extractor 参考</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/logging">Logging</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/logging#log-levels">Log levels</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/logging#log">如何设置log级别</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/logging#log-messages">如何记录信息(log messages)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/logging#spiderlog-logging-from-spiders">在Spider中添加log(Logging from Spiders)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/logging#module-scrapy.log">scrapy.log模块</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/logging#id1">Logging设置</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/stats">数据收集(Stats Collection)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/stats#topics-stats-usecases">常见数据收集器使用方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/stats#id2">可用的数据收集器</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/email">发送email</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/email#id1">简单例子</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/email#mailsender">MailSender类参考手册</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/email#mail">Mail设置</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/telnetconsole">Telnet终端(Telnet Console)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/telnetconsole#telnet">如何访问telnet终端</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/telnetconsole#id1">telnet终端中可用的变量</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/telnetconsole#telnet-console-usage-examples">Telnet console usage examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/telnetconsole#id3">Telnet终端信号</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/telnetconsole#id4">Telnet设定</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/webservice">Web Service</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/webservice#web-service-resources">Web Service资源(resources)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/webservice#web">Web服务设置</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/webservice#web-resource">编写web服务资源(resource)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/webservice#id2">web服务资源例子</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/webservice#example-of-web-service-client">Example of web service client</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-faq">常见问题(FAQ)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#scrapybeautifulsouplxml">Scrapy相BeautifulSoup或lxml比较,如何呢？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#scrapypython">Scrapy支持那些Python版本？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#scrapypython-3">Scrapy支持Python 3么？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#scrapydjango-x">Scrapy是否从Django中&#8221;剽窃&#8221;了X呢？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#scrapyhttp">Scrapy支持HTTP代理么？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#item">如何爬取属性在不同页面的item呢？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#scrapy-importerror-nomodule-named-win32api">Scrapy退出，ImportError: Nomodule named win32api</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#spider">我要如何在spider里模拟用户登录呢?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#scrapy">Scrapy是以广度优先还是深度优先进行爬取的呢？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#id3">我的Scrapy爬虫有内存泄露，怎么办?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#id4">如何让Scrapy减少内存消耗?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#spiderhttp">我能在spider中使用基本HTTP认证么？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#id5">为什么Scrapy下载了英文的页面，而不是我的本国语言？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#id6">我能在哪里找到Scrapy项目的例子？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#scrapy-spider">我能在不创建Scrapy项目的情况下运行一个爬虫(spider)么？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#filtered-offsite-request">我收到了 &#8220;Filtered offsite request&#8221; 消息。如何修复？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#id7">发布Scrapy爬虫到生产环境的推荐方式？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#large-exports-json">我能对大数据(large exports)使用JSON么？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#signal-handler-twisted">我能在信号处理器(signal handler)中返回(Twisted)引用么？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#reponse999">reponse返回的状态值999代表了什么?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#spider-pdb-set-trace">我能在spider中调用 <tt class="docutils literal"><span class="pre">pdb.set_trace()</span></tt> 来调试么？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#item-dump-json-csv-xml">将所有爬取到的item转存(dump)到JSON/CSV/XML文件的最简单的方法?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#viewstate">在某些表单中巨大神秘的 <tt class="docutils literal"><span class="pre">__VIEWSTATE</span></tt> 参数是什么？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#xml-csv">分析大XML/CSV数据源的最好方法是?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#scrapycookies">Scrapy自动管理cookies么？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#scrapyscrapy">如何才能看到Scrapy发出及接收到的Scrapy呢？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#id10">要怎么停止爬虫呢?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#scrapy-bot-ban">如何避免我的Scrapy机器人(bot)被禁止(ban)呢？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#spider-arguments-settings-spider">我应该使用spider参数(arguments)还是设置(settings)来配置spider呢？</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#xmlxpathitem">我爬取了一个XML文档但是XPath选择器不返回任何的item</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#name-crawler">我得到错误: &#8220;不能导入name crawler“</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/debug">调试(Debugging)Spiders</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/debug#parse">Parse命令</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/debug#scrapy-shell">Scrapy终端(Shell)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/debug#id1">在浏览器中打开</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/debug#logging">Logging</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/contracts">Spiders Contracts</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/contracts#contracts">自定义Contracts</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/practices">实践经验(Common Practices)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/practices#scrapy">在脚本中运行Scrapy</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/practices#spider">同一进程运行多个spider</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/practices#distributed-crawls">分布式爬虫(Distributed crawls)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/practices#ban">避免被禁止(ban)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/practices#item">动态创建Item类</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/broad-crawls">通用爬虫(Broad Crawls)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/broad-crawls#id1">增加并发</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/broad-crawls#log">降低log级别</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/broad-crawls#cookies">禁止cookies</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/broad-crawls#id2">禁止重试</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/broad-crawls#id3">减小下载超时</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/broad-crawls#id4">禁止重定向</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/broad-crawls#ajax-crawlable-pages">启用 &#8220;Ajax Crawlable Pages&#8221; 爬取</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/firefox">借助Firefox来爬取</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/firefox#dom">在浏览器中检查DOM的注意事项</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/firefox#topics-firefox-addons">对爬取有帮助的实用Firefox插件</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/firebug">使用Firebug进行爬取</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/firebug#id1">介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/firebug#follow">获取到跟进(follow)的链接</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/firebug#id4">提取数据</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/leaks">调试内存溢出</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/leaks#id2">内存泄露的常见原因</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/leaks#trackref">使用 <tt class="docutils literal"><span class="pre">trackref</span></tt> 调试内存泄露</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/leaks#guppy">使用Guppy调试内存泄露</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/leaks#leaks-without-leaks">Leaks without leaks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/images">下载项目图片</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/images#id2">使用图片管道</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/images#id3">使用样例</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/images#topics-images-enabling">开启你的图片管道</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/images#id5">图片存储</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/images#id7">额外的特性</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/images#module-scrapy.contrib.pipeline.images">实现定制图片管道</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/images#id12">定制图片管道的例子</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/ubuntu">Ubuntu 软件包</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/scrapyd">Scrapyd</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/autothrottle">自动限速(AutoThrottle)扩展</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/autothrottle#id1">设计目标</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/autothrottle#id2">扩展是如何实现的</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/autothrottle#autothrottle-algorithm">限速算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/autothrottle#id4">设置</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/benchmarking">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/jobs">Jobs: 暂停，恢复爬虫</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/jobs#job">Job 路径</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/jobs#id1">怎么使用</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/jobs#id2">保持状态</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/jobs#id3">持久化的一些坑</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/djangoitem">DjangoItem</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/djangoitem#id1">使用DjangoItem</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/djangoitem#id2">DjangoItem注意事项</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/djangoitem#django">配置Django的设置</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/architecture">架构概览</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/architecture#id2">概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/architecture#id3">组件</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/architecture#data-flow">数据流(Data flow)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/architecture#event-driven-networking">事件驱动网络(Event-driven networking)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/downloader-middleware">下载器中间件(Downloader Middleware)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/downloader-middleware#topics-downloader-middleware-setting">激活下载器中间件</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/downloader-middleware#id2">编写您自己的下载器中间件</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/downloader-middleware#topics-downloader-middleware-ref">内置下载中间件参考手册</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/spider-middleware">Spider中间件(Middleware)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/spider-middleware#spider">激活spider中间件</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/spider-middleware#id1">编写您自己的spider中间件</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/spider-middleware#topics-spider-middleware-ref">内置spider中间件参考手册</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/extensions">扩展(Extensions)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/extensions#extension-settings">扩展设置(Extension settings)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/extensions#id1">加载和激活扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/extensions#available-enabled-disabled">可用的(Available)、开启的(enabled)和禁用的(disabled)的扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/extensions#disabling-an-extension">禁用扩展(Disabling an extension)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/extensions#id2">实现你的扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/extensions#topics-extensions-ref">内置扩展介绍</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/api">核心API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/api#crawler-api">Crawler API</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/api#module-scrapy.settings">设置(Settings) API</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/api#module-scrapy.signalmanager">信号(Signals) API</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/api#stats-collector-api">状态收集器(Stats Collector) API</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/request-response">Requests and Responses</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/request-response#request-objects">Request objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/request-response#request-meta-special-keys">Request.meta special keys</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/request-response#request-subclasses">Request subclasses</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/request-response#response-objects">Response objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/request-response#response-subclasses">Response subclasses</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/settings">Settings</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/settings#designating-the-settings">指定设定(Designating the settings)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/settings#populating-the-settings">获取设定值(Populating the settings)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/settings#how-to-access-settings">如何访问设定(How to access settings)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/settings#id1">设定名字的命名规则</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/settings#topics-settings-ref">内置设定参考手册</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/signals">信号(Signals)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/signals#deferred-signal-handlers">延迟的信号处理器(Deferred signal handlers)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/signals#module-scrapy.signals">内置信号参考手册(Built-in signals reference)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/exceptions">异常(Exceptions)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/exceptions#built-in-exceptions-reference">内置异常参考手册(Built-in Exceptions reference)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/exporters">Item Exporters</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/exporters#item-exporter">使用 Item Exporter</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/exporters#topics-exporters-reference">Item Exporters 参考资料</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-news">Release notes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id1">0.24.0 (2014-06-26)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2014-02-14">0.22.2 (released 2014-02-14)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2014-02-08">0.22.1 (released 2014-02-08)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2014-01-17">0.22.0 (released 2014-01-17)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2013-12-09">0.20.2 (released 2013-12-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2013-11-28">0.20.1 (released 2013-11-28)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2013-11-08">0.20.0 (released 2013-11-08)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2013-10-10">0.18.4 (released 2013-10-10)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2013-10-03">0.18.3 (released 2013-10-03)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2013-09-03">0.18.2 (released 2013-09-03)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2013-08-27">0.18.1 (released 2013-08-27)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2013-08-09">0.18.0 (released 2013-08-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2013-05-30">0.16.5 (released 2013-05-30)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2013-01-23">0.16.4 (released 2013-01-23)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2012-12-07">0.16.3 (released 2012-12-07)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2012-11-09">0.16.2 (released 2012-11-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2012-10-26">0.16.1 (released 2012-10-26)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2012-10-18">0.16.0 (released 2012-10-18)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id5">0.14.4</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id6">0.14.3</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id7">0.14.2</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id8">0.14.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id9">0.14</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id10">0.12</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id11">0.10</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id14">0.9</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id17">0.8</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id18">0.7</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-contributing">Contributing to Scrapy</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-contributing#reporting-bugs">Reporting bugs</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-contributing#writing-patches">Writing patches</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-contributing#submitting-patches">Submitting patches</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-contributing#coding-style">Coding style</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-contributing#scrapy-contrib">Scrapy Contrib</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-contributing#documentation-policies">Documentation policies</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-contributing#tests">Tests</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-versioning">Versioning and API Stability</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-versioning#id1">Versioning</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-versioning#api-stability">API Stability</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-experimental/index">试验阶段特性</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-experimental/index#id3">使用外部库插入命令</a></li>
</ul>
</li>
</ul>

        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html#document-index">Scrapy</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html#document-index">Docs</a> &raquo;</li>
      
    <li>Scrapy 0.24.0 文档</li>
      <li class="wy-breadcrumbs-aside">
        
          <a href="https://github.com/marchtea/scrapy_doc_chs/blob/0.24/index.rst" class="fa fa-github"> Edit on GitHub</a>
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            
    
  <div class="section" id="scrapy-version">
<span id="topics-index"></span><h1>Scrapy 0.24 文档<a class="headerlink" href="#scrapy-version" title="永久链接至标题">¶</a></h1>
<p>本文档涵盖了所有Scrapy的内容。</p>
<div class="section" id="id1">
<h2>获得帮助<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h2>
<p>遇到问题了？我们来帮您！</p>
<ul class="simple">
<li>查看下 <a class="reference internal" href="index.html#document-faq"><em>FAQ</em></a> ，这里有些常见的问题的解决办法。</li>
<li>寻找详细的信息？试试 <a class="reference internal" href="genindex.html"><em>索引</em></a> 或者 <a class="reference internal" href="py-modindex.html"><em>模块索引</em></a> 。</li>
<li>您可以在 <a class="reference external" href="http://groups.google.com/group/scrapy-users/">scrapy-users的邮件列表</a> 中寻找内容，或者 <a class="reference external" href="http://groups.google.com/group/scrapy-users/">提问问题</a></li>
<li>在 <a class="reference external" href="irc://irc.freenode.net/scrapy">#scrapy IRC channel</a> 提问</li>
<li>在 <a class="reference external" href="https://github.com/scrapy/scrapy/issues">issue tracker</a> 中提交Scrapy的bug</li>
</ul>
</div>
<div class="section" id="id3">
<h2>第一步<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-intro/overview"></span><div class="section" id="scrapy">
<span id="intro-overview"></span><h3>初窥Scrapy<a class="headerlink" href="#scrapy" title="永久链接至标题">¶</a></h3>
<p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。
可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。</p>
<p>其最初是为了 <a class="reference external" href="http://en.wikipedia.org/wiki/Screen_scraping">页面抓取</a> (更确切来说, <a class="reference external" href="http://en.wikipedia.org/wiki/Web_scraping">网络抓取</a> )所设计的，
也可以应用在获取API所返回的数据(例如 <a class="reference external" href="http://aws.amazon.com/associates/">Amazon Associates Web Services</a> ) 或者通用的网络爬虫。</p>
<p>本文档将通过介绍Scrapy背后的概念使您对其工作原理有所了解，
并确定Scrapy是否是您所需要的。</p>
<p>当您准备好开始您的项目后，您可以参考 <a class="reference internal" href="index.html#intro-tutorial"><em>入门教程</em></a> 。</p>
<div class="section" id="id1">
<h4>选择一个网站<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h4>
<p>当您需要从某个网站中获取信息，但该网站未提供API或能通过程序获取信息的机制时，
Scrapy可以助你一臂之力。</p>
<p>以 <a class="reference external" href="http://www.mininova.org">Mininova</a> 网站为例，我们想要获取今日添加的所有种子的URL、
名字、描述以及文件大小信息。</p>
<p>今日添加的种子列表可以通过这个页面找到:</p>
<blockquote>
<div><a class="reference external" href="http://www.mininova.org/today">http://www.mininova.org/today</a></div></blockquote>
</div>
<div class="section" id="intro-overview-item">
<span id="id2"></span><h4>定义您想抓取的数据<a class="headerlink" href="#intro-overview-item" title="永久链接至标题">¶</a></h4>
<p>第一步是定义我们需要爬取的数据。在Scrapy中，
这是通过 <a class="reference internal" href="index.html#topics-items"><em>Scrapy Items</em></a> 来完成的。(在本例子中为种子文件)</p>
<p>我们定义的Item:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">TorrentItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">url</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">description</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="spider">
<h4>编写提取数据的Spider<a class="headerlink" href="#spider" title="永久链接至标题">¶</a></h4>
<p>第二步是编写一个spider。其定义了初始URL(<a class="reference external" href="http://www.mininova.org/today">http://www.mininova.org/today</a>)、
针对后续链接的规则以及从页面中提取数据的规则。</p>
<p>通过观察页面的内容可以发现，所有种子的URL都类似 <tt class="docutils literal"><span class="pre">http://www.mininova.org/tor/NUMBER</span></tt> 。
其中， <tt class="docutils literal"><span class="pre">NUMBER</span></tt> 是一个整数。
根据此规律，我们可以定义需要进行跟进的链接的正则表达式: <tt class="docutils literal"><span class="pre">/tor/\d+</span></tt> 。</p>
<p>我们使用 <a class="reference external" href="http://www.w3.org/TR/xpath">XPath</a> 来从页面的HTML源码中选择需要提取的数据。
以其中一个种子文件的页面为例:</p>
<blockquote>
<div><a class="reference external" href="http://www.mininova.org/tor/2676093">http://www.mininova.org/tor/2676093</a></div></blockquote>
<p>观察HTML页面源码并创建我们需要的数据(种子名字，描述和大小)的XPath表达式。</p>
<p>通过观察，我们可以发现文件名是包含在 <tt class="docutils literal"><span class="pre">&lt;h1&gt;</span></tt> 标签中的:</p>
<div class="highlight-html"><div class="highlight"><pre><span class="nt">&lt;h1&gt;</span>Darwin - The Evolution Of An Exhibition<span class="nt">&lt;/h1&gt;</span>
</pre></div>
</div>
<p>与此对应的XPath表达式:</p>
<div class="highlight-none"><div class="highlight"><pre>//h1/text()
</pre></div>
</div>
<p>种子的描述是被包含在 <tt class="docutils literal"><span class="pre">id=&quot;description&quot;</span></tt> 的 <tt class="docutils literal"><span class="pre">&lt;div&gt;</span></tt> 标签中:</p>
<div class="highlight-html"><div class="highlight"><pre><span class="nt">&lt;h2&gt;</span>Description:<span class="nt">&lt;/h2&gt;</span>

<span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">&quot;description&quot;</span><span class="nt">&gt;</span>
Short documentary made for Plymouth City Museum and Art Gallery regarding the setup of an exhibit about Charles Darwin in conjunction with the 200th anniversary of his birth.

...
</pre></div>
</div>
<p>对应获取描述的XPath表达式:</p>
<div class="highlight-none"><div class="highlight"><pre>//div[@id=&#39;description&#39;]
</pre></div>
</div>
<p>文件大小的信息包含在 <tt class="docutils literal"><span class="pre">id=specifications</span></tt> 的 <tt class="docutils literal"><span class="pre">&lt;div&gt;</span></tt> 的第二个 <tt class="docutils literal"><span class="pre">&lt;p&gt;</span></tt> 标签中:</p>
<div class="highlight-html"><div class="highlight"><pre><span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">&quot;specifications&quot;</span><span class="nt">&gt;</span>

<span class="nt">&lt;p&gt;</span>
<span class="nt">&lt;strong&gt;</span>Category:<span class="nt">&lt;/strong&gt;</span>
<span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">&quot;/cat/4&quot;</span><span class="nt">&gt;</span>Movies<span class="nt">&lt;/a&gt;</span> <span class="ni">&amp;gt;</span> <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">&quot;/sub/35&quot;</span><span class="nt">&gt;</span>Documentary<span class="nt">&lt;/a&gt;</span>
<span class="nt">&lt;/p&gt;</span>

<span class="nt">&lt;p&gt;</span>
<span class="nt">&lt;strong&gt;</span>Total size:<span class="nt">&lt;/strong&gt;</span>
150.62<span class="ni">&amp;nbsp;</span>megabyte<span class="nt">&lt;/p&gt;</span>
</pre></div>
</div>
<p>选择文件大小的XPath表达式:</p>
<div class="highlight-none"><div class="highlight"><pre>//div[@id=&#39;specifications&#39;]/p[2]/text()[2]
</pre></div>
</div>
<p>关于XPath的详细内容请参考 <a class="reference external" href="http://www.w3.org/TR/xpath">XPath参考</a> 。</p>
<p>最后，结合以上内容给出spider的代码:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.spiders</span> <span class="kn">import</span> <span class="n">CrawlSpider</span><span class="p">,</span> <span class="n">Rule</span>
<span class="kn">from</span> <span class="nn">scrapy.contrib.linkextractors</span> <span class="kn">import</span> <span class="n">LinkExtractor</span>

<span class="k">class</span> <span class="nc">MininovaSpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;mininova&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;mininova.org&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.mininova.org/today&#39;</span><span class="p">]</span>
    <span class="n">rules</span> <span class="o">=</span> <span class="p">[</span><span class="n">Rule</span><span class="p">(</span><span class="n">LinkExtractor</span><span class="p">(</span><span class="n">allow</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;/tor/\d+&#39;</span><span class="p">]),</span> <span class="s">&#39;parse_torrent&#39;</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">parse_torrent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">torrent</span> <span class="o">=</span> <span class="n">TorrentItem</span><span class="p">()</span>
        <span class="n">torrent</span><span class="p">[</span><span class="s">&#39;url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span>
        <span class="n">torrent</span><span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//h1/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="n">torrent</span><span class="p">[</span><span class="s">&#39;description&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//div[@id=&#39;description&#39;]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="n">torrent</span><span class="p">[</span><span class="s">&#39;size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//div[@id=&#39;info-left&#39;]/p[2]/text()[2]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">torrent</span>
</pre></div>
</div>
<p><tt class="docutils literal"><span class="pre">TorrentItem</span></tt> 的定义在 <a class="reference internal" href="index.html#intro-overview-item"><em>上面</em></a> 。</p>
</div>
<div class="section" id="id3">
<h4>执行spider，获取数据<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h4>
<p>终于，我们可以运行spider来获取网站的数据，并以JSON格式存入到
<tt class="docutils literal"><span class="pre">scraped_data.json</span></tt> 文件中:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy crawl mininova -o scraped_data.json
</pre></div>
</div>
<p>命令中使用了 <a class="reference internal" href="index.html#topics-feed-exports"><em>feed导出</em></a> 来导出JSON文件。您可以修改导出格式(XML或者CSV)或者存储后端(FTP或者 <a class="reference external" href="http://aws.amazon.com/s3/">Amazon S3</a>)，这并不困难。</p>
<p>同时，您也可以编写 <a class="reference internal" href="index.html#topics-item-pipeline"><em>item管道</em></a> 将item存储到数据库中。</p>
</div>
<div class="section" id="id4">
<h4>查看提取到的数据<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h4>
<p>执行结束后，当您查看 <tt class="docutils literal"><span class="pre">scraped_data.json</span></tt> , 您将看到提取到的item:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">[{</span><span class="s">&quot;url&quot;</span><span class="p">:</span> <span class="s">&quot;http://www.mininova.org/tor/2676093&quot;</span><span class="p">,</span> <span class="s">&quot;name&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s">&quot;Darwin - The Evolution Of An Exhibition&quot;</span><span class="p">],</span> <span class="s">&quot;description&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s">&quot;Short documentary made for Plymouth ...&quot;</span><span class="p">],</span> <span class="s">&quot;size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s">&quot;150.62 megabyte&quot;</span><span class="p">]},</span>
<span class="c"># ... other items ...</span>
<span class="p">]</span>
</pre></div>
</div>
<p>由于 <a class="reference internal" href="index.html#topics-selectors"><em>selectors</em></a> 返回list, 所以值都是以list存储的(除了 <tt class="docutils literal"><span class="pre">url</span></tt> 是直接赋值之外)。
如果您想要保存单个数据或者对数据执行额外的处理,那将是 <a class="reference internal" href="index.html#topics-loaders"><em>Item Loaders</em></a> 发挥作用的地方。</p>
</div>
<div class="section" id="topics-whatelse">
<span id="id5"></span><h4>还有什么？<a class="headerlink" href="#topics-whatelse" title="永久链接至标题">¶</a></h4>
<p>您已经了解了如何通过Scrapy提取存储网页中的信息，但这仅仅只是冰山一角。Scrapy提供了很多强大的特性来使得爬取更为简单高效, 例如:</p>
<ul class="simple">
<li>HTML, XML源数据 <a class="reference internal" href="index.html#topics-selectors"><em>选择及提取</em></a> 的内置支持</li>
<li>提供了一系列在spider之间共享的可复用的过滤器(即 <a class="reference internal" href="index.html#topics-loaders"><em>Item Loaders</em></a>)，对智能处理爬取数据提供了内置支持。</li>
<li>通过 <a class="reference internal" href="index.html#topics-feed-exports"><em>feed导出</em></a> 提供了多格式(JSON、CSV、XML)，多存储后端(FTP、S3、本地文件系统)的内置支持</li>
<li>提供了media pipeline，可以 <a class="reference internal" href="index.html#topics-images"><em>自动下载</em></a> 爬取到的数据中的图片(或者其他资源)。</li>
<li>高扩展性。您可以通过使用 <a class="reference internal" href="index.html#topics-signals"><em>signals</em></a> ，设计好的API(中间件, <a class="reference internal" href="index.html#topics-extensions"><em>extensions</em></a>, <a class="reference internal" href="index.html#topics-item-pipeline"><em>pipelines</em></a>)来定制实现您的功能。</li>
<li>内置的中间件及扩展为下列功能提供了支持:<ul>
<li>cookies and session 处理</li>
<li>HTTP 压缩</li>
<li>HTTP 认证</li>
<li>HTTP 缓存</li>
<li>user-agent模拟</li>
<li>robots.txt</li>
<li>爬取深度限制</li>
<li>其他</li>
</ul>
</li>
<li>针对非英语语系中不标准或者错误的编码声明, 提供了自动检测以及健壮的编码支持。</li>
<li>支持根据模板生成爬虫。在加速爬虫创建的同时，保持在大型项目中的代码更为一致。详细内容请参阅 <a class="reference internal" href="index.html#std:command-genspider"><tt class="xref std std-command docutils literal"><span class="pre">genspider</span></tt></a> 命令。</li>
<li>针对多爬虫下性能评估、失败检测，提供了可扩展的 <a class="reference internal" href="index.html#topics-stats"><em>状态收集工具</em></a> 。</li>
<li>提供 <a class="reference internal" href="index.html#topics-shell"><em>交互式shell终端</em></a> , 为您测试XPath表达式，编写和调试爬虫提供了极大的方便</li>
<li>提供 <a class="reference internal" href="index.html#topics-scrapyd"><em>System service</em></a>, 简化在生产环境的部署及运行</li>
<li>内置 <a class="reference internal" href="index.html#topics-webservice"><em>Web service</em></a>, 使您可以监视及控制您的机器</li>
<li>内置 <a class="reference internal" href="index.html#topics-telnetconsole"><em>Telnet终端</em></a> ，通过在Scrapy进程中钩入Python终端，使您可以查看并且调试爬虫</li>
<li><a class="reference internal" href="index.html#topics-logging"><em>Logging</em></a> 为您在爬取过程中捕捉错误提供了方便</li>
<li>支持 <a class="reference external" href="http://www.sitemaps.org">Sitemaps</a> 爬取</li>
<li>具有缓存的DNS解析器</li>
</ul>
</div>
<div class="section" id="id6">
<h4>接下来<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h4>
<p>下一步当然是 <a class="reference external" href="http://scrapy.org/download/">下载Scrapy</a> 了， 您可以阅读 <a class="reference internal" href="index.html#intro-tutorial"><em>入门教程</em></a> 并加入 <a class="reference external" href="http://scrapy.org/community/">社区</a> 。感谢您的支持!</p>
</div>
</div>
<span id="document-intro/install"></span><div class="section" id="intro-install">
<span id="id1"></span><h3>安装指南<a class="headerlink" href="#intro-install" title="永久链接至标题">¶</a></h3>
<div class="section" id="id2">
<h4>前期准备<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h4>
<p>下列的安装步骤假定您已经安装好下列程序:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.python.org">Python</a> 2.7</li>
<li><a class="reference external" href="http://lxml.de/">lxml</a>. 大多数Linux发行版自带了lxml。如果缺失，请查看http://lxml.de/installation.html</li>
<li><a class="reference external" href="https://pypi.python.org/pypi/pyOpenSSL">OpenSSL</a>. 除了Windows(请查看 <a class="reference internal" href="index.html#intro-install-platform-notes"><em>平台安装指南</em></a>)之外的系统都已经提供。</li>
<li><a class="reference external" href="http://www.pip-installer.org/en/latest/installing.html">pip</a> 或者 <a class="reference external" href="http://pypi.python.org/pypi/setuptools">easy_install</a> Python安装管理器</li>
</ul>
</div>
<div class="section" id="scrapy">
<h4>安装Scrapy<a class="headerlink" href="#scrapy" title="永久链接至标题">¶</a></h4>
<p>您可以通过使用easy_install或者pip来安装Scrapy。(pip是更受推荐用来分发和安装Python包的工具)</p>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">请先查看 <a class="reference internal" href="index.html#intro-install-platform-notes"><em>平台安装指南</em></a>.</p>
</div>
<p>使用pip安装:</p>
<div class="highlight-python"><div class="highlight"><pre>pip install Scrapy
</pre></div>
</div>
<p>使用easy_install安装:</p>
<div class="highlight-python"><div class="highlight"><pre>easy_install Scrapy
</pre></div>
</div>
</div>
<div class="section" id="intro-install-platform-notes">
<span id="id3"></span><h4>平台安装指南<a class="headerlink" href="#intro-install-platform-notes" title="永久链接至标题">¶</a></h4>
<div class="section" id="windows">
<h5>Windows<a class="headerlink" href="#windows" title="永久链接至标题">¶</a></h5>
<p>安装完Python后，在安装Scrapy前请先执行下列步骤:</p>
<ul class="simple">
<li>修改 <a class="reference external" href="http://www.microsoft.com/resources/documentation/windows/xp/all/proddocs/en-us/sysdm_advancd_environmnt_addchange_variable.mspx">控制面板</a> 中的 <tt class="docutils literal"><span class="pre">PATH</span></tt> 环境变量，将 <tt class="docutils literal"><span class="pre">C:\python27\Scripts</span></tt> 和 <tt class="docutils literal"><span class="pre">C:\python27</span></tt> 添加到系统路径中。</li>
<li>执行下列步骤来安装OpenSSL:<ol class="arabic">
<li>打开 <a class="reference external" href="http://slproweb.com/products/Win32OpenSSL.html">Win32 OpenSSL页面</a></li>
<li>根据您的Windows版本和架构(32位或64位)，下载Visual C++ 2008 redistributables.</li>
<li>根据您的Windows版本和架构来下载OpenSSL(完整版，不是简化版)</li>
<li>按照上面添加 <tt class="docutils literal"><span class="pre">python27</span></tt> 环境变量的方式将 <tt class="docutils literal"><span class="pre">c:\openssl-win32\bin</span></tt> 添加到您的 <tt class="docutils literal"><span class="pre">PATH</span></tt> 中。</li>
</ol>
</li>
<li>一些Scrapy依赖的二进制包(Twisted, lxml以及pyOpenSSL)要求系统中预装有编译器。如果没有安装Visual Studio,则会出现错误。您可以在下边的链接中找到Windows的安装包。在安装前请注意您的Python版本和Windows的架构(32或64位)<ul>
<li>pywin32: <a class="reference external" href="http://sourceforge.net/projects/pywin32/files/">http://sourceforge.net/projects/pywin32/files/</a></li>
<li>Twisted: <a class="reference external" href="http://twistedmatrix.com/trac/wiki/Downloads">http://twistedmatrix.com/trac/wiki/Downloads</a></li>
<li>zope.interface: 您可以从 <a class="reference external" href="http://pypi.python.org/pypi/zope.interface">zope.interface pypi page</a> 下载并运行 <tt class="docutils literal"><span class="pre">easy_install</span> <span class="pre">file.egg</span></tt> 安装</li>
<li>lxml: <a class="reference external" href="http://pypi.python.org/pypi/lxml/">http://pypi.python.org/pypi/lxml/</a></li>
<li>pyOpenSSL: <a class="reference external" href="https://launchpad.net/pyopenssl">https://launchpad.net/pyopenssl</a></li>
</ul>
</li>
</ul>
<p>另外， 下面的页面包括很多已经编译好的Python二进制库，方便满足Scrapy的依赖。</p>
<blockquote>
<div><a class="reference external" href="http://www.lfd.uci.edu/~gohlke/pythonlibs/">http://www.lfd.uci.edu/~gohlke/pythonlibs/</a></div></blockquote>
<div class="section" id="ubuntu-9-10">
<h6>Ubuntu 9.10及以上版本<a class="headerlink" href="#ubuntu-9-10" title="永久链接至标题">¶</a></h6>
<p><strong>不要</strong> 使用Ubuntu提供的 <tt class="docutils literal"><span class="pre">python-scrapy</span></tt> ，相较于最新版的Scrapy，该包版本太旧，并且运行速度也较为缓慢。</p>
<p>您可以使用官方提供的 <a class="reference internal" href="index.html#topics-ubuntu"><em>Ubuntu Packages</em></a> 。该包解决了全部依赖问题，并且与最新的bug修复保持持续更新。</p>
</div>
</div>
</div>
</div>
<span id="document-intro/tutorial"></span><div class="section" id="scrapy">
<span id="intro-tutorial"></span><h3>Scrapy入门教程<a class="headerlink" href="#scrapy" title="永久链接至标题">¶</a></h3>
<p>在本篇教程中，我们假定您已经安装好Scrapy。
如若不然，请参考 <a class="reference internal" href="index.html#intro-install"><em>安装指南</em></a> 。</p>
<p>接下来以 <a class="reference external" href="http://www.dmoz.org/">Open Directory Project(dmoz) (dmoz)</a>
为例来讲述爬取。</p>
<p>本篇教程中将带您完成下列任务:</p>
<ol class="arabic simple">
<li>创建一个Scrapy项目</li>
<li>定义提取的Item</li>
<li>编写爬取网站的 <a class="reference internal" href="index.html#topics-spiders"><em>spider</em></a> 并提取 <a class="reference internal" href="index.html#topics-items"><em>Item</em></a></li>
<li>编写 <a class="reference internal" href="index.html#topics-item-pipeline"><em>Item Pipeline</em></a> 来存储提取到的Item(即数据)</li>
</ol>
<p>Scrapy由 <a class="reference external" href="http://www.python.org">Python</a> 编写。如果您刚接触并且好奇这门语言的特性以及Scrapy的详情，
对于已经熟悉其他语言并且想快速学习Python的编程老手，
我们推荐 <a class="reference external" href="http://learnpythonthehardway.org/book/">Learn Python The Hard Way</a> ，
对于想从Python开始学习的编程新手，
<a class="reference external" href="http://wiki.python.org/moin/BeginnersGuide/NonProgrammers">非程序员的Python学习资料列表</a> 将是您的选择。</p>
<div class="section" id="id2">
<h4>创建项目<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h4>
<p>在开始爬取之前，您必须创建一个新的Scrapy项目。
进入您打算存储代码的目录中，运行下列命令:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy startproject tutorial
</pre></div>
</div>
<p>该命令将会创建包含下列内容的 <tt class="docutils literal"><span class="pre">tutorial</span></tt> 目录:</p>
<div class="highlight-python"><div class="highlight"><pre>tutorial/
    scrapy.cfg
    tutorial/
        __init__.py
        items.py
        pipelines.py
        settings.py
        spiders/
            __init__.py
            ...
</pre></div>
</div>
<p>这些文件分别是:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">scrapy.cfg</span></tt>: 项目的配置文件</li>
<li><tt class="docutils literal"><span class="pre">tutorial/</span></tt>: 该项目的python模块。之后您将在此加入代码。</li>
<li><tt class="docutils literal"><span class="pre">tutorial/items.py</span></tt>: 项目中的item文件.</li>
<li><tt class="docutils literal"><span class="pre">tutorial/pipelines.py</span></tt>: 项目中的pipelines文件.</li>
<li><tt class="docutils literal"><span class="pre">tutorial/settings.py</span></tt>: 项目的设置文件.</li>
<li><tt class="docutils literal"><span class="pre">tutorial/spiders/</span></tt>: 放置spider代码的目录.</li>
</ul>
</div>
<div class="section" id="item">
<h4>定义Item<a class="headerlink" href="#item" title="永久链接至标题">¶</a></h4>
<p><cite>Item</cite> 是保存爬取到的数据的容器；其使用方法和python字典类似，
并且提供了额外保护机制来避免拼写错误导致的未定义字段错误。</p>
<p>类似在ORM中做的一样，您可以通过创建一个 <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">scrapy.Item</span></tt></a> 类，
并且定义类型为 <a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">scrapy.Field</span></tt></a> 的类属性来定义一个Item。
(如果不了解ORM, 不用担心，您会发现这个步骤非常简单)</p>
<p>首先根据需要从dmoz.org获取到的数据对item进行建模。
我们需要从dmoz中获取名字，url，以及网站的描述。
对此，在item中定义相应的字段。编辑 <tt class="docutils literal"><span class="pre">tutorial</span></tt> 目录中的 <tt class="docutils literal"><span class="pre">items.py</span></tt> 文件:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">DmozItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">link</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">desc</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
<p>一开始这看起来可能有点复杂，但是通过定义item，
您可以很方便的使用Scrapy的其他方法。而这些方法需要知道您的item的定义。</p>
</div>
<div class="section" id="spider">
<h4>编写第一个爬虫(Spider)<a class="headerlink" href="#spider" title="永久链接至标题">¶</a></h4>
<p>Spider是用户编写用于从单个网站(或者一些网站)爬取数据的类。</p>
<p>其包含了一个用于下载的初始URL，如何跟进网页中的链接以及如何分析页面中的内容，
提取生成 <a class="reference internal" href="index.html#topics-items"><em>item</em></a> 的方法。</p>
<p>为了创建一个Spider，您必须继承 <a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">scrapy.Spider</span></tt></a> 类，
且定义以下三个属性:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#scrapy.spider.Spider.name" title="scrapy.spider.Spider.name"><tt class="xref py py-attr docutils literal"><span class="pre">name</span></tt></a>: 用于区别Spider。
该名字必须是唯一的，您不可以为不同的Spider设定相同的名字。</li>
<li><a class="reference internal" href="index.html#scrapy.spider.Spider.start_urls" title="scrapy.spider.Spider.start_urls"><tt class="xref py py-attr docutils literal"><span class="pre">start_urls</span></tt></a>: 包含了Spider在启动时进行爬取的url列表。
因此，第一个被获取到的页面将是其中之一。
后续的URL则从初始的URL获取到的数据中提取。</li>
<li><a class="reference internal" href="index.html#scrapy.spider.Spider.parse" title="scrapy.spider.Spider.parse"><tt class="xref py py-meth docutils literal"><span class="pre">parse()</span></tt></a> 是spider的一个方法。
被调用时，每个初始URL完成下载后生成的 <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a>
对象将会作为唯一的参数传递给该函数。
该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象。</li>
</ul>
<p>以下为我们的第一个Spider代码，保存在 <tt class="docutils literal"><span class="pre">tutorial/spiders</span></tt> 目录下的 <tt class="docutils literal"><span class="pre">dmoz_spider.py</span></tt> 文件中:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">DmozSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&quot;dmoz&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;dmoz.org&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;</span><span class="p">,</span>
        <span class="s">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot;</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="id3">
<h5>爬取<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h5>
<p>进入项目的根目录，执行下列命令启动spider:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy crawl dmoz
</pre></div>
</div>
<p><tt class="docutils literal"><span class="pre">crawl</span> <span class="pre">dmoz</span></tt> 启动用于爬取 <tt class="docutils literal"><span class="pre">dmoz.org</span></tt> 的spider，您将得到类似的输出:</p>
<div class="highlight-python"><div class="highlight"><pre>2014-01-23 18:13:07-0400 [scrapy] INFO: Scrapy started (bot: tutorial)
2014-01-23 18:13:07-0400 [scrapy] INFO: Optional features available: ...
2014-01-23 18:13:07-0400 [scrapy] INFO: Overridden settings: {}
2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled extensions: ...
2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled downloader middlewares: ...
2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled spider middlewares: ...
2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled item pipelines: ...
2014-01-23 18:13:07-0400 [dmoz] INFO: Spider opened
2014-01-23 18:13:08-0400 [dmoz] DEBUG: Crawled (200) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&gt; (referer: None)
2014-01-23 18:13:09-0400 [dmoz] DEBUG: Crawled (200) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; (referer: None)
2014-01-23 18:13:09-0400 [dmoz] INFO: Closing spider (finished)
</pre></div>
</div>
<p>查看包含 <tt class="docutils literal"><span class="pre">[dmoz]</span></tt> 的输出，可以看到输出的log中包含定义在 <tt class="docutils literal"><span class="pre">start_urls</span></tt> 的初始URL，并且与spider中是一一对应的。在log中可以看到其没有指向其他页面( <tt class="docutils literal"><span class="pre">(referer:None)</span></tt> )。</p>
<p>除此之外，更有趣的事情发生了。就像我们 <tt class="docutils literal"><span class="pre">parse</span></tt> 方法指定的那样，有两个包含url所对应的内容的文件被创建了: <em>Book</em> , <em>Resources</em> 。</p>
<div class="section" id="id4">
<h6>刚才发生了什么？<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h6>
<p>Scrapy为Spider的 <tt class="docutils literal"><span class="pre">start_urls</span></tt> 属性中的每个URL创建了 <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">scrapy.Request</span></tt></a> 对象，并将 <tt class="docutils literal"><span class="pre">parse</span></tt> 方法作为回调函数(callback)赋值给了Request。</p>
<p>Request对象经过调度，执行生成 <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">scrapy.http.Response</span></tt></a> 对象并送回给spider <a class="reference internal" href="index.html#scrapy.spider.Spider.parse" title="scrapy.spider.Spider.parse"><tt class="xref py py-meth docutils literal"><span class="pre">parse()</span></tt></a> 方法。</p>
</div>
</div>
<div class="section" id="id5">
<h5>提取Item<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h5>
<div class="section" id="selectors">
<h6>Selectors选择器简介<a class="headerlink" href="#selectors" title="永久链接至标题">¶</a></h6>
<p>从网页中提取数据有很多方法。Scrapy使用了一种基于 <a class="reference external" href="http://www.w3.org/TR/xpath">XPath</a> 和 <a class="reference external" href="http://www.w3.org/TR/selectors">CSS</a> 表达式机制:
<a class="reference internal" href="index.html#topics-selectors"><em>Scrapy Selectors</em></a> 。
关于selector和其他提取机制的信息请参考 <a class="reference internal" href="index.html#topics-selectors"><em>Selector文档</em></a> 。</p>
<p>这里给出XPath表达式的例子及对应的含义:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">/html/head/title</span></tt>: 选择HTML文档中 <tt class="docutils literal"><span class="pre">&lt;head&gt;</span></tt> 标签内的 <tt class="docutils literal"><span class="pre">&lt;title&gt;</span></tt> 元素</li>
<li><tt class="docutils literal"><span class="pre">/html/head/title/text()</span></tt>: 选择上面提到的 <tt class="docutils literal"><span class="pre">&lt;title&gt;</span></tt> 元素的文字</li>
<li><tt class="docutils literal"><span class="pre">//td</span></tt>: 选择所有的 <tt class="docutils literal"><span class="pre">&lt;td&gt;</span></tt> 元素</li>
<li><tt class="docutils literal"><span class="pre">//div[&#64;class=&quot;mine&quot;]</span></tt>: 选择所有具有 <tt class="docutils literal"><span class="pre">class=&quot;mine&quot;</span></tt> 属性的 <tt class="docutils literal"><span class="pre">div</span></tt> 元素</li>
</ul>
<p>上边仅仅是几个简单的XPath例子，XPath实际上要比这远远强大的多。
如果您想了解的更多，我们推荐 <a class="reference external" href="http://www.w3schools.com/XPath/default.asp">这篇XPath教程</a> 。</p>
<p>为了配合XPath，Scrapy除了提供了 <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a>
之外，还提供了方法来避免每次从response中提取数据时生成selector的麻烦。</p>
<p>Selector有四个基本的方法(点击相应的方法可以看到详细的API文档):</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#scrapy.selector.Selector.xpath" title="scrapy.selector.Selector.xpath"><tt class="xref py py-meth docutils literal"><span class="pre">xpath()</span></tt></a>: 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表 。</li>
<li><a class="reference internal" href="index.html#scrapy.selector.Selector.css" title="scrapy.selector.Selector.css"><tt class="xref py py-meth docutils literal"><span class="pre">css()</span></tt></a>: 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表.</li>
<li><a class="reference internal" href="index.html#scrapy.selector.Selector.extract" title="scrapy.selector.Selector.extract"><tt class="xref py py-meth docutils literal"><span class="pre">extract()</span></tt></a>: 序列化该节点为unicode字符串并返回list。</li>
<li><a class="reference internal" href="index.html#scrapy.selector.Selector.re" title="scrapy.selector.Selector.re"><tt class="xref py py-meth docutils literal"><span class="pre">re()</span></tt></a>: 根据传入的正则表达式对数据进行提取，返回unicode字符串list列表。</li>
</ul>
</div>
<div class="section" id="shellselector">
<h6>在Shell中尝试Selector选择器<a class="headerlink" href="#shellselector" title="永久链接至标题">¶</a></h6>
<p>为了介绍Selector的使用方法，接下来我们将要使用内置的 <a class="reference internal" href="index.html#topics-shell"><em>Scrapy shell</em></a> 。Scrapy Shell需要您预装好IPython(一个扩展的Python终端)。</p>
<p>您需要进入项目的根目录，执行下列命令来启动shell:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy shell &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">当您在终端运行Scrapy时，请一定记得给url地址加上引号，否则包含参数的url(例如 <tt class="docutils literal"><span class="pre">&amp;</span></tt> 字符)会导致Scrapy运行失败。</p>
</div>
<p>shell的输出类似:</p>
<div class="highlight-python"><div class="highlight"><pre>[ ... Scrapy log here ... ]

2014-01-23 17:11:42-0400 [default] DEBUG: Crawled (200) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; (referer: None)
[s] Available Scrapy objects:
[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x3636b50&gt;
[s]   item       {}
[s]   request    &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;
[s]   response   &lt;200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;
[s]   settings   &lt;scrapy.settings.Settings object at 0x3fadc50&gt;
[s]   spider     &lt;Spider &#39;default&#39; at 0x3cebf50&gt;
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser

In [1]:
</pre></div>
</div>
<p>当shell载入后，您将得到一个包含response数据的本地 <tt class="docutils literal"><span class="pre">response</span></tt> 变量。输入 <tt class="docutils literal"><span class="pre">response.body</span></tt> 将输出response的包体， 输出 <tt class="docutils literal"><span class="pre">response.header</span></tt> 可以看到response的包头。</p>
<p>更为重要的是，当输入 <tt class="docutils literal"><span class="pre">response.selector</span></tt> 时，
您将获取到一个可以用于查询返回数据的selector(选择器)，
以及映射到 <tt class="docutils literal"><span class="pre">response.selector.xpath()</span></tt> 、 <tt class="docutils literal"><span class="pre">response.selector.css()</span></tt> 的
快捷方法(shortcut): <tt class="docutils literal"><span class="pre">response.xpath()</span></tt> 和 <tt class="docutils literal"><span class="pre">response.css()</span></tt> 。</p>
<p>同时，shell根据response提前初始化了变量 <tt class="docutils literal"><span class="pre">sel</span></tt> 。该selector根据response的类型自动选择最合适的分析规则(XML vs HTML)。</p>
<p>让我们来试试:</p>
<div class="highlight-python"><div class="highlight"><pre>In [1]: response.xpath(&#39;//title&#39;)
Out[1]: [&lt;Selector xpath=&#39;//title&#39; data=u&#39;&lt;title&gt;Open Directory - Computers: Progr&#39;&gt;]

In [2]: response.xpath(&#39;//title&#39;).extract()
Out[2]: [u&#39;&lt;title&gt;Open Directory - Computers: Programming: Languages: Python: Books&lt;/title&gt;&#39;]

In [3]: response.xpath(&#39;//title/text()&#39;)
Out[3]: [&lt;Selector xpath=&#39;//title/text()&#39; data=u&#39;Open Directory - Computers: Programming:&#39;&gt;]

In [4]: response.xpath(&#39;//title/text()&#39;).extract()
Out[4]: [u&#39;Open Directory - Computers: Programming: Languages: Python: Books&#39;]

In [5]: response.xpath(&#39;//title/text()&#39;).re(&#39;(\w+):&#39;)
Out[5]: [u&#39;Computers&#39;, u&#39;Programming&#39;, u&#39;Languages&#39;, u&#39;Python&#39;]
</pre></div>
</div>
</div>
<div class="section" id="id7">
<h6>提取数据<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h6>
<p>现在，我们来尝试从这些页面中提取些有用的数据。</p>
<p>您可以在终端中输入 <tt class="docutils literal"><span class="pre">response.body</span></tt> 来观察HTML源码并确定合适的XPath表达式。不过，这任务非常无聊且不易。您可以考虑使用Firefox的Firebug扩展来使得工作更为轻松。详情请参考 <a class="reference internal" href="index.html#topics-firebug"><em>使用Firebug进行爬取</em></a> 和 <a class="reference internal" href="index.html#topics-firefox"><em>借助Firefox来爬取</em></a> 。</p>
<p>在查看了网页的源码后，您会发现网站的信息是被包含在 <em>第二个</em> <tt class="docutils literal"><span class="pre">&lt;ul&gt;</span></tt> 元素中。</p>
<p>我们可以通过这段代码选择该页面中网站列表里所有 <tt class="docutils literal"><span class="pre">&lt;li&gt;</span></tt> 元素:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//ul/li&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>网站的描述:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//ul/li/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
<p>网站的标题:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//ul/li/a/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
<p>以及网站的链接:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//ul/li/a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
<p>之前提到过，每个 <tt class="docutils literal"><span class="pre">.xpath()</span></tt> 调用返回selector组成的list，因此我们可以拼接更多的 <tt class="docutils literal"><span class="pre">.xpath()</span></tt> 来进一步获取某个节点。我们将在下边使用这样的特性:</p>
<div class="highlight-python"><div class="highlight"><pre>for sel in response.xpath(&#39;//ul/li&#39;)
    title = sel.xpath(&#39;a/text()&#39;).extract()
    link = sel.xpath(&#39;a/@href&#39;).extract()
    desc = sel.xpath(&#39;text()&#39;).extract()
    print title, link, desc
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">关于嵌套selctor的更多详细信息，请参考 <a class="reference internal" href="index.html#topics-selectors-nesting-selectors"><em>嵌套选择器(selectors)</em></a> 以及 <a class="reference internal" href="index.html#topics-selectors"><em>选择器(Selectors)</em></a> 文档中的 <a class="reference internal" href="index.html#topics-selectors-relative-xpaths"><em>使用相对XPaths</em></a> 部分。</p>
</div>
<p>在我们的spider中加入这段代码:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">DmozSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&quot;dmoz&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;dmoz.org&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;</span><span class="p">,</span>
        <span class="s">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot;</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">sel</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//ul/li&#39;</span><span class="p">):</span>
            <span class="n">title</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;a/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="n">link</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="n">desc</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="k">print</span> <span class="n">title</span><span class="p">,</span> <span class="n">link</span><span class="p">,</span> <span class="n">desc</span>
</pre></div>
</div>
<p>现在尝试再次爬取dmoz.org，您将看到爬取到的网站信息被成功输出:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy crawl dmoz
</pre></div>
</div>
</div>
</div>
<div class="section" id="id8">
<h5>使用item<a class="headerlink" href="#id8" title="永久链接至标题">¶</a></h5>
<p><a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> 对象是自定义的python字典。
您可以使用标准的字典语法来获取到其每个字段的值。(字段即是我们之前用Field赋值的属性):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">item</span> <span class="o">=</span> <span class="n">DmozItem</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">item</span><span class="p">[</span><span class="s">&#39;title&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s">&#39;Example title&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">item</span><span class="p">[</span><span class="s">&#39;title&#39;</span><span class="p">]</span>
<span class="go">&#39;Example title&#39;</span>
</pre></div>
</div>
<p>一般来说，Spider将会将爬取到的数据以 <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> 对象返回。所以为了将爬取的数据返回，我们最终的代码将是:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="kn">from</span> <span class="nn">tutorial.items</span> <span class="kn">import</span> <span class="n">DmozItem</span>

<span class="k">class</span> <span class="nc">DmozSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&quot;dmoz&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;dmoz.org&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;</span><span class="p">,</span>
        <span class="s">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot;</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">sel</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//ul/li&#39;</span><span class="p">):</span>
            <span class="n">item</span> <span class="o">=</span> <span class="n">DmozItem</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s">&#39;title&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;a/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s">&#39;link&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s">&#39;desc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="k">yield</span> <span class="n">item</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">您可以在 <a class="reference external" href="https://github.com/scrapy/dirbot">dirbot</a> 项目中找到一个具有完整功能的spider。该项目可以通过 <a class="reference external" href="https://github.com/scrapy/dirbot">https://github.com/scrapy/dirbot</a> 找到。</p>
</div>
<p>现在对dmoz.org进行爬取将会产生 <tt class="docutils literal"><span class="pre">DmozItem</span></tt> 对象:</p>
<div class="highlight-python"><div class="highlight"><pre>[dmoz] DEBUG: Scraped from &lt;200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;
     {&#39;desc&#39;: [u&#39; - By David Mertz; Addison Wesley. Book in progress, full text, ASCII format. Asks for feedback. [author website, Gnosis Software, Inc.\n],
      &#39;link&#39;: [u&#39;http://gnosis.cx/TPiP/&#39;],
      &#39;title&#39;: [u&#39;Text Processing in Python&#39;]}
[dmoz] DEBUG: Scraped from &lt;200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;
     {&#39;desc&#39;: [u&#39; - By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has CD-ROM. Methods to build XML applications fast, Python tutorial, DOM and SAX, new Pyxie open source XML processing library. [Prentice Hall PTR]\n&#39;],
      &#39;link&#39;: [u&#39;http://www.informit.com/store/product.aspx?isbn=0130211192&#39;],
      &#39;title&#39;: [u&#39;XML Processing with Python&#39;]}
</pre></div>
</div>
</div>
</div>
<div class="section" id="id9">
<h4>保存爬取到的数据<a class="headerlink" href="#id9" title="永久链接至标题">¶</a></h4>
<p>最简单存储爬取的数据的方式是使用 <a class="reference internal" href="index.html#topics-feed-exports"><em>Feed exports</em></a>:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy crawl dmoz -o items.json
</pre></div>
</div>
<p>该命令将采用 <a class="reference external" href="http://en.wikipedia.org/wiki/JSON">JSON</a> 格式对爬取的数据进行序列化，生成 <tt class="docutils literal"><span class="pre">items.json</span></tt> 文件。</p>
<p>在类似本篇教程里这样小规模的项目中，这种存储方式已经足够。
如果需要对爬取到的item做更多更为复杂的操作，您可以编写
<a class="reference internal" href="index.html#topics-item-pipeline"><em>Item Pipeline</em></a> 。
类似于我们在创建项目时对Item做的，用于您编写自己的
<tt class="docutils literal"><span class="pre">tutorial/pipelines.py</span></tt> 也被创建。
不过如果您仅仅想要保存item，您不需要实现任何的pipeline。</p>
</div>
<div class="section" id="id10">
<h4>下一步<a class="headerlink" href="#id10" title="永久链接至标题">¶</a></h4>
<p>本篇教程仅介绍了Scrapy的基础，还有很多特性没有涉及。请查看 <a class="reference internal" href="index.html#intro-overview"><em>初窥Scrapy</em></a> 章节中的 <a class="reference internal" href="index.html#topics-whatelse"><em>还有什么？</em></a> 部分,大致浏览大部分重要的特性。</p>
<p>接着，我们推荐您把玩一个例子(查看 <a class="reference internal" href="index.html#intro-examples"><em>例子</em></a>)，而后继续阅读 <a class="reference internal" href="#section-basics"><em>基本概念</em></a> 。</p>
</div>
</div>
<span id="document-intro/examples"></span><div class="section" id="intro-examples">
<span id="id1"></span><h3>例子<a class="headerlink" href="#intro-examples" title="永久链接至标题">¶</a></h3>
<p>学习的最好方法就是参考例子，Scrapy也不例外。Scrapy提供了一个叫做 <a class="reference external" href="https://github.com/scrapy/dirbot">dirbot</a> 的样例项目供您把玩学习。其包含了在教程中介绍的dmoz spider。</p>
<p>您可以通过 <a class="reference external" href="https://github.com/scrapy/dirbot">https://github.com/scrapy/dirbot</a> 找到 <a class="reference external" href="https://github.com/scrapy/dirbot">dirbot</a> 。其包含了README文件，详细介绍了项目的内容。</p>
<p>如果您熟悉git，您可以checkout代码。或者您可以点击 <a class="reference external" href="https://github.com/scrapy/dirbot/archives/master">Downloads</a> 来下载项目的tarball或者zip的压缩包。</p>
<p><a class="reference external" href="http://snipplr.com/all/tags/scrapy/">Snipplr上的scrapy标签</a> 是用来分享spider，middeware，extension或者script代码片段。欢迎(并鼓励)在那分享您的代码。</p>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-intro/overview"><em>初窥Scrapy</em></a></dt>
<dd>了解Scrapy如何祝你一臂之力。</dd>
<dt><a class="reference internal" href="index.html#document-intro/install"><em>安装指南</em></a></dt>
<dd>安装Scrapy。</dd>
<dt><a class="reference internal" href="index.html#document-intro/tutorial"><em>Scrapy入门教程</em></a></dt>
<dd>编写您的第一个Scrapy项目。</dd>
<dt><a class="reference internal" href="index.html#document-intro/examples"><em>例子</em></a></dt>
<dd>通过把玩已存在的Scrapy项目来学习更多内容。</dd>
</dl>
</div>
<div class="section" id="section-basics">
<span id="id4"></span><h2>基本概念<a class="headerlink" href="#section-basics" title="永久链接至标题">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-topics/commands"></span><div class="section" id="command-line-tools">
<span id="topics-commands"></span><h3>命令行工具(Command line tools)<a class="headerlink" href="#command-line-tools" title="永久链接至标题">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">0.10 新版功能.</span></p>
</div>
<p>Scrapy是通过 <tt class="docutils literal"><span class="pre">scrapy</span></tt> 命令行工具进行控制的。
这里我们称之为 &#8220;Scrapy tool&#8221; 以用来和子命令进行区分。
对于子命令，我们称为 &#8220;command&#8221; 或者 &#8220;Scrapy commands&#8221;。</p>
<p>Scrapy tool 针对不同的目的提供了多个命令，每个命令支持不同的参数和选项。</p>
<div class="section" id="scrapy">
<span id="topics-project-structure"></span><h4>默认的Scrapy项目结构<a class="headerlink" href="#scrapy" title="永久链接至标题">¶</a></h4>
<p>在开始对命令行工具以及子命令的探索前，让我们首先了解一下Scrapy的项目的目录结构。</p>
<p>虽然可以被修改，但所有的Scrapy项目默认有类似于下边的文件结构:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy.cfg
myproject/
    __init__.py
    items.py
    pipelines.py
    settings.py
    spiders/
        __init__.py
        spider1.py
        spider2.py
        ...
</pre></div>
</div>
<p><tt class="docutils literal"><span class="pre">scrapy.cfg</span></tt> 存放的目录被认为是 <em>项目的根目录</em> 。该文件中包含python模块名的字段定义了项目的设置。例如:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">[</span><span class="n">settings</span><span class="p">]</span>
<span class="n">default</span> <span class="o">=</span> <span class="n">myproject</span><span class="o">.</span><span class="n">settings</span>
</pre></div>
</div>
</div>
<div class="section" id="id1">
<h4>使用 <tt class="docutils literal"><span class="pre">scrapy</span></tt> 工具<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h4>
<p>您可以以无参数的方式启动Scrapy工具。该命令将会给出一些使用帮助以及可用的命令:</p>
<div class="highlight-python"><div class="highlight"><pre>Scrapy X.Y - no active project

Usage:
  scrapy &lt;command&gt; [options] [args]

Available commands:
  crawl         Run a spider
  fetch         Fetch a URL using the Scrapy downloader
[...]
</pre></div>
</div>
<p>如果您在Scrapy项目中运行，当前激活的项目将会显示在输出的第一行。上面的输出就是响应的例子。如果您在一个项目中运行命令将会得到类似的输出:</p>
<div class="highlight-python"><div class="highlight"><pre>Scrapy X.Y - project: myproject

Usage:
  scrapy &lt;command&gt; [options] [args]

[...]
</pre></div>
</div>
<div class="section" id="id2">
<h5>创建项目<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h5>
<p>一般来说，使用 <tt class="docutils literal"><span class="pre">scrapy</span></tt> 工具的第一件事就是创建您的Scrapy项目:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy startproject myproject
</pre></div>
</div>
<p>该命令将会在 <tt class="docutils literal"><span class="pre">myproject</span></tt> 目录中创建一个Scrapy项目。</p>
<p>接下来，进入到项目目录中:</p>
<div class="highlight-python"><div class="highlight"><pre>cd myproject
</pre></div>
</div>
<p>这时候您就可以使用 <tt class="docutils literal"><span class="pre">scrapy</span></tt> 命令来管理和控制您的项目了。</p>
</div>
<div class="section" id="id3">
<h5>控制项目<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h5>
<p>您可以在您的项目中使用 <tt class="docutils literal"><span class="pre">scrapy</span></tt> 工具来对其进行控制和管理。</p>
<p>比如，创建一个新的spider:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy genspider mydomain mydomain.com
</pre></div>
</div>
<p>有些Scrapy命令(比如 <a class="reference internal" href="index.html#std:command-crawl"><tt class="xref std std-command docutils literal"><span class="pre">crawl</span></tt></a>)要求必须在Scrapy项目中运行。
您可以通过下边的 <a class="reference internal" href="index.html#topics-commands-ref"><em>commands reference</em></a>
来了解哪些命令需要在项目中运行，哪些不用。</p>
<p>另外要注意，有些命令在项目里运行时的效果有些许区别。
以fetch命令为例，如果被爬取的url与某个特定spider相关联，
则该命令将会使用spider的动作(spider-overridden behaviours)。
(比如spider指定的 <tt class="docutils literal"><span class="pre">user_agent</span></tt>)。
该表现是有意而为之的。一般来说， <tt class="docutils literal"><span class="pre">fetch</span></tt> 命令就是用来测试检查spider是如何下载页面。</p>
</div>
</div>
<div class="section" id="tool-commands">
<span id="topics-commands-ref"></span><h4>可用的工具命令(tool commands)<a class="headerlink" href="#tool-commands" title="永久链接至标题">¶</a></h4>
<p>该章节提供了可用的内置命令的列表。每个命令都提供了描述以及一些使用例子。您总是可以通过运行命令来获取关于每个命令的详细内容:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">scrapy</span> <span class="o">&lt;</span><span class="n">command</span><span class="o">&gt;</span> <span class="o">-</span><span class="n">h</span>
</pre></div>
</div>
<p>您也可以查看所有可用的命令:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">scrapy</span> <span class="o">-</span><span class="n">h</span>
</pre></div>
</div>
<p>Scrapy提供了两种类型的命令。一种必须在Scrapy项目中运行(针对项目(Project-specific)的命令)，另外一种则不需要(全局命令)。全局命令在项目中运行时的表现可能会与在非项目中运行有些许差别(因为可能会使用项目的设定)。</p>
<p>全局命令:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:command-startproject"><tt class="xref std std-command docutils literal"><span class="pre">startproject</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-settings"><tt class="xref std std-command docutils literal"><span class="pre">settings</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-runspider"><tt class="xref std std-command docutils literal"><span class="pre">runspider</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-shell"><tt class="xref std std-command docutils literal"><span class="pre">shell</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-fetch"><tt class="xref std std-command docutils literal"><span class="pre">fetch</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-view"><tt class="xref std std-command docutils literal"><span class="pre">view</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-version"><tt class="xref std std-command docutils literal"><span class="pre">version</span></tt></a></li>
</ul>
<p>项目(Project-only)命令:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:command-crawl"><tt class="xref std std-command docutils literal"><span class="pre">crawl</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-check"><tt class="xref std std-command docutils literal"><span class="pre">check</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-list"><tt class="xref std std-command docutils literal"><span class="pre">list</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-edit"><tt class="xref std std-command docutils literal"><span class="pre">edit</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-parse"><tt class="xref std std-command docutils literal"><span class="pre">parse</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-genspider"><tt class="xref std std-command docutils literal"><span class="pre">genspider</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-deploy"><tt class="xref std std-command docutils literal"><span class="pre">deploy</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-bench"><tt class="xref std std-command docutils literal"><span class="pre">bench</span></tt></a></li>
</ul>
<div class="section" id="startproject">
<span id="std:command-startproject"></span><h5>startproject<a class="headerlink" href="#startproject" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>语法: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">startproject</span> <span class="pre">&lt;project_name&gt;</span></tt></li>
<li>是否需要项目: <em>no</em></li>
</ul>
<p>在 <tt class="docutils literal"><span class="pre">project_name</span></tt> 文件夹下创建一个名为 <tt class="docutils literal"><span class="pre">project_name</span></tt> 的Scrapy项目。</p>
<p>例子:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy startproject myproject
</pre></div>
</div>
</div>
<div class="section" id="genspider">
<span id="std:command-genspider"></span><h5>genspider<a class="headerlink" href="#genspider" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>语法: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">genspider</span> <span class="pre">[-t</span> <span class="pre">template]</span> <span class="pre">&lt;name&gt;</span> <span class="pre">&lt;domain&gt;</span></tt></li>
<li>是否需要项目: <em>yes</em></li>
</ul>
<p>在当前项目中创建spider。</p>
<p>这仅仅是创建spider的一种快捷方法。该方法可以使用提前定义好的模板来生成spider。您也可以自己创建spider的源码文件。</p>
<p>例子:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed

$ scrapy genspider -d basic
import scrapy

class $classname(scrapy.Spider):
    name = &quot;$name&quot;
    allowed_domains = [&quot;$domain&quot;]
    start_urls = (
        &#39;http://www.$domain/&#39;,
        )

    def parse(self, response):
        pass

$ scrapy genspider -t basic example example.com
Created spider &#39;example&#39; using template &#39;basic&#39; in module:
  mybot.spiders.example
</pre></div>
</div>
</div>
<div class="section" id="crawl">
<span id="std:command-crawl"></span><h5>crawl<a class="headerlink" href="#crawl" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>语法: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">crawl</span> <span class="pre">&lt;spider&gt;</span></tt></li>
<li>是否需要项目: <em>yes</em></li>
</ul>
<p>使用spider进行爬取。</p>
<p>例子:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy crawl myspider
[ ... myspider starts crawling ... ]
</pre></div>
</div>
</div>
<div class="section" id="check">
<span id="std:command-check"></span><h5>check<a class="headerlink" href="#check" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>语法: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">check</span> <span class="pre">[-l]</span> <span class="pre">&lt;spider&gt;</span></tt></li>
<li>是否需要项目: <em>yes</em></li>
</ul>
<p>运行contract检查。</p>
<p>例子:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy check -l
first_spider
  * parse
  * parse_item
second_spider
  * parse
  * parse_item

$ scrapy check
[FAILED] first_spider:parse_item
&gt;&gt;&gt; &#39;RetailPricex&#39; field is missing

[FAILED] first_spider:parse
&gt;&gt;&gt; Returned 92 requests, expected 0..4
</pre></div>
</div>
</div>
<div class="section" id="list">
<span id="std:command-list"></span><h5>list<a class="headerlink" href="#list" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>语法: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">list</span></tt></li>
<li>是否需要项目: <em>yes</em></li>
</ul>
<p>列出当前项目中所有可用的spider。每行输出一个spider。</p>
<p>使用例子:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy list
spider1
spider2
</pre></div>
</div>
</div>
<div class="section" id="edit">
<span id="std:command-edit"></span><h5>edit<a class="headerlink" href="#edit" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>语法: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">edit</span> <span class="pre">&lt;spider&gt;</span></tt></li>
<li>是否需要项目: <em>yes</em></li>
</ul>
<p>使用 <a class="reference internal" href="index.html#std:setting-EDITOR"><tt class="xref std std-setting docutils literal"><span class="pre">EDITOR</span></tt></a> 中设定的编辑器编辑给定的spider</p>
<p>该命令仅仅是提供一个快捷方式。开发者可以自由选择其他工具或者IDE来编写调试spider。</p>
<p>例子:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy edit spider1
</pre></div>
</div>
</div>
<div class="section" id="fetch">
<span id="std:command-fetch"></span><h5>fetch<a class="headerlink" href="#fetch" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>语法: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">fetch</span> <span class="pre">&lt;url&gt;</span></tt></li>
<li>是否需要项目: <em>no</em></li>
</ul>
<p>使用Scrapy下载器(downloader)下载给定的URL，并将获取到的内容送到标准输出。</p>
<p>该命令以spider下载页面的方式获取页面。例如，如果spider有 <tt class="docutils literal"><span class="pre">USER_AGENT</span></tt> 属性修改了 User Agent，该命令将会使用该属性。</p>
<p>因此，您可以使用该命令来查看spider如何获取某个特定页面。</p>
<p>该命令如果非项目中运行则会使用默认Scrapy downloader设定。</p>
<p>例子:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy fetch --nolog http://www.example.com/some/page.html
[ ... html content here ... ]

$ scrapy fetch --nolog --headers http://www.example.com/
{&#39;Accept-Ranges&#39;: [&#39;bytes&#39;],
 &#39;Age&#39;: [&#39;1263   &#39;],
 &#39;Connection&#39;: [&#39;close     &#39;],
 &#39;Content-Length&#39;: [&#39;596&#39;],
 &#39;Content-Type&#39;: [&#39;text/html; charset=UTF-8&#39;],
 &#39;Date&#39;: [&#39;Wed, 18 Aug 2010 23:59:46 GMT&#39;],
 &#39;Etag&#39;: [&#39;&quot;573c1-254-48c9c87349680&quot;&#39;],
 &#39;Last-Modified&#39;: [&#39;Fri, 30 Jul 2010 15:30:18 GMT&#39;],
 &#39;Server&#39;: [&#39;Apache/2.2.3 (CentOS)&#39;]}
</pre></div>
</div>
</div>
<div class="section" id="view">
<span id="std:command-view"></span><h5>view<a class="headerlink" href="#view" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>语法: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">view</span> <span class="pre">&lt;url&gt;</span></tt></li>
<li>是否需要项目: <em>no</em></li>
</ul>
<p>在浏览器中打开给定的URL，并以Scrapy spider获取到的形式展现。
有些时候spider获取到的页面和普通用户看到的并不相同。
因此该命令可以用来检查spider所获取到的页面，并确认这是您所期望的。</p>
<p>例子:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy view http://www.example.com/some/page.html
[ ... browser starts ... ]
</pre></div>
</div>
</div>
<div class="section" id="shell">
<span id="std:command-shell"></span><h5>shell<a class="headerlink" href="#shell" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>语法: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">shell</span> <span class="pre">[url]</span></tt></li>
<li>是否需要项目: <em>no</em></li>
</ul>
<p>以给定的URL(如果给出)或者空(没有给出URL)启动Scrapy shell。
查看 <a class="reference internal" href="index.html#topics-shell"><em>Scrapy终端(Scrapy shell)</em></a> 获取更多信息。</p>
<p>例子:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy shell http://www.example.com/some/page.html
[ ... scrapy shell starts ... ]
</pre></div>
</div>
</div>
<div class="section" id="parse">
<span id="std:command-parse"></span><h5>parse<a class="headerlink" href="#parse" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>语法: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">parse</span> <span class="pre">&lt;url&gt;</span> <span class="pre">[options]</span></tt></li>
<li>是否需要项目: <em>yes</em></li>
</ul>
<p>获取给定的URL并使用相应的spider分析处理。如果您提供 <tt class="docutils literal"><span class="pre">--callback</span></tt> 选项，则使用spider的该方法处理，否则使用 <tt class="docutils literal"><span class="pre">parse</span></tt> 。</p>
<p>支持的选项:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">--spider=SPIDER</span></tt>: 跳过自动检测spider并强制使用特定的spider</li>
<li><tt class="docutils literal"><span class="pre">--a</span> <span class="pre">NAME=VALUE</span></tt>: 设置spider的参数(可能被重复)</li>
<li><tt class="docutils literal"><span class="pre">--callback</span></tt> or <tt class="docutils literal"><span class="pre">-c</span></tt>: spider中用于解析返回(response)的回调函数</li>
<li><tt class="docutils literal"><span class="pre">--pipelines</span></tt>: 在pipeline中处理item</li>
<li><tt class="docutils literal"><span class="pre">--rules</span></tt> or <tt class="docutils literal"><span class="pre">-r</span></tt>: 使用 <a class="reference internal" href="index.html#scrapy.contrib.spiders.CrawlSpider" title="scrapy.contrib.spiders.CrawlSpider"><tt class="xref py py-class docutils literal"><span class="pre">CrawlSpider</span></tt></a> 规则来发现用来解析返回(response)的回调函数</li>
<li><tt class="docutils literal"><span class="pre">--noitems</span></tt>: 不显示爬取到的item</li>
<li><tt class="docutils literal"><span class="pre">--nolinks</span></tt>: 不显示提取到的链接</li>
<li><tt class="docutils literal"><span class="pre">--nocolour</span></tt>: 避免使用pygments对输出着色</li>
<li><tt class="docutils literal"><span class="pre">--depth</span></tt> or <tt class="docutils literal"><span class="pre">-d</span></tt>: 指定跟进链接请求的层次数(默认: 1)</li>
<li><tt class="docutils literal"><span class="pre">--verbose</span></tt> or <tt class="docutils literal"><span class="pre">-v</span></tt>: 显示每个请求的详细信息</li>
</ul>
<p>例子:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy parse http://www.example.com/ -c parse_item
[ ... scrapy log lines crawling example.com spider ... ]

&gt;&gt;&gt; STATUS DEPTH LEVEL 1 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[{&#39;name&#39;: u&#39;Example item&#39;,
 &#39;category&#39;: u&#39;Furniture&#39;,
 &#39;length&#39;: u&#39;12 cm&#39;}]

# Requests  -----------------------------------------------------------------
[]
</pre></div>
</div>
</div>
<div class="section" id="settings">
<span id="std:command-settings"></span><h5>settings<a class="headerlink" href="#settings" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>语法: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">settings</span> <span class="pre">[options]</span></tt></li>
<li>是否需要项目: <em>no</em></li>
</ul>
<p>获取Scrapy的设定</p>
<p>在项目中运行时，该命令将会输出项目的设定值，否则输出Scrapy默认设定。</p>
<p>例子:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy settings --get BOT_NAME
scrapybot
$ scrapy settings --get DOWNLOAD_DELAY
0
</pre></div>
</div>
</div>
<div class="section" id="runspider">
<span id="std:command-runspider"></span><h5>runspider<a class="headerlink" href="#runspider" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>语法: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">runspider</span> <span class="pre">&lt;spider_file.py&gt;</span></tt></li>
<li>是否需要项目: <em>no</em></li>
</ul>
<p>在未创建项目的情况下，运行一个编写在Python文件中的spider。</p>
<p>例子:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy runspider myspider.py
[ ... spider starts crawling ... ]
</pre></div>
</div>
</div>
<div class="section" id="version">
<span id="std:command-version"></span><h5>version<a class="headerlink" href="#version" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>语法: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">version</span> <span class="pre">[-v]</span></tt></li>
<li>是否需要项目: <em>no</em></li>
</ul>
<p>输出Scrapy版本。配合 <tt class="docutils literal"><span class="pre">-v</span></tt> 运行时，该命令同时输出Python, Twisted以及平台的信息，方便bug提交。</p>
</div>
<div class="section" id="deploy">
<span id="std:command-deploy"></span><h5>deploy<a class="headerlink" href="#deploy" title="永久链接至标题">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">0.11 新版功能.</span></p>
</div>
<ul class="simple">
<li>语法: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">deploy</span> <span class="pre">[</span> <span class="pre">&lt;target:project&gt;</span> <span class="pre">|</span> <span class="pre">-l</span> <span class="pre">&lt;target&gt;</span> <span class="pre">|</span> <span class="pre">-L</span> <span class="pre">]</span></tt></li>
<li>是否需要项目: <em>yes</em></li>
</ul>
<p>将项目部署到Scrapyd服务。查看 <a class="reference external" href="http://scrapyd.readthedocs.org/en/latest/deploy.html">部署您的项目</a> 。</p>
</div>
<div class="section" id="bench">
<span id="std:command-bench"></span><h5>bench<a class="headerlink" href="#bench" title="永久链接至标题">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">0.17 新版功能.</span></p>
</div>
<ul class="simple">
<li>语法: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">bench</span></tt></li>
<li>是否需要项目: <em>no</em></li>
</ul>
<p>运行benchmark测试。 <a class="reference internal" href="index.html#benchmarking"><em>Benchmarking</em></a> 。</p>
</div>
</div>
<div class="section" id="id4">
<h4>自定义项目命令<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h4>
<p>您也可以通过 <a class="reference internal" href="index.html#std:setting-COMMANDS_MODULE"><tt class="xref std std-setting docutils literal"><span class="pre">COMMANDS_MODULE</span></tt></a> 来添加您自己的项目命令。您可以以 <a class="reference external" href="https://github.com/scrapy/scrapy/blob/master/scrapy/commands">scrapy/commands</a> 中Scrapy commands为例来了解如何实现您的命令。</p>
<div class="section" id="commands-module">
<span id="std:setting-COMMANDS_MODULE"></span><h5>COMMANDS_MODULE<a class="headerlink" href="#commands-module" title="永久链接至标题">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">''</span></tt> (empty string)</p>
<p>用于查找添加自定义Scrapy命令的模块。</p>
<p>例子:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">COMMANDS_MODULE</span> <span class="o">=</span> <span class="s">&#39;mybot.commands&#39;</span>
</pre></div>
</div>
</div>
</div>
</div>
<span id="document-topics/items"></span><div class="section" id="module-scrapy.item">
<span id="items"></span><span id="topics-items"></span><h3>Items<a class="headerlink" href="#module-scrapy.item" title="永久链接至标题">¶</a></h3>
<p>爬取的主要目标就是从非结构性的数据源提取结构性数据，例如网页。
Scrapy提供 <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> 类来满足这样的需求。</p>
<p><a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> 对象是种简单的容器，保存了爬取到得数据。
其提供了 <a class="reference external" href="http://docs.python.org/library/stdtypes.html#dict">类似于词典(dictionary-like)</a> 的API以及用于声明可用字段的简单语法。</p>
<div class="section" id="item">
<span id="topics-items-declaring"></span><h4>声明Item<a class="headerlink" href="#item" title="永久链接至标题">¶</a></h4>
<p>Item使用简单的class定义语法以及 <a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a> 对象来声明。例如:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">Product</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">price</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">stock</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">last_updated</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">serializer</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">熟悉 <a class="reference external" href="http://www.djangoproject.com/">Django</a> 的朋友一定会注意到Scrapy Item定义方式与 <a class="reference external" href="http://docs.djangoproject.com/en/dev/topics/db/models/">Django Models</a> 很类似, 不过没有那么多不同的字段类型(Field type)，更为简单。</p>
</div>
</div>
<div class="section" id="item-item-fields">
<span id="topics-items-fields"></span><h4>Item字段(Item Fields)<a class="headerlink" href="#item-item-fields" title="永久链接至标题">¶</a></h4>
<p><a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a> 对象指明了每个字段的元数据(metadata)。例如下面例子中 <tt class="docutils literal"><span class="pre">last_updated</span></tt> 中指明了该字段的序列化函数。</p>
<p>您可以为每个字段指明任何类型的元数据。
<a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a> 对象对接受的值没有任何限制。也正是因为这个原因，文档也无法提供所有可用的元数据的键(key)参考列表。
<a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a> 对象中保存的每个键可以由多个组件使用，并且只有这些组件知道这个键的存在。您可以根据自己的需求，定义使用其他的 <a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a> 键。
设置 <a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a> 对象的主要目的就是在一个地方定义好所有的元数据。
一般来说，那些依赖某个字段的组件肯定使用了特定的键(key)。您必须查看组件相关的文档，查看其用了哪些元数据键(metadata key)。</p>
<p>需要注意的是，用来声明item的 <a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a> 对象并没有被赋值为class的属性。
不过您可以通过 <a class="reference internal" href="index.html#scrapy.item.Item.fields" title="scrapy.item.Item.fields"><tt class="xref py py-attr docutils literal"><span class="pre">Item.fields</span></tt></a> 属性进行访问。</p>
<p>以上就是所有您需要知道的如何声明item的内容了。</p>
</div>
<div class="section" id="id1">
<h4>与Item配合<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h4>
<p>接下来以 <a class="reference internal" href="index.html#topics-items-declaring"><em>下边声明</em></a> 的 <tt class="docutils literal"><span class="pre">Product</span></tt> item来演示一些item的操作。您会发现API和 <a class="reference external" href="http://docs.python.org/library/stdtypes.html#dict">dict API</a> 非常相似。</p>
<div class="section" id="id2">
<h5>创建item<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h5>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">product</span> <span class="o">=</span> <span class="n">Product</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">&#39;Desktop PC&#39;</span><span class="p">,</span> <span class="n">price</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">product</span>
<span class="go">Product(name=&#39;Desktop PC&#39;, price=1000)</span>
</pre></div>
</div>
</div>
<div class="section" id="id3">
<h5>获取字段的值<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h5>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">]</span>
<span class="go">Desktop PC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">)</span>
<span class="go">Desktop PC</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s">&#39;price&#39;</span><span class="p">]</span>
<span class="go">1000</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s">&#39;last_updated&#39;</span><span class="p">]</span>
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">KeyError</span>: <span class="n">&#39;last_updated&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;last_updated&#39;</span><span class="p">,</span> <span class="s">&#39;not set&#39;</span><span class="p">)</span>
<span class="go">not set</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s">&#39;lala&#39;</span><span class="p">]</span> <span class="c"># getting unknown field</span>
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">KeyError</span>: <span class="n">&#39;lala&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;lala&#39;</span><span class="p">,</span> <span class="s">&#39;unknown field&#39;</span><span class="p">)</span>
<span class="go">&#39;unknown field&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="s">&#39;name&#39;</span> <span class="ow">in</span> <span class="n">product</span>  <span class="c"># is name field populated?</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="s">&#39;last_updated&#39;</span> <span class="ow">in</span> <span class="n">product</span>  <span class="c"># is last_updated populated?</span>
<span class="go">False</span>

<span class="gp">&gt;&gt;&gt; </span><span class="s">&#39;last_updated&#39;</span> <span class="ow">in</span> <span class="n">product</span><span class="o">.</span><span class="n">fields</span>  <span class="c"># is last_updated a declared field?</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="s">&#39;lala&#39;</span> <span class="ow">in</span> <span class="n">product</span><span class="o">.</span><span class="n">fields</span>  <span class="c"># is lala a declared field?</span>
<span class="go">False</span>
</pre></div>
</div>
</div>
<div class="section" id="id4">
<h5>设置字段的值<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h5>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s">&#39;last_updated&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s">&#39;today&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s">&#39;last_updated&#39;</span><span class="p">]</span>
<span class="go">today</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s">&#39;lala&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s">&#39;test&#39;</span> <span class="c"># setting unknown field</span>
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">KeyError</span>: <span class="n">&#39;Product does not support field: lala&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="id5">
<h5>获取所有获取到的值<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h5>
<p>您可以使用 <a class="reference external" href="http://docs.python.org/library/stdtypes.html#dict">dict API</a> 来获取所有的值:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;price&#39;, &#39;name&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
<span class="go">[(&#39;price&#39;, 1000), (&#39;name&#39;, &#39;Desktop PC&#39;)]</span>
</pre></div>
</div>
</div>
<div class="section" id="id6">
<h5>其他任务<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h5>
<p>复制item:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">product2</span> <span class="o">=</span> <span class="n">Product</span><span class="p">(</span><span class="n">product</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">product2</span>
<span class="go">Product(name=&#39;Desktop PC&#39;, price=1000)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product3</span> <span class="o">=</span> <span class="n">product2</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">product3</span>
<span class="go">Product(name=&#39;Desktop PC&#39;, price=1000)</span>
</pre></div>
</div>
<p>根据item创建字典(dict):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="nb">dict</span><span class="p">(</span><span class="n">product</span><span class="p">)</span> <span class="c"># create a dict from all populated values</span>
<span class="go">{&#39;price&#39;: 1000, &#39;name&#39;: &#39;Desktop PC&#39;}</span>
</pre></div>
</div>
<p>根据字典(dict)创建item:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Product</span><span class="p">({</span><span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">&#39;Laptop PC&#39;</span><span class="p">,</span> <span class="s">&#39;price&#39;</span><span class="p">:</span> <span class="mi">1500</span><span class="p">})</span>
<span class="go">Product(price=1500, name=&#39;Laptop PC&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">Product</span><span class="p">({</span><span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">&#39;Laptop PC&#39;</span><span class="p">,</span> <span class="s">&#39;lala&#39;</span><span class="p">:</span> <span class="mi">1500</span><span class="p">})</span> <span class="c"># warning: unknown field in dict</span>
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">KeyError</span>: <span class="n">&#39;Product does not support field: lala&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id7">
<h4>扩展Item<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h4>
<p>您可以通过继承原始的Item来扩展item(添加更多的字段或者修改某些字段的元数据)。</p>
<p>例如:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">DiscountedProduct</span><span class="p">(</span><span class="n">Product</span><span class="p">):</span>
    <span class="n">discount_percent</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">serializer</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
    <span class="n">discount_expiration_date</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
<p>您也可以通过使用原字段的元数据,添加新的值或修改原来的值来扩展字段的元数据:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">SpecificProduct</span><span class="p">(</span><span class="n">Product</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">Product</span><span class="o">.</span><span class="n">fields</span><span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">],</span> <span class="n">serializer</span><span class="o">=</span><span class="n">my_serializer</span><span class="p">)</span>
</pre></div>
</div>
<p>这段代码在保留所有原来的元数据值的情况下添加(或者覆盖)了 <tt class="docutils literal"><span class="pre">name</span></tt> 字段的 <tt class="docutils literal"><span class="pre">serializer</span></tt> 。</p>
</div>
<div class="section" id="id8">
<h4>Item对象<a class="headerlink" href="#id8" title="永久链接至标题">¶</a></h4>
<dl class="class">
<dt id="scrapy.item.Item">
<em class="property">class </em><tt class="descclassname">scrapy.item.</tt><tt class="descname">Item</tt><big>(</big><span class="optional">[</span><em>arg</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.item.Item" title="永久链接至目标">¶</a></dt>
<dd><p>返回一个根据给定的参数可选初始化的item。</p>
<p>Item复制了标准的 <a class="reference external" href="http://docs.python.org/library/stdtypes.html#dict">dict API</a> 。包括初始化函数也相同。Item唯一额外添加的属性是:</p>
<dl class="attribute">
<dt id="scrapy.item.Item.fields">
<tt class="descname">fields</tt><a class="headerlink" href="#scrapy.item.Item.fields" title="永久链接至目标">¶</a></dt>
<dd><p>一个包含了item所有声明的字段的字典，而不仅仅是获取到的字段。该字典的key是字段(field)的名字，值是 <a class="reference internal" href="index.html#topics-items-declaring"><em>Item声明</em></a> 中使用到的 <a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a> 对象。</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="field">
<h4>字段(Field)对象<a class="headerlink" href="#field" title="永久链接至标题">¶</a></h4>
<dl class="class">
<dt id="scrapy.item.Field">
<em class="property">class </em><tt class="descclassname">scrapy.item.</tt><tt class="descname">Field</tt><big>(</big><span class="optional">[</span><em>arg</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.item.Field" title="永久链接至目标">¶</a></dt>
<dd><p><a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a> 仅仅是内置的 <a class="reference external" href="http://docs.python.org/library/stdtypes.html#dict">dict</a> 类的一个别名，并没有提供额外的方法或者属性。换句话说， <a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a> 对象完完全全就是Python字典(dict)。被用来基于类属性(class attribute)的方法来支持 <a class="reference internal" href="index.html#topics-items-declaring"><em>item声明语法</em></a> 。</p>
</dd></dl>

</div>
</div>
<span id="document-topics/spiders"></span><div class="section" id="spiders">
<span id="topics-spiders"></span><h3>Spiders<a class="headerlink" href="#spiders" title="永久链接至标题">¶</a></h3>
<p>Spider类定义了如何爬取某个(或某些)网站。包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。
换句话说，Spider就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。</p>
<p>对spider来说，爬取的循环类似下文:</p>
<ol class="arabic">
<li><p class="first">以初始的URL初始化Request，并设置回调函数。
当该request下载完毕并返回时，将生成response，并作为参数传给该回调函数。</p>
<p>spider中初始的request是通过调用 <a class="reference internal" href="index.html#scrapy.spider.Spider.start_requests" title="scrapy.spider.Spider.start_requests"><tt class="xref py py-meth docutils literal"><span class="pre">start_requests()</span></tt></a> 来获取的。
<a class="reference internal" href="index.html#scrapy.spider.Spider.start_requests" title="scrapy.spider.Spider.start_requests"><tt class="xref py py-meth docutils literal"><span class="pre">start_requests()</span></tt></a> 读取 <a class="reference internal" href="index.html#scrapy.spider.Spider.start_urls" title="scrapy.spider.Spider.start_urls"><tt class="xref py py-attr docutils literal"><span class="pre">start_urls</span></tt></a> 中的URL，
并以 <a class="reference internal" href="index.html#scrapy.spider.Spider.parse" title="scrapy.spider.Spider.parse"><tt class="xref py py-attr docutils literal"><span class="pre">parse</span></tt></a> 为回调函数生成 <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 。</p>
</li>
<li><p class="first">在回调函数内分析返回的(网页)内容，返回 <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> 对象或者 <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 或者一个包括二者的可迭代容器。
返回的Request对象之后会经过Scrapy处理，下载相应的内容，并调用设置的callback函数(函数可相同)。</p>
</li>
<li><p class="first">在回调函数内，您可以使用 <a class="reference internal" href="index.html#topics-selectors"><em>选择器(Selectors)</em></a>
(您也可以使用BeautifulSoup, lxml 或者您想用的任何解析器) 来分析网页内容，并根据分析的数据生成item。</p>
</li>
<li><p class="first">最后，由spider返回的item将被存到数据库(由某些
<a class="reference internal" href="index.html#topics-item-pipeline"><em>Item Pipeline</em></a> 处理)或使用
<a class="reference internal" href="index.html#topics-feed-exports"><em>Feed exports</em></a> 存入到文件中。</p>
</li>
</ol>
<p>虽然该循环对任何类型的spider都(多少)适用，但Scrapy仍然为了不同的需求提供了多种默认spider。
之后将讨论这些spider。</p>
<div class="section" id="spider">
<span id="spiderargs"></span><h4>Spider参数<a class="headerlink" href="#spider" title="永久链接至标题">¶</a></h4>
<p>Spider可以通过接受参数来修改其功能。
spider参数一般用来定义初始URL或者指定限制爬取网站的部分。
您也可以使用其来配置spider的任何功能。</p>
<p>在运行 <a class="reference internal" href="index.html#std:command-crawl"><tt class="xref std std-command docutils literal"><span class="pre">crawl</span></tt></a> 时添加 <tt class="docutils literal"><span class="pre">-a</span></tt> 可以传递Spider参数:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy crawl myspider -a category=electronics
</pre></div>
</div>
<p>Spider在构造器(constructor)中获取参数:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;myspider&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MySpider</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.example.com/categories/</span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">category</span><span class="p">]</span>
        <span class="c"># ...</span>
</pre></div>
</div>
<p>Spider参数也可以通过Scrapyd的 <tt class="docutils literal"><span class="pre">schedule.json</span></tt> API来传递。
参见 <a class="reference external" href="http://scrapyd.readthedocs.org/">Scrapyd documentation</a>.</p>
</div>
<div class="section" id="topics-spiders-ref">
<span id="id1"></span><h4>内置Spider参考手册<a class="headerlink" href="#topics-spiders-ref" title="永久链接至标题">¶</a></h4>
<p>Scrapy提供多种方便的通用spider供您继承使用。
这些spider为一些常用的爬取情况提供方便的特性，
例如根据某些规则跟进某个网站的所有链接、根据 <a class="reference external" href="http://www.sitemaps.org">Sitemaps</a> 来进行爬取，或者分析XML/CSV源。</p>
<p>下面spider的示例中，我们假定您有个项目在 <tt class="docutils literal"><span class="pre">myproject.items</span></tt> 模块中声明了 <tt class="docutils literal"><span class="pre">TestItem</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">TestItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="nb">id</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">description</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
<span class="target" id="module-scrapy.spider"></span><div class="section" id="id2">
<h5>Spider<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.spider.Spider">
<em class="property">class </em><tt class="descclassname">scrapy.spider.</tt><tt class="descname">Spider</tt><a class="headerlink" href="#scrapy.spider.Spider" title="永久链接至目标">¶</a></dt>
<dd><p>Spider是最简单的spider。每个其他的spider必须继承自该类(包括Scrapy自带的其他spider以及您自己编写的spider)。
Spider并没有提供什么特殊的功能。
其仅仅请求给定的 <tt class="docutils literal"><span class="pre">start_urls</span></tt>/<tt class="docutils literal"><span class="pre">start_requests</span></tt> ，并根据返回的结果(resulting responses)调用spider的 <tt class="docutils literal"><span class="pre">parse</span></tt> 方法。</p>
<dl class="attribute">
<dt id="scrapy.spider.Spider.name">
<tt class="descname">name</tt><a class="headerlink" href="#scrapy.spider.Spider.name" title="永久链接至目标">¶</a></dt>
<dd><p>定义spider名字的字符串(string)。spider的名字定义了Scrapy如何定位(并初始化)spider，所以其必须是唯一的。
不过您可以生成多个相同的spider实例(instance)，这没有任何限制。
name是spider最重要的属性，而且是必须的。</p>
<p>如果该spider爬取单个网站(single domain)，一个常见的做法是以该网站(domain)(加或不加 <a class="reference external" href="http://en.wikipedia.org/wiki/Top-level_domain">后缀</a> )来命名spider。
例如，如果spider爬取 <tt class="docutils literal"><span class="pre">mywebsite.com</span></tt> ，该spider通常会被命名为 <tt class="docutils literal"><span class="pre">mywebsite</span></tt> 。</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spider.Spider.allowed_domains">
<tt class="descname">allowed_domains</tt><a class="headerlink" href="#scrapy.spider.Spider.allowed_domains" title="永久链接至目标">¶</a></dt>
<dd><p>可选。包含了spider允许爬取的域名(domain)列表(list)。
当 <a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware" title="scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">OffsiteMiddleware</span></tt></a> 启用时，
域名不在列表中的URL不会被跟进。</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spider.Spider.start_urls">
<tt class="descname">start_urls</tt><a class="headerlink" href="#scrapy.spider.Spider.start_urls" title="永久链接至目标">¶</a></dt>
<dd><p>URL列表。当没有制定特定的URL时，spider将从该列表中开始进行爬取。
因此，第一个被获取到的页面的URL将是该列表之一。
后续的URL将会从获取到的数据中提取。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.spider.Spider.start_requests">
<tt class="descname">start_requests</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.spider.Spider.start_requests" title="永久链接至目标">¶</a></dt>
<dd><p>该方法必须返回一个可迭代对象(iterable)。该对象包含了spider用于爬取的第一个Request。</p>
<p>当spider启动爬取并且未制定URL时，该方法被调用。
当指定了URL时，<a class="reference internal" href="index.html#scrapy.spider.Spider.make_requests_from_url" title="scrapy.spider.Spider.make_requests_from_url"><tt class="xref py py-meth docutils literal"><span class="pre">make_requests_from_url()</span></tt></a> 将被调用来创建Request对象。
该方法仅仅会被Scrapy调用一次，因此您可以将其实现为生成器。</p>
<p>该方法的默认实现是使用 <a class="reference internal" href="index.html#scrapy.spider.Spider.start_urls" title="scrapy.spider.Spider.start_urls"><tt class="xref py py-attr docutils literal"><span class="pre">start_urls</span></tt></a> 的url生成Request。</p>
<p>如果您想要修改最初爬取某个网站的Request对象，您可以重写(override)该方法。
例如，如果您需要在启动时以POST登录某个网站，你可以这么写:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">FormRequest</span><span class="p">(</span><span class="s">&quot;http://www.example.com/login&quot;</span><span class="p">,</span>
                               <span class="n">formdata</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;user&#39;</span><span class="p">:</span> <span class="s">&#39;john&#39;</span><span class="p">,</span> <span class="s">&#39;pass&#39;</span><span class="p">:</span> <span class="s">&#39;secret&#39;</span><span class="p">},</span>
                               <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">logged_in</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">logged_in</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="c"># here you would extract links to follow and return Requests for</span>
    <span class="c"># each of them, with another callback</span>
    <span class="k">pass</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.spider.Spider.make_requests_from_url">
<tt class="descname">make_requests_from_url</tt><big>(</big><em>url</em><big>)</big><a class="headerlink" href="#scrapy.spider.Spider.make_requests_from_url" title="永久链接至目标">¶</a></dt>
<dd><p>该方法接受一个URL并返回用于爬取的 <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象。
该方法在初始化request时被 <a class="reference internal" href="index.html#scrapy.spider.Spider.start_requests" title="scrapy.spider.Spider.start_requests"><tt class="xref py py-meth docutils literal"><span class="pre">start_requests()</span></tt></a> 调用，也被用于转化url为request。</p>
<p>默认未被复写(overridden)的情况下，该方法返回的Request对象中，
<a class="reference internal" href="index.html#scrapy.spider.Spider.parse" title="scrapy.spider.Spider.parse"><tt class="xref py py-meth docutils literal"><span class="pre">parse()</span></tt></a> 作为回调函数，dont_filter参数也被设置为开启。
(详情参见 <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a>).</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.spider.Spider.parse">
<tt class="descname">parse</tt><big>(</big><em>response</em><big>)</big><a class="headerlink" href="#scrapy.spider.Spider.parse" title="永久链接至目标">¶</a></dt>
<dd><p>当response没有指定回调函数时，该方法是Scrapy处理下载的response的默认方法。</p>
<p><tt class="docutils literal"><span class="pre">parse</span></tt> 负责处理response并返回处理的数据以及(/或)跟进的URL。
<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> 对其他的Request的回调函数也有相同的要求。</p>
<p>该方法及其他的Request回调函数必须返回一个包含
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 及(或) <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a>
的可迭代的对象。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a>) &#8211; 用于分析的response</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.spider.Spider.log">
<tt class="descname">log</tt><big>(</big><em>message</em><span class="optional">[</span>, <em>level</em>, <em>component</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.spider.Spider.log" title="永久链接至目标">¶</a></dt>
<dd><p>使用 <a class="reference internal" href="index.html#scrapy.log.msg" title="scrapy.log.msg"><tt class="xref py py-func docutils literal"><span class="pre">scrapy.log.msg()</span></tt></a> 方法记录(log)message。
log中自动带上该spider的 <a class="reference internal" href="index.html#scrapy.spider.Spider.name" title="scrapy.spider.Spider.name"><tt class="xref py py-attr docutils literal"><span class="pre">name</span></tt></a> 属性。
更多数据请参见 <a class="reference internal" href="index.html#topics-logging"><em>Logging</em></a> 。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.spider.Spider.closed">
<tt class="descname">closed</tt><big>(</big><em>reason</em><big>)</big><a class="headerlink" href="#scrapy.spider.Spider.closed" title="永久链接至目标">¶</a></dt>
<dd><p>当spider关闭时，该函数被调用。
该方法提供了一个替代调用signals.connect()来监听 <a class="reference internal" href="index.html#std:signal-spider_closed"><tt class="xref std std-signal docutils literal"><span class="pre">spider_closed</span></tt></a> 信号的快捷方式。</p>
</dd></dl>

</dd></dl>

<div class="section" id="id3">
<h6>Spider样例<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h6>
<p>让我们来看一个例子:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;example.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">&#39;http://www.example.com/1.html&#39;</span><span class="p">,</span>
        <span class="s">&#39;http://www.example.com/2.html&#39;</span><span class="p">,</span>
        <span class="s">&#39;http://www.example.com/3.html&#39;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s">&#39;A response from </span><span class="si">%s</span><span class="s"> just arrived!&#39;</span> <span class="o">%</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
<p>另一个在单个回调函数中返回多个Request以及Item的例子:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="kn">import</span> <span class="n">MyItem</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;example.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">&#39;http://www.example.com/1.html&#39;</span><span class="p">,</span>
        <span class="s">&#39;http://www.example.com/2.html&#39;</span><span class="p">,</span>
        <span class="s">&#39;http://www.example.com/3.html&#39;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">sel</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Selector</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">h3</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//h3&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">():</span>
            <span class="k">yield</span> <span class="n">MyItem</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">h3</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">():</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<span class="target" id="module-scrapy.contrib.spiders"></span></div>
</div>
<div class="section" id="crawlspider">
<h5>CrawlSpider<a class="headerlink" href="#crawlspider" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.spiders.CrawlSpider">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spiders.</tt><tt class="descname">CrawlSpider</tt><a class="headerlink" href="#scrapy.contrib.spiders.CrawlSpider" title="永久链接至目标">¶</a></dt>
<dd><p>爬取一般网站常用的spider。其定义了一些规则(rule)来提供跟进link的方便的机制。
也许该spider并不是完全适合您的特定网站或项目，但其对很多情况都使用。
因此您可以以其为起点，根据需求修改部分方法。当然您也可以实现自己的spider。</p>
<p>除了从Spider继承过来的(您必须提供的)属性外，其提供了一个新的属性:</p>
<dl class="attribute">
<dt id="scrapy.contrib.spiders.CrawlSpider.rules">
<tt class="descname">rules</tt><a class="headerlink" href="#scrapy.contrib.spiders.CrawlSpider.rules" title="永久链接至目标">¶</a></dt>
<dd><p>一个包含一个(或多个) <a class="reference internal" href="index.html#scrapy.contrib.spiders.Rule" title="scrapy.contrib.spiders.Rule"><tt class="xref py py-class docutils literal"><span class="pre">Rule</span></tt></a> 对象的集合(list)。
每个 <a class="reference internal" href="index.html#scrapy.contrib.spiders.Rule" title="scrapy.contrib.spiders.Rule"><tt class="xref py py-class docutils literal"><span class="pre">Rule</span></tt></a> 对爬取网站的动作定义了特定表现。
Rule对象在下边会介绍。
如果多个rule匹配了相同的链接，则根据他们在本属性中被定义的顺序，第一个会被使用。</p>
</dd></dl>

<p>该spider也提供了一个可复写(overrideable)的方法:</p>
<dl class="method">
<dt id="scrapy.contrib.spiders.CrawlSpider.parse_start_url">
<tt class="descname">parse_start_url</tt><big>(</big><em>response</em><big>)</big><a class="headerlink" href="#scrapy.contrib.spiders.CrawlSpider.parse_start_url" title="永久链接至目标">¶</a></dt>
<dd><p>当start_url的请求返回时，该方法被调用。
该方法分析最初的返回值并必须返回一个
<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> 对象或者
一个 <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象或者
一个可迭代的包含二者对象。</p>
</dd></dl>

</dd></dl>

<div class="section" id="crawling-rules">
<h6>爬取规则(Crawling rules)<a class="headerlink" href="#crawling-rules" title="永久链接至标题">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.spiders.Rule">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spiders.</tt><tt class="descname">Rule</tt><big>(</big><em>link_extractor</em>, <em>callback=None</em>, <em>cb_kwargs=None</em>, <em>follow=None</em>, <em>process_links=None</em>, <em>process_request=None</em><big>)</big><a class="headerlink" href="#scrapy.contrib.spiders.Rule" title="永久链接至目标">¶</a></dt>
<dd><p><tt class="docutils literal"><span class="pre">link_extractor</span></tt> 是一个 <a class="reference internal" href="index.html#topics-link-extractors"><em>Link Extractor</em></a> 对象。
其定义了如何从爬取到的页面提取链接。</p>
<p><tt class="docutils literal"><span class="pre">callback</span></tt> 是一个callable或string(该spider中同名的函数将会被调用)。
从link_extractor中每获取到链接时将会调用该函数。该回调函数接受一个response作为其第一个参数，
并返回一个包含 <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> 以及(或) <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象(或者这两者的子类)的列表(list)。</p>
<div class="admonition warning">
<p class="first admonition-title">警告</p>
<p class="last">当编写爬虫规则时，请避免使用 <tt class="docutils literal"><span class="pre">parse</span></tt> 作为回调函数。
由于 <a class="reference internal" href="index.html#scrapy.contrib.spiders.CrawlSpider" title="scrapy.contrib.spiders.CrawlSpider"><tt class="xref py py-class docutils literal"><span class="pre">CrawlSpider</span></tt></a> 使用 <tt class="docutils literal"><span class="pre">parse</span></tt> 方法来实现其逻辑，如果
您覆盖了 <tt class="docutils literal"><span class="pre">parse</span></tt> 方法，crawl spider 将会运行失败。</p>
</div>
<p><tt class="docutils literal"><span class="pre">cb_kwargs</span></tt> 包含传递给回调函数的参数(keyword argument)的字典。</p>
<p><tt class="docutils literal"><span class="pre">follow</span></tt> 是一个布尔(boolean)值，指定了根据该规则从response提取的链接是否需要跟进。
如果 <tt class="docutils literal"><span class="pre">callback</span></tt> 为None， <tt class="docutils literal"><span class="pre">follow</span></tt> 默认设置为 <tt class="docutils literal"><span class="pre">True</span></tt> ，否则默认为 <tt class="docutils literal"><span class="pre">False</span></tt> 。</p>
<p><tt class="docutils literal"><span class="pre">process_links</span></tt> 是一个callable或string(该spider中同名的函数将会被调用)。
从link_extractor中获取到链接列表时将会调用该函数。该方法主要用来过滤。</p>
<p><tt class="docutils literal"><span class="pre">process_request</span></tt> 是一个callable或string(该spider中同名的函数将会被调用)。
该规则提取到每个request时都会调用该函数。该函数必须返回一个request或者None。
(用来过滤request)</p>
</dd></dl>

</div>
<div class="section" id="id4">
<h6>CrawlSpider样例<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h6>
<p>接下来给出配合rule使用CrawlSpider的例子:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">scrapy.contrib.spiders</span> <span class="kn">import</span> <span class="n">CrawlSpider</span><span class="p">,</span> <span class="n">Rule</span>
<span class="kn">from</span> <span class="nn">scrapy.contrib.linkextractors</span> <span class="kn">import</span> <span class="n">LinkExtractor</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;example.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.example.com&#39;</span><span class="p">]</span>

    <span class="n">rules</span> <span class="o">=</span> <span class="p">(</span>
        <span class="c"># 提取匹配 &#39;category.php&#39; (但不匹配 &#39;subsection.php&#39;) 的链接并跟进链接(没有callback意味着follow默认为True)</span>
        <span class="n">Rule</span><span class="p">(</span><span class="n">LinkExtractor</span><span class="p">(</span><span class="n">allow</span><span class="o">=</span><span class="p">(</span><span class="s">&#39;category\.php&#39;</span><span class="p">,</span> <span class="p">),</span> <span class="n">deny</span><span class="o">=</span><span class="p">(</span><span class="s">&#39;subsection\.php&#39;</span><span class="p">,</span> <span class="p">))),</span>

        <span class="c"># 提取匹配 &#39;item.php&#39; 的链接并使用spider的parse_item方法进行分析</span>
        <span class="n">Rule</span><span class="p">(</span><span class="n">LinkExtractor</span><span class="p">(</span><span class="n">allow</span><span class="o">=</span><span class="p">(</span><span class="s">&#39;item\.php&#39;</span><span class="p">,</span> <span class="p">)),</span> <span class="n">callback</span><span class="o">=</span><span class="s">&#39;parse_item&#39;</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s">&#39;Hi, this is an item page! </span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>

        <span class="n">item</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s">&#39;id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//td[@id=&quot;item_id&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="s">r&#39;ID: (\d+)&#39;</span><span class="p">)</span>
        <span class="n">item</span><span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//td[@id=&quot;item_name&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s">&#39;description&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//td[@id=&quot;item_description&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>该spider将从example.com的首页开始爬取，获取category以及item的链接并对后者使用 <tt class="docutils literal"><span class="pre">parse_item</span></tt> 方法。
当item获得返回(response)时，将使用XPath处理HTML并生成一些数据填入 <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> 中。</p>
</div>
</div>
<div class="section" id="xmlfeedspider">
<h5>XMLFeedSpider<a class="headerlink" href="#xmlfeedspider" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.spiders.XMLFeedSpider">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spiders.</tt><tt class="descname">XMLFeedSpider</tt><a class="headerlink" href="#scrapy.contrib.spiders.XMLFeedSpider" title="永久链接至目标">¶</a></dt>
<dd><p>XMLFeedSpider被设计用于通过迭代各个节点来分析XML源(XML feed)。
迭代器可以从 <tt class="docutils literal"><span class="pre">iternodes</span></tt> ， <tt class="docutils literal"><span class="pre">xml</span></tt> ， <tt class="docutils literal"><span class="pre">html</span></tt> 选择。
鉴于 <tt class="docutils literal"><span class="pre">xml</span></tt> 以及 <tt class="docutils literal"><span class="pre">html</span></tt> 迭代器需要先读取所有DOM再分析而引起的性能问题，
一般还是推荐使用 <tt class="docutils literal"><span class="pre">iternodes</span></tt> 。
不过使用 <tt class="docutils literal"><span class="pre">html</span></tt> 作为迭代器能有效应对错误的XML。</p>
<p>您必须定义下列类属性来设置迭代器以及标签名(tag name):</p>
<dl class="attribute">
<dt id="scrapy.contrib.spiders.XMLFeedSpider.iterator">
<tt class="descname">iterator</tt><a class="headerlink" href="#scrapy.contrib.spiders.XMLFeedSpider.iterator" title="永久链接至目标">¶</a></dt>
<dd><p>用于确定使用哪个迭代器的string。可选项有:</p>
<blockquote>
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">'iternodes'</span></tt> - 一个高性能的基于正则表达式的迭代器</li>
<li><tt class="docutils literal"><span class="pre">'html'</span></tt> - 使用 <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> 的迭代器。
需要注意的是该迭代器使用DOM进行分析，其需要将所有的DOM载入内存，
当数据量大的时候会产生问题。</li>
<li><tt class="docutils literal"><span class="pre">'xml'</span></tt> - 使用 <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> 的迭代器。
需要注意的是该迭代器使用DOM进行分析，其需要将所有的DOM载入内存，
当数据量大的时候会产生问题。</li>
</ul>
</div></blockquote>
<p>默认值为 <tt class="docutils literal"><span class="pre">iternodes</span></tt> 。</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.spiders.XMLFeedSpider.itertag">
<tt class="descname">itertag</tt><a class="headerlink" href="#scrapy.contrib.spiders.XMLFeedSpider.itertag" title="永久链接至目标">¶</a></dt>
<dd><p>一个包含开始迭代的节点名的string。例如:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">itertag</span> <span class="o">=</span> <span class="s">&#39;product&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.spiders.XMLFeedSpider.namespaces">
<tt class="descname">namespaces</tt><a class="headerlink" href="#scrapy.contrib.spiders.XMLFeedSpider.namespaces" title="永久链接至目标">¶</a></dt>
<dd><p>一个由 <tt class="docutils literal"><span class="pre">(prefix,</span> <span class="pre">url)</span></tt> 元组(tuple)所组成的list。
其定义了在该文档中会被spider处理的可用的namespace。
<tt class="docutils literal"><span class="pre">prefix</span></tt> 及 <tt class="docutils literal"><span class="pre">uri</span></tt> 会被自动调用
<a class="reference internal" href="index.html#scrapy.selector.Selector.register_namespace" title="scrapy.selector.Selector.register_namespace"><tt class="xref py py-meth docutils literal"><span class="pre">register_namespace()</span></tt></a> 生成namespace。</p>
<p>您可以通过在 <a class="reference internal" href="index.html#scrapy.contrib.spiders.XMLFeedSpider.itertag" title="scrapy.contrib.spiders.XMLFeedSpider.itertag"><tt class="xref py py-attr docutils literal"><span class="pre">itertag</span></tt></a> 属性中制定节点的namespace。</p>
<p>例如:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">YourSpider</span><span class="p">(</span><span class="n">XMLFeedSpider</span><span class="p">):</span>

    <span class="n">namespaces</span> <span class="o">=</span> <span class="p">[(</span><span class="s">&#39;n&#39;</span><span class="p">,</span> <span class="s">&#39;http://www.sitemaps.org/schemas/sitemap/0.9&#39;</span><span class="p">)]</span>
    <span class="n">itertag</span> <span class="o">=</span> <span class="s">&#39;n:url&#39;</span>
    <span class="c"># ...</span>
</pre></div>
</div>
</dd></dl>

<p>除了这些新的属性之外，该spider也有以下可以覆盖(overrideable)的方法:</p>
<dl class="method">
<dt id="scrapy.contrib.spiders.XMLFeedSpider.adapt_response">
<tt class="descname">adapt_response</tt><big>(</big><em>response</em><big>)</big><a class="headerlink" href="#scrapy.contrib.spiders.XMLFeedSpider.adapt_response" title="永久链接至目标">¶</a></dt>
<dd><p>该方法在spider分析response前被调用。您可以在response被分析之前使用该函数来修改内容(body)。
该方法接受一个response并返回一个response(可以相同也可以不同)。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.spiders.XMLFeedSpider.parse_node">
<tt class="descname">parse_node</tt><big>(</big><em>response</em>, <em>selector</em><big>)</big><a class="headerlink" href="#scrapy.contrib.spiders.XMLFeedSpider.parse_node" title="永久链接至目标">¶</a></dt>
<dd><p>当节点符合提供的标签名时(<tt class="docutils literal"><span class="pre">itertag</span></tt>)该方法被调用。
接收到的response以及相应的 <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> 作为参数传递给该方法。
该方法返回一个 <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> 对象或者
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象 或者一个包含二者的可迭代对象(iterable)。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.spiders.XMLFeedSpider.process_results">
<tt class="descname">process_results</tt><big>(</big><em>response</em>, <em>results</em><big>)</big><a class="headerlink" href="#scrapy.contrib.spiders.XMLFeedSpider.process_results" title="永久链接至目标">¶</a></dt>
<dd><p>当spider返回结果(item或request)时该方法被调用。
设定该方法的目的是在结果返回给框架核心(framework core)之前做最后的处理，
例如设定item的ID。其接受一个结果的列表(list of results)及对应的response。
其结果必须返回一个结果的列表(list of results)(包含Item或者Request对象)。</p>
</dd></dl>

</dd></dl>

<div class="section" id="id5">
<h6>XMLFeedSpider例子<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h6>
<p>该spider十分易用。下边是其中一个例子:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">log</span>
<span class="kn">from</span> <span class="nn">scrapy.contrib.spiders</span> <span class="kn">import</span> <span class="n">XMLFeedSpider</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="kn">import</span> <span class="n">TestItem</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">XMLFeedSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;example.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.example.com/feed.xml&#39;</span><span class="p">]</span>
    <span class="n">iterator</span> <span class="o">=</span> <span class="s">&#39;iternodes&#39;</span> <span class="c"># This is actually unnecessary, since it&#39;s the default value</span>
    <span class="n">itertag</span> <span class="o">=</span> <span class="s">&#39;item&#39;</span>

    <span class="k">def</span> <span class="nf">parse_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="n">log</span><span class="o">.</span><span class="n">msg</span><span class="p">(</span><span class="s">&#39;Hi, this is a &lt;</span><span class="si">%s</span><span class="s">&gt; node!: </span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">itertag</span><span class="p">,</span> <span class="s">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">extract</span><span class="p">())))</span>

        <span class="n">item</span> <span class="o">=</span> <span class="n">TestItem</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s">&#39;id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;@id&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s">&#39;description&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;description&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>简单来说，我们在这里创建了一个spider，从给定的 <tt class="docutils literal"><span class="pre">start_urls</span></tt> 中下载feed，
并迭代feed中每个 <tt class="docutils literal"><span class="pre">item</span></tt> 标签，输出，并在 <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> 中存储有些随机数据。</p>
</div>
</div>
<div class="section" id="csvfeedspider">
<h5>CSVFeedSpider<a class="headerlink" href="#csvfeedspider" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.spiders.CSVFeedSpider">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spiders.</tt><tt class="descname">CSVFeedSpider</tt><a class="headerlink" href="#scrapy.contrib.spiders.CSVFeedSpider" title="永久链接至目标">¶</a></dt>
<dd><p>该spider除了其按行遍历而不是节点之外其他和XMLFeedSpider十分类似。
而其在每次迭代时调用的是 <a class="reference internal" href="index.html#scrapy.contrib.spiders.CSVFeedSpider.parse_row" title="scrapy.contrib.spiders.CSVFeedSpider.parse_row"><tt class="xref py py-meth docutils literal"><span class="pre">parse_row()</span></tt></a> 。</p>
<dl class="attribute">
<dt id="scrapy.contrib.spiders.CSVFeedSpider.delimiter">
<tt class="descname">delimiter</tt><a class="headerlink" href="#scrapy.contrib.spiders.CSVFeedSpider.delimiter" title="永久链接至目标">¶</a></dt>
<dd><p>在CSV文件中用于区分字段的分隔符。类型为string。
默认为 <tt class="docutils literal"><span class="pre">','</span></tt> (逗号)。</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.spiders.CSVFeedSpider.headers">
<tt class="descname">headers</tt><a class="headerlink" href="#scrapy.contrib.spiders.CSVFeedSpider.headers" title="永久链接至目标">¶</a></dt>
<dd><p>在CSV文件中包含的用来提取字段的行的列表。参考下边的例子。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.spiders.CSVFeedSpider.parse_row">
<tt class="descname">parse_row</tt><big>(</big><em>response</em>, <em>row</em><big>)</big><a class="headerlink" href="#scrapy.contrib.spiders.CSVFeedSpider.parse_row" title="永久链接至目标">¶</a></dt>
<dd><p>该方法接收一个response对象及一个以提供或检测出来的header为键的字典(代表每行)。
该spider中，您也可以覆盖 <tt class="docutils literal"><span class="pre">adapt_response</span></tt> 及
<tt class="docutils literal"><span class="pre">process_results</span></tt> 方法来进行预处理(pre-processing)及后(post-processing)处理。</p>
</dd></dl>

</dd></dl>

<div class="section" id="id6">
<h6>CSVFeedSpider例子<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h6>
<p>下面的例子和之前的例子很像，但使用了
<a class="reference internal" href="index.html#scrapy.contrib.spiders.CSVFeedSpider" title="scrapy.contrib.spiders.CSVFeedSpider"><tt class="xref py py-class docutils literal"><span class="pre">CSVFeedSpider</span></tt></a>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">log</span>
<span class="kn">from</span> <span class="nn">scrapy.contrib.spiders</span> <span class="kn">import</span> <span class="n">CSVFeedSpider</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="kn">import</span> <span class="n">TestItem</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">CSVFeedSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;example.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.example.com/feed.csv&#39;</span><span class="p">]</span>
    <span class="n">delimiter</span> <span class="o">=</span> <span class="s">&#39;;&#39;</span>
    <span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;id&#39;</span><span class="p">,</span> <span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;description&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse_row</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">row</span><span class="p">):</span>
        <span class="n">log</span><span class="o">.</span><span class="n">msg</span><span class="p">(</span><span class="s">&#39;Hi, this is a row!: </span><span class="si">%r</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">row</span><span class="p">)</span>

        <span class="n">item</span> <span class="o">=</span> <span class="n">TestItem</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s">&#39;id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s">&#39;id&#39;</span><span class="p">]</span>
        <span class="n">item</span><span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">]</span>
        <span class="n">item</span><span class="p">[</span><span class="s">&#39;description&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s">&#39;description&#39;</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="sitemapspider">
<h5>SitemapSpider<a class="headerlink" href="#sitemapspider" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.spiders.SitemapSpider">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spiders.</tt><tt class="descname">SitemapSpider</tt><a class="headerlink" href="#scrapy.contrib.spiders.SitemapSpider" title="永久链接至目标">¶</a></dt>
<dd><p>SitemapSpider使您爬取网站时可以通过 <a class="reference external" href="http://www.sitemaps.org">Sitemaps</a> 来发现爬取的URL。</p>
<p>其支持嵌套的sitemap，并能从 <a class="reference external" href="http://www.robotstxt.org/">robots.txt</a> 中获取sitemap的url。</p>
<dl class="attribute">
<dt id="scrapy.contrib.spiders.SitemapSpider.sitemap_urls">
<tt class="descname">sitemap_urls</tt><a class="headerlink" href="#scrapy.contrib.spiders.SitemapSpider.sitemap_urls" title="永久链接至目标">¶</a></dt>
<dd><p>包含您要爬取的url的sitemap的url列表(list)。
您也可以指定为一个 <a class="reference external" href="http://www.robotstxt.org/">robots.txt</a> ，spider会从中分析并提取url。</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.spiders.SitemapSpider.sitemap_rules">
<tt class="descname">sitemap_rules</tt><a class="headerlink" href="#scrapy.contrib.spiders.SitemapSpider.sitemap_rules" title="永久链接至目标">¶</a></dt>
<dd><p>一个包含 <tt class="docutils literal"><span class="pre">(regex,</span> <span class="pre">callback)</span></tt> 元组的列表(list):</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">regex</span></tt> 是一个用于匹配从sitemap提供的url的正则表达式。
<tt class="docutils literal"><span class="pre">regex</span></tt> 可以是一个字符串或者编译的正则对象(compiled regex object)。</li>
<li>callback指定了匹配正则表达式的url的处理函数。
<tt class="docutils literal"><span class="pre">callback</span></tt> 可以是一个字符串(spider中方法的名字)或者是callable。</li>
</ul>
<p>例如:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sitemap_rules</span> <span class="o">=</span> <span class="p">[(</span><span class="s">&#39;/product/&#39;</span><span class="p">,</span> <span class="s">&#39;parse_product&#39;</span><span class="p">)]</span>
</pre></div>
</div>
<p>规则按顺序进行匹配，之后第一个匹配才会被应用。</p>
<p>如果您忽略该属性，sitemap中发现的所有url将会被 <tt class="docutils literal"><span class="pre">parse</span></tt> 函数处理。</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.spiders.SitemapSpider.sitemap_follow">
<tt class="descname">sitemap_follow</tt><a class="headerlink" href="#scrapy.contrib.spiders.SitemapSpider.sitemap_follow" title="永久链接至目标">¶</a></dt>
<dd><p>一个用于匹配要跟进的sitemap的正则表达式的列表(list)。其仅仅被应用在
使用 <cite>Sitemap index files</cite> 来指向其他sitemap文件的站点。</p>
<p>默认情况下所有的sitemap都会被跟进。</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.spiders.SitemapSpider.sitemap_alternate_links">
<tt class="descname">sitemap_alternate_links</tt><a class="headerlink" href="#scrapy.contrib.spiders.SitemapSpider.sitemap_alternate_links" title="永久链接至目标">¶</a></dt>
<dd><p>指定当一个 <tt class="docutils literal"><span class="pre">url</span></tt> 有可选的链接时，是否跟进。
有些非英文网站会在一个 <tt class="docutils literal"><span class="pre">url</span></tt> 块内提供其他语言的网站链接。</p>
<p>例如:</p>
<div class="highlight-python"><div class="highlight"><pre>&lt;url&gt;
    &lt;loc&gt;http://example.com/&lt;/loc&gt;
    &lt;xhtml:link rel=&quot;alternate&quot; hreflang=&quot;de&quot; href=&quot;http://example.com/de&quot;/&gt;
&lt;/url&gt;
</pre></div>
</div>
<p>当 <tt class="docutils literal"><span class="pre">sitemap_alternate_links</span></tt> 设置时，两个URL都会被获取。
当 <tt class="docutils literal"><span class="pre">sitemap_alternate_links</span></tt> 关闭时，只有 <tt class="docutils literal"><span class="pre">http://example.com/</span></tt> 会被获取。</p>
<p>默认 <tt class="docutils literal"><span class="pre">sitemap_alternate_links</span></tt> 关闭。</p>
</dd></dl>

</dd></dl>

<div class="section" id="id7">
<h6>SitemapSpider样例<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h6>
<p>简单的例子: 使用 <tt class="docutils literal"><span class="pre">parse</span></tt> 处理通过sitemap发现的所有url:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.spiders</span> <span class="kn">import</span> <span class="n">SitemapSpider</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.example.com/sitemap.xml&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c"># ... scrape item here ...</span>
</pre></div>
</div>
<p>用特定的函数处理某些url，其他的使用另外的callback:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.spiders</span> <span class="kn">import</span> <span class="n">SitemapSpider</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.example.com/sitemap.xml&#39;</span><span class="p">]</span>
    <span class="n">sitemap_rules</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s">&#39;/product/&#39;</span><span class="p">,</span> <span class="s">&#39;parse_product&#39;</span><span class="p">),</span>
        <span class="p">(</span><span class="s">&#39;/category/&#39;</span><span class="p">,</span> <span class="s">&#39;parse_category&#39;</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse_product</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c"># ... scrape product ...</span>

    <span class="k">def</span> <span class="nf">parse_category</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c"># ... scrape category ...</span>
</pre></div>
</div>
<p>跟进 <a class="reference external" href="http://www.robotstxt.org/">robots.txt</a> 文件定义的sitemap并只跟进包含有 <tt class="docutils literal"><span class="pre">..sitemap_shop</span></tt> 的url:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.spiders</span> <span class="kn">import</span> <span class="n">SitemapSpider</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.example.com/robots.txt&#39;</span><span class="p">]</span>
    <span class="n">sitemap_rules</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s">&#39;/shop/&#39;</span><span class="p">,</span> <span class="s">&#39;parse_shop&#39;</span><span class="p">),</span>
    <span class="p">]</span>
    <span class="n">sitemap_follow</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;/sitemap_shops&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse_shop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c"># ... scrape shop here ...</span>
</pre></div>
</div>
<p>在SitemapSpider中使用其他url:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.spiders</span> <span class="kn">import</span> <span class="n">SitemapSpider</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.example.com/robots.txt&#39;</span><span class="p">]</span>
    <span class="n">sitemap_rules</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s">&#39;/shop/&#39;</span><span class="p">,</span> <span class="s">&#39;parse_shop&#39;</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="n">other_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.example.com/about&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">requests</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">super</span><span class="p">(</span><span class="n">MySpider</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">start_requests</span><span class="p">())</span>
        <span class="n">requests</span> <span class="o">+=</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse_other</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">other_urls</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">requests</span>

    <span class="k">def</span> <span class="nf">parse_shop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c"># ... scrape shop here ...</span>

    <span class="k">def</span> <span class="nf">parse_other</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c"># ... scrape other here ...</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<span id="document-topics/selectors"></span><div class="section" id="selectors">
<span id="topics-selectors"></span><h3>选择器(Selectors)<a class="headerlink" href="#selectors" title="永久链接至标题">¶</a></h3>
<p>当抓取网页时，你做的最常见的任务是从HTML源码中提取数据。现有的一些库可以达到这个目的：</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference external" href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> 是在程序员间非常流行的网页分析库，它基于HTML代码的结构来构造一个Python对象， 对不良标记的处理也非常合理，但它有一个缺点：慢。</li>
<li><a class="reference external" href="http://lxml.de/">lxml</a> 是一个基于 <a class="reference external" href="http://docs.python.org/library/xml.etree.elementtree.html">ElementTree</a> (不是Python标准库的一部分)的python化的XML解析库(也可以解析HTML)。</li>
</ul>
</div></blockquote>
<p>Scrapy提取数据有自己的一套机制。它们被称作选择器(seletors)，因为他们通过特定的 <a class="reference external" href="http://www.w3.org/TR/xpath">XPath</a> 或者 <a class="reference external" href="http://www.w3.org/TR/selectors">CSS</a> 表达式来“选择” HTML文件中的某个部分。</p>
<p><a class="reference external" href="http://www.w3.org/TR/xpath">XPath</a> 是一门用来在XML文件中选择节点的语言，也可以用在HTML上。 <a class="reference external" href="http://www.w3.org/TR/selectors">CSS</a> 是一门将HTML文档样式化的语言。选择器由它定义，并与特定的HTML元素的样式相关连。</p>
<p>Scrapy选择器构建于 <a class="reference external" href="http://lxml.de/">lxml</a> 库之上，这意味着它们在速度和解析准确性上非常相似。</p>
<p>本页面解释了选择器如何工作，并描述了相应的API。不同于 <a class="reference external" href="http://lxml.de/">lxml</a> API的臃肿，该API短小而简洁。这是因为 <a class="reference external" href="http://lxml.de/">lxml</a> 库除了用来选择标记化文档外，还可以用到许多任务上。</p>
<p>选择器API的完全参考详见
<a class="reference internal" href="index.html#topics-selectors-ref"><em>Selector reference</em></a></p>
<div class="section" id="id1">
<h4>使用选择器(selectors)<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h4>
<div class="section" id="id2">
<h5>构造选择器(selectors)<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h5>
<p>Scrapy selector是以 <strong>文字(text)</strong> 或 <a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> 构造的
<a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> 实例。
其根据输入的类型自动选择最优的分析方法(XML vs HTML):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.selector</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.http</span> <span class="kn">import</span> <span class="n">HtmlResponse</span>
</pre></div>
</div>
<p>以文字构造:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">body</span> <span class="o">=</span> <span class="s">&#39;&lt;html&gt;&lt;body&gt;&lt;span&gt;good&lt;/span&gt;&lt;/body&gt;&lt;/html&gt;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">body</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//span/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;good&#39;]</span>
</pre></div>
</div>
<p>以response构造:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span> <span class="o">=</span> <span class="n">HtmlResponse</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s">&#39;http://example.com&#39;</span><span class="p">,</span> <span class="n">body</span><span class="o">=</span><span class="n">body</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Selector</span><span class="p">(</span><span class="n">response</span><span class="o">=</span><span class="n">response</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//span/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;good&#39;]</span>
</pre></div>
</div>
<p>为了方便起见，response对象以 <cite>.selector</cite> 属性提供了一个selector，
您可以随时使用该快捷方法:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//span/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;good&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="id3">
<h5>使用选择器(selectors)<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h5>
<p>我们将使用 <cite>Scrapy shell</cite> (提供交互测试)和位于Scrapy文档服务器的一个样例页面，来解释如何使用选择器：</p>
<blockquote>
<div><a class="reference external" href="http://doc.scrapy.org/en/latest/_static/selectors-sample1.html">http://doc.scrapy.org/en/latest/_static/selectors-sample1.html</a></div></blockquote>
<p id="topics-selectors-htmlcode">这里是它的HTML源码:</p>
<div class="highlight-html"><div class="highlight"><pre><span class="nt">&lt;html&gt;</span>
 <span class="nt">&lt;head&gt;</span>
  <span class="nt">&lt;base</span> <span class="na">href=</span><span class="s">&#39;http://example.com/&#39;</span> <span class="nt">/&gt;</span>
  <span class="nt">&lt;title&gt;</span>Example website<span class="nt">&lt;/title&gt;</span>
 <span class="nt">&lt;/head&gt;</span>
 <span class="nt">&lt;body&gt;</span>
  <span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">&#39;images&#39;</span><span class="nt">&gt;</span>
   <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">&#39;image1.html&#39;</span><span class="nt">&gt;</span>Name: My image 1 <span class="nt">&lt;br</span> <span class="nt">/&gt;&lt;img</span> <span class="na">src=</span><span class="s">&#39;image1_thumb.jpg&#39;</span> <span class="nt">/&gt;&lt;/a&gt;</span>
   <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">&#39;image2.html&#39;</span><span class="nt">&gt;</span>Name: My image 2 <span class="nt">&lt;br</span> <span class="nt">/&gt;&lt;img</span> <span class="na">src=</span><span class="s">&#39;image2_thumb.jpg&#39;</span> <span class="nt">/&gt;&lt;/a&gt;</span>
   <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">&#39;image3.html&#39;</span><span class="nt">&gt;</span>Name: My image 3 <span class="nt">&lt;br</span> <span class="nt">/&gt;&lt;img</span> <span class="na">src=</span><span class="s">&#39;image3_thumb.jpg&#39;</span> <span class="nt">/&gt;&lt;/a&gt;</span>
   <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">&#39;image4.html&#39;</span><span class="nt">&gt;</span>Name: My image 4 <span class="nt">&lt;br</span> <span class="nt">/&gt;&lt;img</span> <span class="na">src=</span><span class="s">&#39;image4_thumb.jpg&#39;</span> <span class="nt">/&gt;&lt;/a&gt;</span>
   <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">&#39;image5.html&#39;</span><span class="nt">&gt;</span>Name: My image 5 <span class="nt">&lt;br</span> <span class="nt">/&gt;&lt;img</span> <span class="na">src=</span><span class="s">&#39;image5_thumb.jpg&#39;</span> <span class="nt">/&gt;&lt;/a&gt;</span>
  <span class="nt">&lt;/div&gt;</span>
 <span class="nt">&lt;/body&gt;</span>
<span class="nt">&lt;/html&gt;</span>
</pre></div>
</div>
<p>首先, 我们打开shell:</p>
<div class="highlight-sh"><div class="highlight"><pre>scrapy shell http://doc.scrapy.org/en/latest/_static/selectors-sample1.html
</pre></div>
</div>
<p>接着，当shell载入后，您将获得名为 <tt class="docutils literal"><span class="pre">response</span></tt> 的shell变量，其为响应的response，
并且在其 <tt class="docutils literal"><span class="pre">response.selector</span></tt> 属性上绑定了一个selector。</p>
<p>因为我们处理的是HTML，选择器将自动使用HTML语法分析。</p>
<p>那么，通过查看 <a class="reference internal" href="index.html#topics-selectors-htmlcode"><em>HTML code</em></a> 该页面的源码，我们构建一个XPath来选择title标签内的文字:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//title/text()&#39;</span><span class="p">)</span>
<span class="go">[&lt;Selector (text) xpath=//title/text()&gt;]</span>
</pre></div>
</div>
<p>由于在response中使用XPath、CSS查询十分普遍，因此，Scrapy提供了两个实用的快捷方式:
<tt class="docutils literal"><span class="pre">response.xpath()</span></tt> 及 <tt class="docutils literal"><span class="pre">response.css()</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//title/text()&#39;</span><span class="p">)</span>
<span class="go">[&lt;Selector (text) xpath=//title/text()&gt;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s">&#39;title::text&#39;</span><span class="p">)</span>
<span class="go">[&lt;Selector (text) xpath=//title/text()&gt;]</span>
</pre></div>
</div>
<p>如你所见， <tt class="docutils literal"><span class="pre">.xpath()</span></tt> 及 <tt class="docutils literal"><span class="pre">.css()</span></tt> 方法返回一个类
<a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><tt class="xref py py-class docutils literal"><span class="pre">SelectorList</span></tt></a> 的实例, 它是一个新选择器的列表。这个API可以用来快速的提取嵌套数据。</p>
<p>为了提取真实的原文数据，你需要调用 <tt class="docutils literal"><span class="pre">.extract()</span></tt> 方法如下:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//title/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;Example website&#39;]</span>
</pre></div>
</div>
<p>注意CSS选择器可以使用CSS3伪元素(pseudo-elements)来选择文字或者属性节点:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s">&#39;title::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;Example website&#39;]</span>
</pre></div>
</div>
<p>现在我们将得到根URL(base URL)和一些图片链接:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//base/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;http://example.com/&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s">&#39;base::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;http://example.com/&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//a[contains(@href, &quot;image&quot;)]/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;image1.html&#39;,</span>
<span class="go"> u&#39;image2.html&#39;,</span>
<span class="go"> u&#39;image3.html&#39;,</span>
<span class="go"> u&#39;image4.html&#39;,</span>
<span class="go"> u&#39;image5.html&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s">&#39;a[href*=image]::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;image1.html&#39;,</span>
<span class="go"> u&#39;image2.html&#39;,</span>
<span class="go"> u&#39;image3.html&#39;,</span>
<span class="go"> u&#39;image4.html&#39;,</span>
<span class="go"> u&#39;image5.html&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//a[contains(@href, &quot;image&quot;)]/img/@src&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;image1_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image2_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image3_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image4_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image5_thumb.jpg&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s">&#39;a[href*=image] img::attr(src)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;image1_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image2_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image3_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image4_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image5_thumb.jpg&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="topics-selectors-nesting-selectors">
<span id="id4"></span><h5>嵌套选择器(selectors)<a class="headerlink" href="#topics-selectors-nesting-selectors" title="永久链接至标题">¶</a></h5>
<p>选择器方法( <tt class="docutils literal"><span class="pre">.xpath()</span></tt> or <tt class="docutils literal"><span class="pre">.css()</span></tt> )返回相同类型的选择器列表，因此你也可以对这些选择器调用选择器方法。下面是一个例子:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">links</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//a[contains(@href, &quot;image&quot;)]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">links</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;&lt;a href=&quot;image1.html&quot;&gt;Name: My image 1 &lt;br&gt;&lt;img src=&quot;image1_thumb.jpg&quot;&gt;&lt;/a&gt;&#39;,</span>
<span class="go"> u&#39;&lt;a href=&quot;image2.html&quot;&gt;Name: My image 2 &lt;br&gt;&lt;img src=&quot;image2_thumb.jpg&quot;&gt;&lt;/a&gt;&#39;,</span>
<span class="go"> u&#39;&lt;a href=&quot;image3.html&quot;&gt;Name: My image 3 &lt;br&gt;&lt;img src=&quot;image3_thumb.jpg&quot;&gt;&lt;/a&gt;&#39;,</span>
<span class="go"> u&#39;&lt;a href=&quot;image4.html&quot;&gt;Name: My image 4 &lt;br&gt;&lt;img src=&quot;image4_thumb.jpg&quot;&gt;&lt;/a&gt;&#39;,</span>
<span class="go"> u&#39;&lt;a href=&quot;image5.html&quot;&gt;Name: My image 5 &lt;br&gt;&lt;img src=&quot;image5_thumb.jpg&quot;&gt;&lt;/a&gt;&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">link</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">links</span><span class="p">):</span>
<span class="go">        args = (index, link.xpath(&#39;@href&#39;).extract(), link.xpath(&#39;img/@src&#39;).extract())</span>
<span class="go">        print &#39;Link number %d points to url %s and image %s&#39; % args</span>

<span class="go">Link number 0 points to url [u&#39;image1.html&#39;] and image [u&#39;image1_thumb.jpg&#39;]</span>
<span class="go">Link number 1 points to url [u&#39;image2.html&#39;] and image [u&#39;image2_thumb.jpg&#39;]</span>
<span class="go">Link number 2 points to url [u&#39;image3.html&#39;] and image [u&#39;image3_thumb.jpg&#39;]</span>
<span class="go">Link number 3 points to url [u&#39;image4.html&#39;] and image [u&#39;image4_thumb.jpg&#39;]</span>
<span class="go">Link number 4 points to url [u&#39;image5.html&#39;] and image [u&#39;image5_thumb.jpg&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="id5">
<h5>结合正则表达式使用选择器(selectors)<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h5>
<p><a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> 也有一个 <tt class="docutils literal"><span class="pre">.re()</span></tt> 方法，用来通过正则表达式来提取数据。然而，不同于使用 <tt class="docutils literal"><span class="pre">.xpath()</span></tt> 或者 <tt class="docutils literal"><span class="pre">.css()</span></tt> 方法, <tt class="docutils literal"><span class="pre">.re()</span></tt> 方法返回unicode字符串的列表。所以你无法构造嵌套式的 <tt class="docutils literal"><span class="pre">.re()</span></tt> 调用。</p>
<p>下面是一个例子，从上面的 <a class="reference internal" href="index.html#topics-selectors-htmlcode"><em>HTML code</em></a> 中提取图像名字:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//a[contains(@href, &quot;image&quot;)]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="s">r&#39;Name:\s*(.*)&#39;</span><span class="p">)</span>
<span class="go">[u&#39;My image 1&#39;,</span>
<span class="go"> u&#39;My image 2&#39;,</span>
<span class="go"> u&#39;My image 3&#39;,</span>
<span class="go"> u&#39;My image 4&#39;,</span>
<span class="go"> u&#39;My image 5&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="xpaths">
<span id="topics-selectors-relative-xpaths"></span><h5>使用相对XPaths<a class="headerlink" href="#xpaths" title="永久链接至标题">¶</a></h5>
<p>记住如果你使用嵌套的选择器，并使用起始为 <tt class="docutils literal"><span class="pre">/</span></tt> 的XPath，那么该XPath将对文档使用绝对路径，而且对于你调用的 <tt class="docutils literal"><span class="pre">Selector</span></tt> 不是相对路径。</p>
<p>比如，假设你想提取在 <tt class="docutils literal"><span class="pre">&lt;div&gt;</span></tt> 元素中的所有 <tt class="docutils literal"><span class="pre">&lt;p&gt;</span></tt> 元素。首先，你将先得到所有的 <tt class="docutils literal"><span class="pre">&lt;div&gt;</span></tt> 元素:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">divs</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//div&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>开始时，你可能会尝试使用下面的错误的方法，因为它其实是从整篇文档中，而不仅仅是从那些 <tt class="docutils literal"><span class="pre">&lt;div&gt;</span></tt> 元素内部提取所有的 <tt class="docutils literal"><span class="pre">&lt;p&gt;</span></tt> 元素:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">divs</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//p&#39;</span><span class="p">):</span>  <span class="c"># this is wrong - gets all &lt;p&gt; from the whole document</span>
<span class="gp">... </span>    <span class="k">print</span> <span class="n">p</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
<p>下面是比较合适的处理方法(注意 <tt class="docutils literal"><span class="pre">.//p</span></tt> XPath的点前缀):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">divs</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;.//p&#39;</span><span class="p">):</span>  <span class="c"># extracts all &lt;p&gt; inside</span>
<span class="gp">... </span>    <span class="k">print</span> <span class="n">p</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
<p>另一种常见的情况将是提取所有直系 <tt class="docutils literal"><span class="pre">&lt;p&gt;</span></tt> 的结果:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">divs</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;p&#39;</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">print</span> <span class="n">p</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
<p>更多关于相对XPaths的细节详见XPath说明中的 <a class="reference external" href="http://www.w3.org/TR/xpath#location-paths">Location Paths</a> 部分。</p>
</div>
<div class="section" id="exslt">
<h5>使用EXSLT扩展<a class="headerlink" href="#exslt" title="永久链接至标题">¶</a></h5>
<p>因建于 <a class="reference external" href="http://lxml.de/">lxml</a> 之上, Scrapy选择器也支持一些 <a class="reference external" href="http://www.exslt.org/">EXSLT</a> 扩展，可以在XPath表达式中使用这些预先制定的命名空间：</p>
<table border="1" class="docutils">
<colgroup>
<col width="9%" />
<col width="56%" />
<col width="35%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">前缀</th>
<th class="head">命名空间</th>
<th class="head">用途</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>re</td>
<td>http://exslt.org/regular-expressions</td>
<td><a class="reference external" href="http://www.exslt.org/regexp/index.html">正则表达式</a></td>
</tr>
<tr class="row-odd"><td>set</td>
<td>http://exslt.org/sets</td>
<td><a class="reference external" href="http://www.exslt.org/set/index.html">集合操作</a></td>
</tr>
</tbody>
</table>
<div class="section" id="id6">
<h6>正则表达式<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h6>
<p>例如在XPath的 <tt class="docutils literal"><span class="pre">starts-with()</span></tt> 或 <tt class="docutils literal"><span class="pre">contains()</span></tt> 无法满足需求时， <tt class="docutils literal"><span class="pre">test()</span></tt> 函数可以非常有用。</p>
<p>例如在列表中选择有&#8221;class&#8221;元素且结尾为一个数字的链接:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">doc</span> <span class="o">=</span> <span class="s">&quot;&quot;&quot;</span>
<span class="gp">... </span><span class="s">&lt;div&gt;</span>
<span class="gp">... </span><span class="s">    &lt;ul&gt;</span>
<span class="gp">... </span><span class="s">        &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s">        &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s">        &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s">    &lt;/ul&gt;</span>
<span class="gp">... </span><span class="s">&lt;/div&gt;</span>
<span class="gp">... </span><span class="s">&quot;&quot;&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">doc</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s">&quot;html&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//li//@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;link1.html&#39;, u&#39;link2.html&#39;, u&#39;link3.html&#39;, u&#39;link4.html&#39;, u&#39;link5.html&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//li[re:test(@class, &quot;item-\d$&quot;)]//@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;link1.html&#39;, u&#39;link2.html&#39;, u&#39;link4.html&#39;, u&#39;link5.html&#39;]</span>
<span class="go">&gt;&gt;&gt;</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="first admonition-title">警告</p>
<p class="last">C语言库 <tt class="docutils literal"><span class="pre">libxslt</span></tt> 不原生支持EXSLT正则表达式，因此 <a class="reference external" href="http://lxml.de/">lxml</a> 在实现时使用了Python <tt class="docutils literal"><span class="pre">re</span></tt> 模块的钩子。
因此，在XPath表达式中使用regexp函数可能会牺牲少量的性能。</p>
</div>
</div>
<div class="section" id="id7">
<h6>集合操作<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h6>
<p>集合操作可以方便地用于在提取文字元素前从文档树中去除一些部分。</p>
<p>例如使用itemscopes组和对应的itemprops来提取微数据(来自http://schema.org/Product的样本内容):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">doc</span> <span class="o">=</span> <span class="s">&quot;&quot;&quot;</span>
<span class="gp">... </span><span class="s">&lt;div itemscope itemtype=&quot;http://schema.org/Product&quot;&gt;</span>
<span class="gp">... </span><span class="s">  &lt;span itemprop=&quot;name&quot;&gt;Kenmore White 17&quot; Microwave&lt;/span&gt;</span>
<span class="gp">... </span><span class="s">  &lt;img src=&quot;kenmore-microwave-17in.jpg&quot; alt=&#39;Kenmore 17&quot; Microwave&#39; /&gt;</span>
<span class="gp">... </span><span class="s">  &lt;div itemprop=&quot;aggregateRating&quot;</span>
<span class="gp">... </span><span class="s">    itemscope itemtype=&quot;http://schema.org/AggregateRating&quot;&gt;</span>
<span class="gp">... </span><span class="s">   Rated &lt;span itemprop=&quot;ratingValue&quot;&gt;3.5&lt;/span&gt;/5</span>
<span class="gp">... </span><span class="s">   based on &lt;span itemprop=&quot;reviewCount&quot;&gt;11&lt;/span&gt; customer reviews</span>
<span class="gp">... </span><span class="s">  &lt;/div&gt;</span>
<span class="gp">...</span><span class="s"></span>
<span class="gp">... </span><span class="s">  &lt;div itemprop=&quot;offers&quot; itemscope itemtype=&quot;http://schema.org/Offer&quot;&gt;</span>
<span class="gp">... </span><span class="s">    &lt;span itemprop=&quot;price&quot;&gt;$55.00&lt;/span&gt;</span>
<span class="gp">... </span><span class="s">    &lt;link itemprop=&quot;availability&quot; href=&quot;http://schema.org/InStock&quot; /&gt;In stock</span>
<span class="gp">... </span><span class="s">  &lt;/div&gt;</span>
<span class="gp">...</span><span class="s"></span>
<span class="gp">... </span><span class="s">  Product description:</span>
<span class="gp">... </span><span class="s">  &lt;span itemprop=&quot;description&quot;&gt;0.7 cubic feet countertop microwave.</span>
<span class="gp">... </span><span class="s">  Has six preset cooking categories and convenience features like</span>
<span class="gp">... </span><span class="s">  Add-A-Minute and Child Lock.&lt;/span&gt;</span>
<span class="gp">...</span><span class="s"></span>
<span class="gp">... </span><span class="s">  Customer reviews:</span>
<span class="gp">...</span><span class="s"></span>
<span class="gp">... </span><span class="s">  &lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&gt;</span>
<span class="gp">... </span><span class="s">    &lt;span itemprop=&quot;name&quot;&gt;Not a happy camper&lt;/span&gt; -</span>
<span class="gp">... </span><span class="s">    by &lt;span itemprop=&quot;author&quot;&gt;Ellie&lt;/span&gt;,</span>
<span class="gp">... </span><span class="s">    &lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-04-01&quot;&gt;April 1, 2011</span>
<span class="gp">... </span><span class="s">    &lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&gt;</span>
<span class="gp">... </span><span class="s">      &lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;&gt;</span>
<span class="gp">... </span><span class="s">      &lt;span itemprop=&quot;ratingValue&quot;&gt;1&lt;/span&gt;/</span>
<span class="gp">... </span><span class="s">      &lt;span itemprop=&quot;bestRating&quot;&gt;5&lt;/span&gt;stars</span>
<span class="gp">... </span><span class="s">    &lt;/div&gt;</span>
<span class="gp">... </span><span class="s">    &lt;span itemprop=&quot;description&quot;&gt;The lamp burned out and now I have to replace</span>
<span class="gp">... </span><span class="s">    it. &lt;/span&gt;</span>
<span class="gp">... </span><span class="s">  &lt;/div&gt;</span>
<span class="gp">...</span><span class="s"></span>
<span class="gp">... </span><span class="s">  &lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&gt;</span>
<span class="gp">... </span><span class="s">    &lt;span itemprop=&quot;name&quot;&gt;Value purchase&lt;/span&gt; -</span>
<span class="gp">... </span><span class="s">    by &lt;span itemprop=&quot;author&quot;&gt;Lucas&lt;/span&gt;,</span>
<span class="gp">... </span><span class="s">    &lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-03-25&quot;&gt;March 25, 2011</span>
<span class="gp">... </span><span class="s">    &lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&gt;</span>
<span class="gp">... </span><span class="s">      &lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;/&gt;</span>
<span class="gp">... </span><span class="s">      &lt;span itemprop=&quot;ratingValue&quot;&gt;4&lt;/span&gt;/</span>
<span class="gp">... </span><span class="s">      &lt;span itemprop=&quot;bestRating&quot;&gt;5&lt;/span&gt;stars</span>
<span class="gp">... </span><span class="s">    &lt;/div&gt;</span>
<span class="gp">... </span><span class="s">    &lt;span itemprop=&quot;description&quot;&gt;Great microwave for the price. It is small and</span>
<span class="gp">... </span><span class="s">    fits in my apartment.&lt;/span&gt;</span>
<span class="gp">... </span><span class="s">  &lt;/div&gt;</span>
<span class="gp">... </span><span class="s">  ...</span>
<span class="gp">... </span><span class="s">&lt;/div&gt;</span>
<span class="gp">... </span><span class="s">&quot;&quot;&quot;</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">scope</span> <span class="ow">in</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//div[@itemscope]&#39;</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">print</span> <span class="s">&quot;current scope:&quot;</span><span class="p">,</span> <span class="n">scope</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;@itemtype&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="gp">... </span>    <span class="n">props</span> <span class="o">=</span> <span class="n">scope</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;&#39;&#39;</span>
<span class="gp">... </span><span class="s">                set:difference(./descendant::*/@itemprop,</span>
<span class="gp">... </span><span class="s">                               .//*[@itemscope]/*/@itemprop)&#39;&#39;&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">print</span> <span class="s">&quot;    properties:&quot;</span><span class="p">,</span> <span class="n">props</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="gp">... </span>    <span class="k">print</span>
<span class="gp">...</span>

<span class="go">current scope: [u&#39;http://schema.org/Product&#39;]</span>
<span class="go">    properties: [u&#39;name&#39;, u&#39;aggregateRating&#39;, u&#39;offers&#39;, u&#39;description&#39;, u&#39;review&#39;, u&#39;review&#39;]</span>

<span class="go">current scope: [u&#39;http://schema.org/AggregateRating&#39;]</span>
<span class="go">    properties: [u&#39;ratingValue&#39;, u&#39;reviewCount&#39;]</span>

<span class="go">current scope: [u&#39;http://schema.org/Offer&#39;]</span>
<span class="go">    properties: [u&#39;price&#39;, u&#39;availability&#39;]</span>

<span class="go">current scope: [u&#39;http://schema.org/Review&#39;]</span>
<span class="go">    properties: [u&#39;name&#39;, u&#39;author&#39;, u&#39;datePublished&#39;, u&#39;reviewRating&#39;, u&#39;description&#39;]</span>

<span class="go">current scope: [u&#39;http://schema.org/Rating&#39;]</span>
<span class="go">    properties: [u&#39;worstRating&#39;, u&#39;ratingValue&#39;, u&#39;bestRating&#39;]</span>

<span class="go">current scope: [u&#39;http://schema.org/Review&#39;]</span>
<span class="go">    properties: [u&#39;name&#39;, u&#39;author&#39;, u&#39;datePublished&#39;, u&#39;reviewRating&#39;, u&#39;description&#39;]</span>

<span class="go">current scope: [u&#39;http://schema.org/Rating&#39;]</span>
<span class="go">    properties: [u&#39;worstRating&#39;, u&#39;ratingValue&#39;, u&#39;bestRating&#39;]</span>

<span class="go">&gt;&gt;&gt;</span>
</pre></div>
</div>
<p>在这里，我们首先在 <tt class="docutils literal"><span class="pre">itemscope</span></tt> 元素上迭代，对于其中的每一个元素，我们寻找所有的 <tt class="docutils literal"><span class="pre">itemprops</span></tt> 元素，并排除那些本身在另一个 <tt class="docutils literal"><span class="pre">itemscope</span></tt> 内的元素。</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.selector">
<span id="id11"></span><span id="topics-selectors-ref"></span><h4>内建选择器的参考<a class="headerlink" href="#module-scrapy.selector" title="永久链接至标题">¶</a></h4>
<dl class="class">
<dt id="scrapy.selector.Selector">
<em class="property">class </em><tt class="descclassname">scrapy.selector.</tt><tt class="descname">Selector</tt><big>(</big><em>response=None</em>, <em>text=None</em>, <em>type=None</em><big>)</big><a class="headerlink" href="#scrapy.selector.Selector" title="永久链接至目标">¶</a></dt>
<dd><blockquote>
<div><a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> 的实例是对选择某些内容响应的封装。</div></blockquote>
<p><tt class="docutils literal"><span class="pre">response</span></tt> 是 <a class="reference internal" href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">HtmlResponse</span></tt></a> 或
<a class="reference internal" href="index.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">XmlResponse</span></tt></a> 的一个对象，将被用来选择和提取数据。</p>
<p><tt class="docutils literal"><span class="pre">text</span></tt> 是在 <tt class="docutils literal"><span class="pre">response</span></tt> 不可用时的一个unicode字符串或utf-8编码的文字。将 <tt class="docutils literal"><span class="pre">text</span></tt> 和 <tt class="docutils literal"><span class="pre">response</span></tt> 一起使用是未定义行为。</p>
<p><tt class="docutils literal"><span class="pre">type</span></tt> 定义了选择器类型，可以是 <tt class="docutils literal"><span class="pre">&quot;html&quot;</span></tt>, <tt class="docutils literal"><span class="pre">&quot;xml&quot;</span></tt> or <tt class="docutils literal"><span class="pre">None</span></tt> (默认).</p>
<blockquote>
<div><blockquote>
<div><p>如果 <tt class="docutils literal"><span class="pre">type</span></tt> 是 <tt class="docutils literal"><span class="pre">None</span></tt> ，选择器会根据 <tt class="docutils literal"><span class="pre">response</span></tt> 类型(参见下面)自动选择最佳的类型，或者在和 <tt class="docutils literal"><span class="pre">text</span></tt> 一起使用时，默认为 <tt class="docutils literal"><span class="pre">&quot;html&quot;</span></tt> 。</p>
<p>如果 <tt class="docutils literal"><span class="pre">type</span></tt> 是 <tt class="docutils literal"><span class="pre">None</span></tt> ，并传递了一个 <tt class="docutils literal"><span class="pre">response</span></tt> ，选择器类型将从response类型中推导如下：</p>
<blockquote>
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">&quot;html&quot;</span></tt> for <a class="reference internal" href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">HtmlResponse</span></tt></a> type</li>
<li><tt class="docutils literal"><span class="pre">&quot;xml&quot;</span></tt> for <a class="reference internal" href="index.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">XmlResponse</span></tt></a> type</li>
<li><tt class="docutils literal"><span class="pre">&quot;html&quot;</span></tt> for anything else</li>
</ul>
</div></blockquote>
</div></blockquote>
<p>其他情况下，如果设定了 <tt class="docutils literal"><span class="pre">type</span></tt> ，选择器类型将被强制设定，而不进行检测。</p>
</div></blockquote>
<dl class="method">
<dt id="scrapy.selector.Selector.xpath">
<tt class="descname">xpath</tt><big>(</big><em>query</em><big>)</big><a class="headerlink" href="#scrapy.selector.Selector.xpath" title="永久链接至目标">¶</a></dt>
<dd><p>寻找可以匹配xpath <tt class="docutils literal"><span class="pre">query</span></tt> 的节点，并返回 <a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><tt class="xref py py-class docutils literal"><span class="pre">SelectorList</span></tt></a> 的一个实例结果，单一化其所有元素。列表元素也实现了 <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> 的接口。</p>
<p><tt class="docutils literal"><span class="pre">query</span></tt> 是包含XPATH查询请求的字符串。</p>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">为了方便起见，该方法也可以通过 <tt class="docutils literal"><span class="pre">response.xpath()</span></tt> 调用</p>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.css">
<tt class="descname">css</tt><big>(</big><em>query</em><big>)</big><a class="headerlink" href="#scrapy.selector.Selector.css" title="永久链接至目标">¶</a></dt>
<dd><p>应用给定的CSS选择器，返回 <a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><tt class="xref py py-class docutils literal"><span class="pre">SelectorList</span></tt></a> 的一个实例。</p>
<p><tt class="docutils literal"><span class="pre">query</span></tt> 是一个包含CSS选择器的字符串。</p>
<p>在后台，通过 <cite>cssselect</cite> 库和运行 <tt class="docutils literal"><span class="pre">.xpath()</span></tt> 方法，CSS查询会被转换为XPath查询。</p>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">为了方便起见，该方法也可以通过 <tt class="docutils literal"><span class="pre">response.css()</span></tt> 调用</p>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.extract">
<tt class="descname">extract</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.selector.Selector.extract" title="永久链接至目标">¶</a></dt>
<dd><p>串行化并将匹配到的节点返回一个unicode字符串列表。
结尾是编码内容的百分比。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.re">
<tt class="descname">re</tt><big>(</big><em>regex</em><big>)</big><a class="headerlink" href="#scrapy.selector.Selector.re" title="永久链接至目标">¶</a></dt>
<dd><p>应用给定的regex，并返回匹配到的unicode字符串列表。、</p>
<p><tt class="docutils literal"><span class="pre">regex</span></tt> 可以是一个已编译的正则表达式，也可以是一个将被 <tt class="docutils literal"><span class="pre">re.compile(regex)</span></tt> 编译为正则表达式的字符串。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.register_namespace">
<tt class="descname">register_namespace</tt><big>(</big><em>prefix</em>, <em>uri</em><big>)</big><a class="headerlink" href="#scrapy.selector.Selector.register_namespace" title="永久链接至目标">¶</a></dt>
<dd><p>注册给定的命名空间，其将在 <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> 中使用。
不注册命名空间，你将无法从非标准命名空间中选择或提取数据。参见下面的例子。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.remove_namespaces">
<tt class="descname">remove_namespaces</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.selector.Selector.remove_namespaces" title="永久链接至目标">¶</a></dt>
<dd><p>移除所有的命名空间，允许使用少量的命名空间xpaths遍历文档。参加下面的例子。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.__nonzero__">
<tt class="descname">__nonzero__</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.selector.Selector.__nonzero__" title="永久链接至目标">¶</a></dt>
<dd><p>如果选择了任意的真实文档，将返回 <tt class="docutils literal"><span class="pre">True</span></tt> ，否则返回 <tt class="docutils literal"><span class="pre">False</span></tt> 。
也就是说， <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> 的布尔值是通过它选择的内容确定的。</p>
</dd></dl>

</dd></dl>

<div class="section" id="selectorlist">
<h5>SelectorList对象<a class="headerlink" href="#selectorlist" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.selector.SelectorList">
<em class="property">class </em><tt class="descclassname">scrapy.selector.</tt><tt class="descname">SelectorList</tt><a class="headerlink" href="#scrapy.selector.SelectorList" title="永久链接至目标">¶</a></dt>
<dd><blockquote>
<div><a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><tt class="xref py py-class docutils literal"><span class="pre">SelectorList</span></tt></a> 类是内建 <tt class="docutils literal"><span class="pre">list</span></tt> 类的子类，提供了一些额外的方法。</div></blockquote>
<dl class="method">
<dt id="scrapy.selector.SelectorList.xpath">
<tt class="descname">xpath</tt><big>(</big><em>query</em><big>)</big><a class="headerlink" href="#scrapy.selector.SelectorList.xpath" title="永久链接至目标">¶</a></dt>
<dd><p>对列表中的每个元素调用 <tt class="docutils literal"><span class="pre">.xpath()</span></tt> 方法，返回结果为另一个单一化的 <a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><tt class="xref py py-class docutils literal"><span class="pre">SelectorList</span></tt></a> 。</p>
<p><tt class="docutils literal"><span class="pre">query</span></tt> 和 <a class="reference internal" href="index.html#scrapy.selector.Selector.xpath" title="scrapy.selector.Selector.xpath"><tt class="xref py py-meth docutils literal"><span class="pre">Selector.xpath()</span></tt></a> 中的参数相同。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.SelectorList.css">
<tt class="descname">css</tt><big>(</big><em>query</em><big>)</big><a class="headerlink" href="#scrapy.selector.SelectorList.css" title="永久链接至目标">¶</a></dt>
<dd><p>对列表中的各个元素调用 <tt class="docutils literal"><span class="pre">.css()</span></tt> 方法，返回结果为另一个单一化的 <a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><tt class="xref py py-class docutils literal"><span class="pre">SelectorList</span></tt></a> 。</p>
<p><tt class="docutils literal"><span class="pre">query</span></tt> 和 <a class="reference internal" href="index.html#scrapy.selector.Selector.css" title="scrapy.selector.Selector.css"><tt class="xref py py-meth docutils literal"><span class="pre">Selector.css()</span></tt></a> 中的参数相同。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.SelectorList.extract">
<tt class="descname">extract</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.selector.SelectorList.extract" title="永久链接至目标">¶</a></dt>
<dd><p>对列表中的各个元素调用 <tt class="docutils literal"><span class="pre">.extract()</span></tt> 方法，返回结果为单一化的unicode字符串列表。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.SelectorList.re">
<tt class="descname">re</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.selector.SelectorList.re" title="永久链接至目标">¶</a></dt>
<dd><p>对列表中的各个元素调用 <tt class="docutils literal"><span class="pre">.re()</span></tt> 方法，返回结果为单一化的unicode字符串列表。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.SelectorList.__nonzero__">
<tt class="descname">__nonzero__</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.selector.SelectorList.__nonzero__" title="永久链接至目标">¶</a></dt>
<dd><p>列表非空则返回True，否则返回False。</p>
</dd></dl>

</dd></dl>

<div class="section" id="html">
<h6>在HTML响应上的选择器样例<a class="headerlink" href="#html" title="永久链接至标题">¶</a></h6>
<p>这里是一些 <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> 的样例，用来说明一些概念。
在所有的例子中，我们假设已经有一个通过 <a class="reference internal" href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">HtmlResponse</span></tt></a> 对象实例化的 <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> ，如下:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">html_response</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic">
<li><p class="first">从HTML响应主体中提取所有的 <tt class="docutils literal"><span class="pre">&lt;h1&gt;</span></tt> 元素，返回:class:<cite>Selector</cite> 对象(即 <a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><tt class="xref py py-class docutils literal"><span class="pre">SelectorList</span></tt></a> 的一个对象)的列表:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//h1&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p class="first">从HTML响应主体上提取所有 <tt class="docutils literal"><span class="pre">&lt;h1&gt;</span></tt> 元素的文字，返回一个unicode字符串的列表:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//h1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>         <span class="c"># this includes the h1 tag</span>
<span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//h1/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>  <span class="c"># this excludes the h1 tag</span>
</pre></div>
</div>
</li>
<li><p class="first">在所有 <tt class="docutils literal"><span class="pre">&lt;p&gt;</span></tt> 标签上迭代，打印它们的类属性:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//p&quot;</span><span class="p">):</span>
    <span class="k">print</span> <span class="n">node</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;@class&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="xml">
<h6>在XML响应上的选择器样例<a class="headerlink" href="#xml" title="永久链接至标题">¶</a></h6>
<p>这里是一些样例，用来说明一些概念。在两个例子中，我们假设已经有一个通过 <a class="reference internal" href="index.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">XmlResponse</span></tt></a> 对象实例化的 <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> ，如下:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">xml_response</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic">
<li><p class="first">从XML响应主体中选择所有的 <tt class="docutils literal"><span class="pre">&lt;product&gt;</span></tt> 元素，返回 <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> 对象(即 <a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><tt class="xref py py-class docutils literal"><span class="pre">SelectorList</span></tt></a> 对象)的列表:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//product&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p class="first">从 <a class="reference external" href="https://support.google.com/merchants/answer/160589?hl=en&amp;ref_topic=2473799">Google Base XML feed</a> 中提取所有的价钱，这需要注册一个命名空间:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sel</span><span class="o">.</span><span class="n">register_namespace</span><span class="p">(</span><span class="s">&quot;g&quot;</span><span class="p">,</span> <span class="s">&quot;http://base.google.com/ns/1.0&quot;</span><span class="p">)</span>
<span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//g:price&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="removing-namespaces">
<span id="id12"></span><h6>移除命名空间<a class="headerlink" href="#removing-namespaces" title="永久链接至标题">¶</a></h6>
<p>在处理爬虫项目时，完全去掉命名空间而仅仅处理元素名字，写更多简单/实用的XPath会方便很多。你可以为此使用 <a class="reference internal" href="index.html#scrapy.selector.Selector.remove_namespaces" title="scrapy.selector.Selector.remove_namespaces"><tt class="xref py py-meth docutils literal"><span class="pre">Selector.remove_namespaces()</span></tt></a> 方法。</p>
<p>让我们来看一个例子，以Github博客的atom订阅来解释这个情况。</p>
<p>首先，我们使用想爬取的url来打开shell:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy shell https://github.com/blog.atom
</pre></div>
</div>
<p>一旦进入shell，我们可以尝试选择所有的 <tt class="docutils literal"><span class="pre">&lt;link&gt;</span></tt> 对象，可以看到没有结果(因为Atom XML命名空间混淆了这些节点):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//link&quot;</span><span class="p">)</span>
<span class="go">[]</span>
</pre></div>
</div>
<p>但一旦我们调用 <a class="reference internal" href="index.html#scrapy.selector.Selector.remove_namespaces" title="scrapy.selector.Selector.remove_namespaces"><tt class="xref py py-meth docutils literal"><span class="pre">Selector.remove_namespaces()</span></tt></a> 方法，所有的节点都可以直接通过他们的名字来访问:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">selector</span><span class="o">.</span><span class="n">remove_namespaces</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//link&quot;</span><span class="p">)</span>
<span class="go">[&lt;Selector xpath=&#39;//link&#39; data=u&#39;&lt;link xmlns=&quot;http://www.w3.org/2005/Atom&#39;&gt;,</span>
<span class="go"> &lt;Selector xpath=&#39;//link&#39; data=u&#39;&lt;link xmlns=&quot;http://www.w3.org/2005/Atom&#39;&gt;,</span>
<span class="go"> ...</span>
</pre></div>
</div>
<p>如果你对为什么命名空间移除操作并不总是被调用，而需要手动调用有疑惑。这是因为存在如下两个原因，按照相关顺序如下：</p>
<ol class="arabic simple">
<li>移除命名空间需要迭代并修改文件的所有节点，而这对于Scrapy爬取的所有文档操作需要一定的性能消耗</li>
<li>会存在这样的情况，确实需要使用命名空间，但有些元素的名字与命名空间冲突。尽管这些情况非常少见。</li>
</ol>
</div>
</div>
</div>
</div>
<span id="document-topics/loaders"></span><div class="section" id="module-scrapy.contrib.loader">
<span id="item-loaders"></span><span id="topics-loaders"></span><h3>Item Loaders<a class="headerlink" href="#module-scrapy.contrib.loader" title="永久链接至标题">¶</a></h3>
<p>Item Loaders 提供了一种便捷的 convenient mechanism for populating scraped <a class="reference internal" href="index.html#topics-items"><em>Items</em></a>. Even though Items can be populated using their own
dictionary-like API, the Item Loaders provide a much more convenient API for
populating them from a scraping process, by automating some common tasks like
parsing the raw extracted data before assigning it.</p>
<p>In other words, <a class="reference internal" href="index.html#topics-items"><em>Items</em></a> provide the <em>container</em> of
scraped data, while Item Loaders provide the mechanism for <em>populating</em> that
container.</p>
<p>Item Loaders are designed to provide a flexible, efficient and easy mechanism
for extending and overriding different field parsing rules, either by spider,
or by source format (HTML, XML, etc) without becoming a nightmare to maintain.</p>
<div class="section" id="using-item-loaders-to-populate-items">
<h4>Using Item Loaders to populate items<a class="headerlink" href="#using-item-loaders-to-populate-items" title="永久链接至标题">¶</a></h4>
<p>要使用Item Loader, 你必须先将它实例化. 你可以使用类似字典的对象(例如: Item or dict)来进行实例化, 或者不使用对象也可以, 当不用对象进行实例化的时候,Item会自动使用 <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.default_item_class" title="scrapy.contrib.loader.ItemLoader.default_item_class"><tt class="xref py py-attr docutils literal"><span class="pre">ItemLoader.default_item_class</span></tt></a>
属性中指定的Item 类在Item Loader constructor中实例化.</p>
<p>然后,你开始收集数值到Item Loader时,通常使用
<a class="reference internal" href="index.html#topics-selectors"><em>Selectors</em></a>. 你可以在同一个item field 里面添加多个数值;Item Loader将知道如何用合适的处理函数来“添加”这些数值.</p>
<p>下面是在 <a class="reference internal" href="index.html#topics-spiders"><em>Spider</em></a> 中典型的Item Loader的用法, 使用 <a class="reference internal" href="index.html#topics-items"><em>Items chapter</em></a> 中声明的 <a class="reference internal" href="index.html#topics-items-declaring"><em>Product item</em></a>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.loader</span> <span class="kn">import</span> <span class="n">ItemLoader</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="kn">import</span> <span class="n">Product</span>

<span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">item</span><span class="o">=</span><span class="n">Product</span><span class="p">(),</span> <span class="n">response</span><span class="o">=</span><span class="n">response</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;//div[@class=&quot;product_name&quot;]&#39;</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;//div[@class=&quot;product_title&quot;]&#39;</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s">&#39;price&#39;</span><span class="p">,</span> <span class="s">&#39;//p[@id=&quot;price&quot;]&#39;</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_css</span><span class="p">(</span><span class="s">&#39;stock&#39;</span><span class="p">,</span> <span class="s">&#39;p#stock]&#39;</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s">&#39;last_updated&#39;</span><span class="p">,</span> <span class="s">&#39;today&#39;</span><span class="p">)</span> <span class="c"># you can also use literal values</span>
    <span class="k">return</span> <span class="n">l</span><span class="o">.</span><span class="n">load_item</span><span class="p">()</span>
</pre></div>
</div>
<p>快速查看这些代码之后,我们可以看到发现 <tt class="docutils literal"><span class="pre">name</span></tt>  字段被从页面中两个不同的XPath位置提取:</p>
<ol class="arabic simple">
<li><tt class="docutils literal"><span class="pre">//div[&#64;class=&quot;product_name&quot;]</span></tt></li>
<li><tt class="docutils literal"><span class="pre">//div[&#64;class=&quot;product_title&quot;]</span></tt></li>
</ol>
<p>换言之,数据通过用 <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_xpath" title="scrapy.contrib.loader.ItemLoader.add_xpath"><tt class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></tt></a> 的方法,把从两个不同的XPath位置提取的数据收集起来. 这是将在以后分配给 <tt class="docutils literal"><span class="pre">name</span></tt> 字段中的数据｡</p>
<p>之后,类似的请求被用于 <tt class="docutils literal"><span class="pre">price</span></tt> 和 <tt class="docutils literal"><span class="pre">stock</span></tt> 字段
(后者使用 CSS selector 和 <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_css" title="scrapy.contrib.loader.ItemLoader.add_css"><tt class="xref py py-meth docutils literal"><span class="pre">add_css()</span></tt></a> 方法),
最后使用不同的方法 <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_value" title="scrapy.contrib.loader.ItemLoader.add_value"><tt class="xref py py-meth docutils literal"><span class="pre">add_value()</span></tt></a> 对 <tt class="docutils literal"><span class="pre">last_update</span></tt> 填充文本值( <tt class="docutils literal"><span class="pre">today</span></tt> ).</p>
<p>最终, 当所有数据被收集起来之后, 调用 <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.load_item" title="scrapy.contrib.loader.ItemLoader.load_item"><tt class="xref py py-meth docutils literal"><span class="pre">ItemLoader.load_item()</span></tt></a> 方法, 实际上填充并且返回了之前通过调用 <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_xpath" title="scrapy.contrib.loader.ItemLoader.add_xpath"><tt class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></tt></a>,
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_css" title="scrapy.contrib.loader.ItemLoader.add_css"><tt class="xref py py-meth docutils literal"><span class="pre">add_css()</span></tt></a>, and <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_value" title="scrapy.contrib.loader.ItemLoader.add_value"><tt class="xref py py-meth docutils literal"><span class="pre">add_value()</span></tt></a> 所提取和收集到的数据的Item.</p>
</div>
<div class="section" id="input-and-output-processors">
<span id="topics-loaders-processors"></span><h4>Input and Output processors<a class="headerlink" href="#input-and-output-processors" title="永久链接至标题">¶</a></h4>
<p>Item Loader在每个(Item)字段中都包含了一个输入处理器和一个输出处理器｡ 输入处理器收到数据时立刻提取数据 (通过 <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_xpath" title="scrapy.contrib.loader.ItemLoader.add_xpath"><tt class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></tt></a>, <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_css" title="scrapy.contrib.loader.ItemLoader.add_css"><tt class="xref py py-meth docutils literal"><span class="pre">add_css()</span></tt></a> 或者
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_value" title="scrapy.contrib.loader.ItemLoader.add_value"><tt class="xref py py-meth docutils literal"><span class="pre">add_value()</span></tt></a> 方法) 之后输入处理器的结果被收集起来并且保存在ItemLoader内. 收集到所有的数据后, 调用
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.load_item" title="scrapy.contrib.loader.ItemLoader.load_item"><tt class="xref py py-meth docutils literal"><span class="pre">ItemLoader.load_item()</span></tt></a> 方法来填充,并得到填充后的
<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> 对象.  这是当输出处理器被和之前收集到的数据(和用输入处理器处理的)被调用.输出处理器的结果是被分配到Item的最终值｡</p>
<p>让我们看一个例子来说明如何输入和输出处理器被一个特定的字段调用(同样适用于其他field)::</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">l</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">Product</span><span class="p">(),</span> <span class="n">some_selector</span><span class="p">)</span>
<span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="n">xpath1</span><span class="p">)</span> <span class="c"># (1)</span>
<span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="n">xpath2</span><span class="p">)</span> <span class="c"># (2)</span>
<span class="n">l</span><span class="o">.</span><span class="n">add_css</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="n">css</span><span class="p">)</span> <span class="c"># (3)</span>
<span class="n">l</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;test&#39;</span><span class="p">)</span> <span class="c"># (4)</span>
<span class="k">return</span> <span class="n">l</span><span class="o">.</span><span class="n">load_item</span><span class="p">()</span> <span class="c"># (5)</span>
</pre></div>
</div>
<p>发生了这些事情:</p>
<ol class="arabic simple">
<li>从 <tt class="docutils literal"><span class="pre">xpath1</span></tt> 提取出的数据,传递给 <em>输入处理器</em> 的 <tt class="docutils literal"><span class="pre">name</span></tt> 字段.输入处理器的结果被收集和保存在Item Loader中(但尚未分配给该Item)｡</li>
<li>从 <tt class="docutils literal"><span class="pre">xpath2</span></tt> 提取出来的数据,传递给(1)中使用的相同的 <em>输入处理器</em> .输入处理器的结果被附加到在(1)中收集的数据(如果有的话) ｡</li>
<li>This case is similar to the previous ones, except that the data is extracted
from the <tt class="docutils literal"><span class="pre">css</span></tt> CSS selector, and passed through the same <em>input
processor</em> used in (1) and (2). The result of the input processor is appended to the
data collected in (1) and (2) (if any).</li>
<li>This case is also similar to the previous ones, except that the value to be
collected is assigned directly, instead of being extracted from a XPath
expression or a CSS selector.
However, the value is still passed through the input processors. In this
case, since the value is not iterable it is converted to an iterable of a
single element before passing it to the input processor, because input
processor always receive iterables.</li>
<li>The data collected in steps (1), (2), (3) and (4) is passed through
the <em>output processor</em> of the <tt class="docutils literal"><span class="pre">name</span></tt> field.
The result of the output processor is the value assigned to the <tt class="docutils literal"><span class="pre">name</span></tt>
field in the item.</li>
</ol>
<p>It&#8217;s worth noticing that processors are just callable objects, which are called
with the data to be parsed, and return a parsed value. So you can use any
function as input or output processor. The only requirement is that they must
accept one (and only one) positional argument, which will be an iterator.</p>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">Both input and output processors must receive an iterator as their
first argument. The output of those functions can be anything. The result of
input processors will be appended to an internal list (in the Loader)
containing the collected values (for that field). The result of the output
processors is the value that will be finally assigned to the item.</p>
</div>
<p>The other thing you need to keep in mind is that the values returned by input
processors are collected internally (in lists) and then passed to output
processors to populate the fields.</p>
<p>Last, but not least, Scrapy comes with some <a class="reference internal" href="index.html#topics-loaders-available-processors"><em>commonly used processors</em></a> built-in for convenience.</p>
</div>
<div class="section" id="declaring-item-loaders">
<h4>Declaring Item Loaders<a class="headerlink" href="#declaring-item-loaders" title="永久链接至标题">¶</a></h4>
<p>Item Loaders are declared like Items, by using a class definition syntax. Here
is an example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.loader</span> <span class="kn">import</span> <span class="n">ItemLoader</span>
<span class="kn">from</span> <span class="nn">scrapy.contrib.loader.processor</span> <span class="kn">import</span> <span class="n">TakeFirst</span><span class="p">,</span> <span class="n">MapCompose</span><span class="p">,</span> <span class="n">Join</span>

<span class="k">class</span> <span class="nc">ProductLoader</span><span class="p">(</span><span class="n">ItemLoader</span><span class="p">):</span>

    <span class="n">default_output_processor</span> <span class="o">=</span> <span class="n">TakeFirst</span><span class="p">()</span>

    <span class="n">name_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="nb">unicode</span><span class="o">.</span><span class="n">title</span><span class="p">)</span>
    <span class="n">name_out</span> <span class="o">=</span> <span class="n">Join</span><span class="p">()</span>

    <span class="n">price_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="nb">unicode</span><span class="o">.</span><span class="n">strip</span><span class="p">)</span>

    <span class="c"># ...</span>
</pre></div>
</div>
<p>As you can see, input processors are declared using the <tt class="docutils literal"><span class="pre">_in</span></tt> suffix while
output processors are declared using the <tt class="docutils literal"><span class="pre">_out</span></tt> suffix. And you can also
declare a default input/output processors using the
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.default_input_processor" title="scrapy.contrib.loader.ItemLoader.default_input_processor"><tt class="xref py py-attr docutils literal"><span class="pre">ItemLoader.default_input_processor</span></tt></a> and
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.default_output_processor" title="scrapy.contrib.loader.ItemLoader.default_output_processor"><tt class="xref py py-attr docutils literal"><span class="pre">ItemLoader.default_output_processor</span></tt></a> attributes.</p>
</div>
<div class="section" id="declaring-input-and-output-processors">
<span id="topics-loaders-processors-declaring"></span><h4>Declaring Input and Output Processors<a class="headerlink" href="#declaring-input-and-output-processors" title="永久链接至标题">¶</a></h4>
<p>As seen in the previous section, input and output processors can be declared in
the Item Loader definition, and it&#8217;s very common to declare input processors
this way. However, there is one more place where you can specify the input and
output processors to use: in the <a class="reference internal" href="index.html#topics-items-fields"><em>Item Field</em></a>
metadata. Here is an example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">scrapy.contrib.loader.processor</span> <span class="kn">import</span> <span class="n">MapCompose</span><span class="p">,</span> <span class="n">Join</span><span class="p">,</span> <span class="n">TakeFirst</span>

<span class="kn">from</span> <span class="nn">w3lib.html</span> <span class="kn">import</span> <span class="n">remove_entities</span>
<span class="kn">from</span> <span class="nn">myproject.utils</span> <span class="kn">import</span> <span class="n">filter_prices</span>

<span class="k">class</span> <span class="nc">Product</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span>
        <span class="n">input_processor</span><span class="o">=</span><span class="n">MapCompose</span><span class="p">(</span><span class="n">remove_entities</span><span class="p">),</span>
        <span class="n">output_processor</span><span class="o">=</span><span class="n">Join</span><span class="p">(),</span>
    <span class="p">)</span>
    <span class="n">price</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">input_processor</span><span class="o">=</span><span class="n">MapCompose</span><span class="p">(</span><span class="n">remove_entities</span><span class="p">,</span> <span class="n">filter_prices</span><span class="p">),</span>
        <span class="n">output_processor</span><span class="o">=</span><span class="n">TakeFirst</span><span class="p">(),</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>The precedence order, for both input and output processors, is as follows:</p>
<ol class="arabic simple">
<li>Item Loader field-specific attributes: <tt class="docutils literal"><span class="pre">field_in</span></tt> and <tt class="docutils literal"><span class="pre">field_out</span></tt> (most
precedence)</li>
<li>Field metadata (<tt class="docutils literal"><span class="pre">input_processor</span></tt> and <tt class="docutils literal"><span class="pre">output_processor</span></tt> key)</li>
<li>Item Loader defaults: <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.default_input_processor" title="scrapy.contrib.loader.ItemLoader.default_input_processor"><tt class="xref py py-meth docutils literal"><span class="pre">ItemLoader.default_input_processor()</span></tt></a> and
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.default_output_processor" title="scrapy.contrib.loader.ItemLoader.default_output_processor"><tt class="xref py py-meth docutils literal"><span class="pre">ItemLoader.default_output_processor()</span></tt></a> (least precedence)</li>
</ol>
<p>See also: <a class="reference internal" href="index.html#topics-loaders-extending"><em>Reusing and extending Item Loaders</em></a>.</p>
</div>
<div class="section" id="item-loader-context">
<span id="topics-loaders-context"></span><h4>Item Loader Context<a class="headerlink" href="#item-loader-context" title="永久链接至标题">¶</a></h4>
<p>The Item Loader Context is a dict of arbitrary key/values which is shared among
all input and output processors in the Item Loader. It can be passed when
declaring, instantiating or using Item Loader. They are used to modify the
behaviour of the input/output processors.</p>
<p>For example, suppose you have a function <tt class="docutils literal"><span class="pre">parse_length</span></tt> which receives a text
value and extracts a length from it:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">parse_length</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">loader_context</span><span class="p">):</span>
    <span class="n">unit</span> <span class="o">=</span> <span class="n">loader_context</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;unit&#39;</span><span class="p">,</span> <span class="s">&#39;m&#39;</span><span class="p">)</span>
    <span class="c"># ... length parsing code goes here ...</span>
    <span class="k">return</span> <span class="n">parsed_length</span>
</pre></div>
</div>
<p>By accepting a <tt class="docutils literal"><span class="pre">loader_context</span></tt> argument the function is explicitly telling
the Item Loader that it&#8217;s able to receive an Item Loader context, so the Item
Loader passes the currently active context when calling it, and the processor
function (<tt class="docutils literal"><span class="pre">parse_length</span></tt> in this case) can thus use them.</p>
<p>There are several ways to modify Item Loader context values:</p>
<ol class="arabic">
<li><p class="first">By modifying the currently active Item Loader context
(<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.context" title="scrapy.contrib.loader.ItemLoader.context"><tt class="xref py py-attr docutils literal"><span class="pre">context</span></tt></a> attribute):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">loader</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">product</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">context</span><span class="p">[</span><span class="s">&#39;unit&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s">&#39;cm&#39;</span>
</pre></div>
</div>
</li>
<li><p class="first">On Item Loader instantiation (the keyword arguments of Item Loader
constructor are stored in the Item Loader context):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">loader</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">product</span><span class="p">,</span> <span class="n">unit</span><span class="o">=</span><span class="s">&#39;cm&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p class="first">On Item Loader declaration, for those input/output processors that support
instantiating them with an Item Loader context. <a class="reference internal" href="index.html#scrapy.contrib.loader.processor.MapCompose" title="scrapy.contrib.loader.processor.MapCompose"><tt class="xref py py-class docutils literal"><span class="pre">MapCompose</span></tt></a> is one of
them:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">ProductLoader</span><span class="p">(</span><span class="n">ItemLoader</span><span class="p">):</span>
    <span class="n">length_out</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">parse_length</span><span class="p">,</span> <span class="n">unit</span><span class="o">=</span><span class="s">&#39;cm&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="itemloader-objects">
<h4>ItemLoader objects<a class="headerlink" href="#itemloader-objects" title="永久链接至标题">¶</a></h4>
<dl class="class">
<dt id="scrapy.contrib.loader.ItemLoader">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.loader.</tt><tt class="descname">ItemLoader</tt><big>(</big><span class="optional">[</span><em>item</em>, <em>selector</em>, <em>response</em>, <span class="optional">]</span><em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader" title="永久链接至目标">¶</a></dt>
<dd><p>Return a new Item Loader for populating the given Item. If no item is
given, one is instantiated automatically using the class in
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.default_item_class" title="scrapy.contrib.loader.ItemLoader.default_item_class"><tt class="xref py py-attr docutils literal"><span class="pre">default_item_class</span></tt></a>.</p>
<p>When instantiated with a <cite>selector</cite> or a <cite>response</cite> parameters
the <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader" title="scrapy.contrib.loader.ItemLoader"><tt class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></tt></a> class provides convenient mechanisms for extracting
data from web pages using <a class="reference internal" href="index.html#topics-selectors"><em>selectors</em></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>item</strong> (<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> object) &#8211; The item instance to populate using subsequent calls to
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_xpath" title="scrapy.contrib.loader.ItemLoader.add_xpath"><tt class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></tt></a>, <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_css" title="scrapy.contrib.loader.ItemLoader.add_css"><tt class="xref py py-meth docutils literal"><span class="pre">add_css()</span></tt></a>,
or <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_value" title="scrapy.contrib.loader.ItemLoader.add_value"><tt class="xref py py-meth docutils literal"><span class="pre">add_value()</span></tt></a>.</li>
<li><strong>selector</strong> (<a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> object) &#8211; The selector to extract data from, when using the
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_xpath" title="scrapy.contrib.loader.ItemLoader.add_xpath"><tt class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></tt></a> (resp. <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_css" title="scrapy.contrib.loader.ItemLoader.add_css"><tt class="xref py py-meth docutils literal"><span class="pre">add_css()</span></tt></a>) or <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.replace_xpath" title="scrapy.contrib.loader.ItemLoader.replace_xpath"><tt class="xref py py-meth docutils literal"><span class="pre">replace_xpath()</span></tt></a>
(resp. <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.replace_css" title="scrapy.contrib.loader.ItemLoader.replace_css"><tt class="xref py py-meth docutils literal"><span class="pre">replace_css()</span></tt></a>) method.</li>
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object) &#8211; The response used to construct the selector using the
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.default_selector_class" title="scrapy.contrib.loader.ItemLoader.default_selector_class"><tt class="xref py py-attr docutils literal"><span class="pre">default_selector_class</span></tt></a>, unless the selector argument is given,
in which case this argument is ignored.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>The item, selector, response and the remaining keyword arguments are
assigned to the Loader context (accessible through the <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.context" title="scrapy.contrib.loader.ItemLoader.context"><tt class="xref py py-attr docutils literal"><span class="pre">context</span></tt></a> attribute).</p>
<p><a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader" title="scrapy.contrib.loader.ItemLoader"><tt class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></tt></a> instances have the following methods:</p>
<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.get_value">
<tt class="descname">get_value</tt><big>(</big><em>value</em>, <em>*processors</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.get_value" title="永久链接至目标">¶</a></dt>
<dd><p>Process the given <tt class="docutils literal"><span class="pre">value</span></tt> by the given <tt class="docutils literal"><span class="pre">processors</span></tt> and keyword
arguments.</p>
<p>Available keyword arguments:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><strong>re</strong> (<em>str or compiled regex</em>) &#8211; a regular expression to use for extracting data from the
given value using <tt class="xref py py-meth docutils literal"><span class="pre">extract_regex()</span></tt> method,
applied before processors</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.contrib.loader.processor</span> <span class="kn">import</span> <span class="n">TakeFirst</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loader</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="s">u&#39;name: foo&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="nb">unicode</span><span class="o">.</span><span class="n">upper</span><span class="p">,</span> <span class="n">re</span><span class="o">=</span><span class="s">&#39;name: (.+)&#39;</span><span class="p">)</span>
<span class="go">&#39;FOO`</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.add_value">
<tt class="descname">add_value</tt><big>(</big><em>field_name</em>, <em>value</em>, <em>*processors</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.add_value" title="永久链接至目标">¶</a></dt>
<dd><p>Process and then add the given <tt class="docutils literal"><span class="pre">value</span></tt> for the given field.</p>
<p>The value is first passed through <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.get_value" title="scrapy.contrib.loader.ItemLoader.get_value"><tt class="xref py py-meth docutils literal"><span class="pre">get_value()</span></tt></a> by giving the
<tt class="docutils literal"><span class="pre">processors</span></tt> and <tt class="docutils literal"><span class="pre">kwargs</span></tt>, and then passed through the
<a class="reference internal" href="index.html#topics-loaders-processors"><em>field input processor</em></a> and its result
appended to the data collected for that field. If the field already
contains collected data, the new data is added.</p>
<p>The given <tt class="docutils literal"><span class="pre">field_name</span></tt> can be <tt class="docutils literal"><span class="pre">None</span></tt>, in which case values for
multiple fields may be added. And the processed value should be a dict
with field_name mapped to values.</p>
<p>Examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">u&#39;Color TV&#39;</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s">&#39;colours&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s">u&#39;white&#39;</span><span class="p">,</span> <span class="s">u&#39;blue&#39;</span><span class="p">])</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s">&#39;length&#39;</span><span class="p">,</span> <span class="s">u&#39;100&#39;</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">u&#39;name: foo&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="n">re</span><span class="o">=</span><span class="s">&#39;name: (.+)&#39;</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="p">{</span><span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">u&#39;foo&#39;</span><span class="p">,</span> <span class="s">&#39;sex&#39;</span><span class="p">:</span> <span class="s">u&#39;male&#39;</span><span class="p">})</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.replace_value">
<tt class="descname">replace_value</tt><big>(</big><em>field_name</em>, <em>value</em>, <em>*processors</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.replace_value" title="永久链接至目标">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_value" title="scrapy.contrib.loader.ItemLoader.add_value"><tt class="xref py py-meth docutils literal"><span class="pre">add_value()</span></tt></a> but replaces the collected data with the
new value instead of adding it.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.get_xpath">
<tt class="descname">get_xpath</tt><big>(</big><em>xpath</em>, <em>*processors</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.get_xpath" title="永久链接至目标">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.get_value" title="scrapy.contrib.loader.ItemLoader.get_value"><tt class="xref py py-meth docutils literal"><span class="pre">ItemLoader.get_value()</span></tt></a> but receives an XPath instead of a
value, which is used to extract a list of unicode strings from the
selector associated with this <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader" title="scrapy.contrib.loader.ItemLoader"><tt class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>xpath</strong> (<em>str</em>) &#8211; the XPath to extract data from</li>
<li><strong>re</strong> (<em>str or compiled regex</em>) &#8211; a regular expression to use for extracting data from the
selected XPath region</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_xpath</span><span class="p">(</span><span class="s">&#39;//p[@class=&quot;product-name&quot;]&#39;</span><span class="p">)</span>
<span class="c"># HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_xpath</span><span class="p">(</span><span class="s">&#39;//p[@id=&quot;price&quot;]&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="n">re</span><span class="o">=</span><span class="s">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.add_xpath">
<tt class="descname">add_xpath</tt><big>(</big><em>field_name</em>, <em>xpath</em>, <em>*processors</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.add_xpath" title="永久链接至目标">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_value" title="scrapy.contrib.loader.ItemLoader.add_value"><tt class="xref py py-meth docutils literal"><span class="pre">ItemLoader.add_value()</span></tt></a> but receives an XPath instead of a
value, which is used to extract a list of unicode strings from the
selector associated with this <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader" title="scrapy.contrib.loader.ItemLoader"><tt class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></tt></a>.</p>
<p>See <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.get_xpath" title="scrapy.contrib.loader.ItemLoader.get_xpath"><tt class="xref py py-meth docutils literal"><span class="pre">get_xpath()</span></tt></a> for <tt class="docutils literal"><span class="pre">kwargs</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><strong>xpath</strong> (<em>str</em>) &#8211; the XPath to extract data from</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;//p[@class=&quot;product-name&quot;]&#39;</span><span class="p">)</span>
<span class="c"># HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s">&#39;price&#39;</span><span class="p">,</span> <span class="s">&#39;//p[@id=&quot;price&quot;]&#39;</span><span class="p">,</span> <span class="n">re</span><span class="o">=</span><span class="s">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.replace_xpath">
<tt class="descname">replace_xpath</tt><big>(</big><em>field_name</em>, <em>xpath</em>, <em>*processors</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.replace_xpath" title="永久链接至目标">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_xpath" title="scrapy.contrib.loader.ItemLoader.add_xpath"><tt class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></tt></a> but replaces collected data instead of
adding it.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.get_css">
<tt class="descname">get_css</tt><big>(</big><em>css</em>, <em>*processors</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.get_css" title="永久链接至目标">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.get_value" title="scrapy.contrib.loader.ItemLoader.get_value"><tt class="xref py py-meth docutils literal"><span class="pre">ItemLoader.get_value()</span></tt></a> but receives a CSS selector
instead of a value, which is used to extract a list of unicode strings
from the selector associated with this <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader" title="scrapy.contrib.loader.ItemLoader"><tt class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>css</strong> (<em>str</em>) &#8211; the CSS selector to extract data from</li>
<li><strong>re</strong> (<em>str or compiled regex</em>) &#8211; a regular expression to use for extracting data from the
selected CSS region</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_css</span><span class="p">(</span><span class="s">&#39;p.product-name&#39;</span><span class="p">)</span>
<span class="c"># HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_css</span><span class="p">(</span><span class="s">&#39;p#price&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="n">re</span><span class="o">=</span><span class="s">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.add_css">
<tt class="descname">add_css</tt><big>(</big><em>field_name</em>, <em>css</em>, <em>*processors</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.add_css" title="永久链接至目标">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_value" title="scrapy.contrib.loader.ItemLoader.add_value"><tt class="xref py py-meth docutils literal"><span class="pre">ItemLoader.add_value()</span></tt></a> but receives a CSS selector
instead of a value, which is used to extract a list of unicode strings
from the selector associated with this <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader" title="scrapy.contrib.loader.ItemLoader"><tt class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></tt></a>.</p>
<p>See <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.get_css" title="scrapy.contrib.loader.ItemLoader.get_css"><tt class="xref py py-meth docutils literal"><span class="pre">get_css()</span></tt></a> for <tt class="docutils literal"><span class="pre">kwargs</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><strong>css</strong> (<em>str</em>) &#8211; the CSS selector to extract data from</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_css</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;p.product-name&#39;</span><span class="p">)</span>
<span class="c"># HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_css</span><span class="p">(</span><span class="s">&#39;price&#39;</span><span class="p">,</span> <span class="s">&#39;p#price&#39;</span><span class="p">,</span> <span class="n">re</span><span class="o">=</span><span class="s">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.replace_css">
<tt class="descname">replace_css</tt><big>(</big><em>field_name</em>, <em>css</em>, <em>*processors</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.replace_css" title="永久链接至目标">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_css" title="scrapy.contrib.loader.ItemLoader.add_css"><tt class="xref py py-meth docutils literal"><span class="pre">add_css()</span></tt></a> but replaces collected data instead of
adding it.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.load_item">
<tt class="descname">load_item</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.load_item" title="永久链接至目标">¶</a></dt>
<dd><p>Populate the item with the data collected so far, and return it. The
data collected is first passed through the <a class="reference internal" href="index.html#topics-loaders-processors"><em>output processors</em></a> to get the final value to assign to each
item field.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.get_collected_values">
<tt class="descname">get_collected_values</tt><big>(</big><em>field_name</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.get_collected_values" title="永久链接至目标">¶</a></dt>
<dd><p>Return the collected values for the given field.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.get_output_value">
<tt class="descname">get_output_value</tt><big>(</big><em>field_name</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.get_output_value" title="永久链接至目标">¶</a></dt>
<dd><p>Return the collected values parsed using the output processor, for the
given field. This method doesn&#8217;t populate or modify the item at all.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.get_input_processor">
<tt class="descname">get_input_processor</tt><big>(</big><em>field_name</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.get_input_processor" title="永久链接至目标">¶</a></dt>
<dd><p>Return the input processor for the given field.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.get_output_processor">
<tt class="descname">get_output_processor</tt><big>(</big><em>field_name</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.get_output_processor" title="永久链接至目标">¶</a></dt>
<dd><p>Return the output processor for the given field.</p>
</dd></dl>

<p><a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader" title="scrapy.contrib.loader.ItemLoader"><tt class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></tt></a> instances have the following attributes:</p>
<dl class="attribute">
<dt id="scrapy.contrib.loader.ItemLoader.item">
<tt class="descname">item</tt><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.item" title="永久链接至目标">¶</a></dt>
<dd><p>The <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> object being parsed by this Item Loader.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.loader.ItemLoader.context">
<tt class="descname">context</tt><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.context" title="永久链接至目标">¶</a></dt>
<dd><p>The currently active <a class="reference internal" href="index.html#topics-loaders-context"><em>Context</em></a> of this
Item Loader.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.loader.ItemLoader.default_item_class">
<tt class="descname">default_item_class</tt><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.default_item_class" title="永久链接至目标">¶</a></dt>
<dd><p>An Item class (or factory), used to instantiate items when not given in
the constructor.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.loader.ItemLoader.default_input_processor">
<tt class="descname">default_input_processor</tt><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.default_input_processor" title="永久链接至目标">¶</a></dt>
<dd><p>The default input processor to use for those fields which don&#8217;t specify
one.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.loader.ItemLoader.default_output_processor">
<tt class="descname">default_output_processor</tt><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.default_output_processor" title="永久链接至目标">¶</a></dt>
<dd><p>The default output processor to use for those fields which don&#8217;t specify
one.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.loader.ItemLoader.default_selector_class">
<tt class="descname">default_selector_class</tt><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.default_selector_class" title="永久链接至目标">¶</a></dt>
<dd><p>The class used to construct the <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.selector" title="scrapy.contrib.loader.ItemLoader.selector"><tt class="xref py py-attr docutils literal"><span class="pre">selector</span></tt></a> of this
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader" title="scrapy.contrib.loader.ItemLoader"><tt class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></tt></a>, if only a response is given in the constructor.
If a selector is given in the constructor this attribute is ignored.
This attribute is sometimes overridden in subclasses.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.loader.ItemLoader.selector">
<tt class="descname">selector</tt><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.selector" title="永久链接至目标">¶</a></dt>
<dd><p>The <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> object to extract data from.
It&#8217;s either the selector given in the constructor or one created from
the response given in the constructor using the
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.default_selector_class" title="scrapy.contrib.loader.ItemLoader.default_selector_class"><tt class="xref py py-attr docutils literal"><span class="pre">default_selector_class</span></tt></a>. This attribute is meant to be
read-only.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="reusing-and-extending-item-loaders">
<span id="topics-loaders-extending"></span><h4>Reusing and extending Item Loaders<a class="headerlink" href="#reusing-and-extending-item-loaders" title="永久链接至标题">¶</a></h4>
<p>As your project grows bigger and acquires more and more spiders, maintenance
becomes a fundamental problem, especially when you have to deal with many
different parsing rules for each spider, having a lot of exceptions, but also
wanting to reuse the common processors.</p>
<p>Item Loaders are designed to ease the maintenance burden of parsing rules,
without losing flexibility and, at the same time, providing a convenient
mechanism for extending and overriding them. For this reason Item Loaders
support traditional Python class inheritance for dealing with differences of
specific spiders (or groups of spiders).</p>
<p>Suppose, for example, that some particular site encloses their product names in
three dashes (e.g. <tt class="docutils literal"><span class="pre">---Plasma</span> <span class="pre">TV---</span></tt>) and you don&#8217;t want to end up scraping
those dashes in the final product names.</p>
<p>Here&#8217;s how you can remove those dashes by reusing and extending the default
Product Item Loader (<tt class="docutils literal"><span class="pre">ProductLoader</span></tt>):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.loader.processor</span> <span class="kn">import</span> <span class="n">MapCompose</span>
<span class="kn">from</span> <span class="nn">myproject.ItemLoaders</span> <span class="kn">import</span> <span class="n">ProductLoader</span>

<span class="k">def</span> <span class="nf">strip_dashes</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s">&#39;-&#39;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">SiteSpecificLoader</span><span class="p">(</span><span class="n">ProductLoader</span><span class="p">):</span>
    <span class="n">name_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">strip_dashes</span><span class="p">,</span> <span class="n">ProductLoader</span><span class="o">.</span><span class="n">name_in</span><span class="p">)</span>
</pre></div>
</div>
<p>Another case where extending Item Loaders can be very helpful is when you have
multiple source formats, for example XML and HTML. In the XML version you may
want to remove <tt class="docutils literal"><span class="pre">CDATA</span></tt> occurrences. Here&#8217;s an example of how to do it:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.loader.processor</span> <span class="kn">import</span> <span class="n">MapCompose</span>
<span class="kn">from</span> <span class="nn">myproject.ItemLoaders</span> <span class="kn">import</span> <span class="n">ProductLoader</span>
<span class="kn">from</span> <span class="nn">myproject.utils.xml</span> <span class="kn">import</span> <span class="n">remove_cdata</span>

<span class="k">class</span> <span class="nc">XmlProductLoader</span><span class="p">(</span><span class="n">ProductLoader</span><span class="p">):</span>
    <span class="n">name_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">remove_cdata</span><span class="p">,</span> <span class="n">ProductLoader</span><span class="o">.</span><span class="n">name_in</span><span class="p">)</span>
</pre></div>
</div>
<p>And that&#8217;s how you typically extend input processors.</p>
<p>As for output processors, it is more common to declare them in the field metadata,
as they usually depend only on the field and not on each specific site parsing
rule (as input processors do). See also:
<a class="reference internal" href="index.html#topics-loaders-processors-declaring"><em>Declaring Input and Output Processors</em></a>.</p>
<p>There are many other possible ways to extend, inherit and override your Item
Loaders, and different Item Loaders hierarchies may fit better for different
projects. Scrapy only provides the mechanism; it doesn&#8217;t impose any specific
organization of your Loaders collection - that&#8217;s up to you and your project&#8217;s
needs.</p>
</div>
<div class="section" id="module-scrapy.contrib.loader.processor">
<span id="available-built-in-processors"></span><span id="topics-loaders-available-processors"></span><h4>Available built-in processors<a class="headerlink" href="#module-scrapy.contrib.loader.processor" title="永久链接至标题">¶</a></h4>
<p>Even though you can use any callable function as input and output processors,
Scrapy provides some commonly used processors, which are described below. Some
of them, like the <a class="reference internal" href="index.html#scrapy.contrib.loader.processor.MapCompose" title="scrapy.contrib.loader.processor.MapCompose"><tt class="xref py py-class docutils literal"><span class="pre">MapCompose</span></tt></a> (which is typically used as input
processor) compose the output of several functions executed in order, to
produce the final parsed value.</p>
<p>Here is a list of all built-in processors:</p>
<dl class="class">
<dt id="scrapy.contrib.loader.processor.Identity">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.loader.processor.</tt><tt class="descname">Identity</tt><a class="headerlink" href="#scrapy.contrib.loader.processor.Identity" title="永久链接至目标">¶</a></dt>
<dd><p>The simplest processor, which doesn&#8217;t do anything. It returns the original
values unchanged. It doesn&#8217;t receive any constructor arguments nor accepts
Loader contexts.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.contrib.loader.processor</span> <span class="kn">import</span> <span class="n">Identity</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">Identity</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s">&#39;one&#39;</span><span class="p">,</span> <span class="s">&#39;two&#39;</span><span class="p">,</span> <span class="s">&#39;three&#39;</span><span class="p">])</span>
<span class="go">[&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="scrapy.contrib.loader.processor.TakeFirst">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.loader.processor.</tt><tt class="descname">TakeFirst</tt><a class="headerlink" href="#scrapy.contrib.loader.processor.TakeFirst" title="永久链接至目标">¶</a></dt>
<dd><p>Returns the first non-null/non-empty value from the values received,
so it&#8217;s typically used as an output processor to single-valued fields.
It doesn&#8217;t receive any constructor arguments, nor accept Loader contexts.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.contrib.loader.processor</span> <span class="kn">import</span> <span class="n">TakeFirst</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">TakeFirst</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s">&#39;&#39;</span><span class="p">,</span> <span class="s">&#39;one&#39;</span><span class="p">,</span> <span class="s">&#39;two&#39;</span><span class="p">,</span> <span class="s">&#39;three&#39;</span><span class="p">])</span>
<span class="go">&#39;one&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="scrapy.contrib.loader.processor.Join">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.loader.processor.</tt><tt class="descname">Join</tt><big>(</big><em>separator=u' '</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.processor.Join" title="永久链接至目标">¶</a></dt>
<dd><p>Returns the values joined with the separator given in the constructor, which
defaults to <tt class="docutils literal"><span class="pre">u'</span> <span class="pre">'</span></tt>. It doesn&#8217;t accept Loader contexts.</p>
<p>When using the default separator, this processor is equivalent to the
function: <tt class="docutils literal"><span class="pre">u'</span> <span class="pre">'.join</span></tt></p>
<p>Examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.contrib.loader.processor</span> <span class="kn">import</span> <span class="n">Join</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">Join</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s">&#39;one&#39;</span><span class="p">,</span> <span class="s">&#39;two&#39;</span><span class="p">,</span> <span class="s">&#39;three&#39;</span><span class="p">])</span>
<span class="go">u&#39;one two three&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">Join</span><span class="p">(</span><span class="s">&#39;&lt;br&gt;&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s">&#39;one&#39;</span><span class="p">,</span> <span class="s">&#39;two&#39;</span><span class="p">,</span> <span class="s">&#39;three&#39;</span><span class="p">])</span>
<span class="go">u&#39;one&lt;br&gt;two&lt;br&gt;three&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="scrapy.contrib.loader.processor.Compose">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.loader.processor.</tt><tt class="descname">Compose</tt><big>(</big><em>*functions</em>, <em>**default_loader_context</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.processor.Compose" title="永久链接至目标">¶</a></dt>
<dd><p>A processor which is constructed from the composition of the given
functions. This means that each input value of this processor is passed to
the first function, and the result of that function is passed to the second
function, and so on, until the last function returns the output value of
this processor.</p>
<p>By default, stop process on <tt class="docutils literal"><span class="pre">None</span></tt> value. This behaviour can be changed by
passing keyword argument <tt class="docutils literal"><span class="pre">stop_on_none=False</span></tt>.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.contrib.loader.processor</span> <span class="kn">import</span> <span class="n">Compose</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="o">.</span><span class="n">upper</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s">&#39;hello&#39;</span><span class="p">,</span> <span class="s">&#39;world&#39;</span><span class="p">])</span>
<span class="go">&#39;HELLO&#39;</span>
</pre></div>
</div>
<p>Each function can optionally receive a <tt class="docutils literal"><span class="pre">loader_context</span></tt> parameter. For
those which do, this processor will pass the currently active <a class="reference internal" href="index.html#topics-loaders-context"><em>Loader
context</em></a> through that parameter.</p>
<p>The keyword arguments passed in the constructor are used as the default
Loader context values passed to each function call. However, the final
Loader context values passed to functions are overridden with the currently
active Loader context accessible through the <tt class="xref py py-meth docutils literal"><span class="pre">ItemLoader.context()</span></tt>
attribute.</p>
</dd></dl>

<dl class="class">
<dt id="scrapy.contrib.loader.processor.MapCompose">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.loader.processor.</tt><tt class="descname">MapCompose</tt><big>(</big><em>*functions</em>, <em>**default_loader_context</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.processor.MapCompose" title="永久链接至目标">¶</a></dt>
<dd><p>A processor which is constructed from the composition of the given
functions, similar to the <a class="reference internal" href="index.html#scrapy.contrib.loader.processor.Compose" title="scrapy.contrib.loader.processor.Compose"><tt class="xref py py-class docutils literal"><span class="pre">Compose</span></tt></a> processor. The difference with
this processor is the way internal results are passed among functions,
which is as follows:</p>
<p>The input value of this processor is <em>iterated</em> and the first function is
applied to each element. The results of these function calls (one for each element)
are concatenated to construct a new iterable, which is then used to apply the
second function, and so on, until the last function is applied to each
value of the list of values collected so far. The output values of the last
function are concatenated together to produce the output of this processor.</p>
<p>Each particular function can return a value or a list of values, which is
flattened with the list of values returned by the same function applied to
the other input values. The functions can also return <tt class="docutils literal"><span class="pre">None</span></tt> in which
case the output of that function is ignored for further processing over the
chain.</p>
<p>This processor provides a convenient way to compose functions that only
work with single values (instead of iterables). For this reason the
<a class="reference internal" href="index.html#scrapy.contrib.loader.processor.MapCompose" title="scrapy.contrib.loader.processor.MapCompose"><tt class="xref py py-class docutils literal"><span class="pre">MapCompose</span></tt></a> processor is typically used as input processor, since
data is often extracted using the
<a class="reference internal" href="index.html#scrapy.selector.Selector.extract" title="scrapy.selector.Selector.extract"><tt class="xref py py-meth docutils literal"><span class="pre">extract()</span></tt></a> method of <a class="reference internal" href="index.html#topics-selectors"><em>selectors</em></a>, which returns a list of unicode strings.</p>
<p>The example below should clarify how it works:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">filter_world</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="bp">None</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s">&#39;world&#39;</span> <span class="k">else</span> <span class="n">x</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.contrib.loader.processor</span> <span class="kn">import</span> <span class="n">MapCompose</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">filter_world</span><span class="p">,</span> <span class="nb">unicode</span><span class="o">.</span><span class="n">upper</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s">u&#39;hello&#39;</span><span class="p">,</span> <span class="s">u&#39;world&#39;</span><span class="p">,</span> <span class="s">u&#39;this&#39;</span><span class="p">,</span> <span class="s">u&#39;is&#39;</span><span class="p">,</span> <span class="s">u&#39;scrapy&#39;</span><span class="p">])</span>
<span class="go">[u&#39;HELLO, u&#39;THIS&#39;, u&#39;IS&#39;, u&#39;SCRAPY&#39;]</span>
</pre></div>
</div>
<p>As with the Compose processor, functions can receive Loader contexts, and
constructor keyword arguments are used as default context values. See
<a class="reference internal" href="index.html#scrapy.contrib.loader.processor.Compose" title="scrapy.contrib.loader.processor.Compose"><tt class="xref py py-class docutils literal"><span class="pre">Compose</span></tt></a> processor for more info.</p>
</dd></dl>

</div>
</div>
<span id="document-topics/shell"></span><div class="section" id="scrapy-scrapy-shell">
<span id="topics-shell"></span><h3>Scrapy终端(Scrapy shell)<a class="headerlink" href="#scrapy-scrapy-shell" title="永久链接至标题">¶</a></h3>
<p>Scrapy终端是一个交互终端，供您在未启动spider的情况下尝试及调试您的爬取代码。
其本意是用来测试提取数据的代码，不过您可以将其作为正常的Python终端，在上面测试任何的Python代码。</p>
<p>该终端是用来测试XPath或CSS表达式，查看他们的工作方式及从爬取的网页中提取的数据。
在编写您的spider时，该终端提供了交互性测试您的表达式代码的功能，免去了每次修改后运行spider的麻烦。</p>
<p>一旦熟悉了Scrapy终端后，您会发现其在开发和调试spider时发挥的巨大作用。</p>
<p>如果您安装了 <a class="reference external" href="http://ipython.org/">IPython</a> ，Scrapy终端将使用 <a class="reference external" href="http://ipython.org/">IPython</a> (替代标准Python终端)。
<a class="reference external" href="http://ipython.org/">IPython</a> 终端与其他相比更为强大，提供智能的自动补全，高亮输出，及其他特性。</p>
<p>我们强烈推荐您安装 <a class="reference external" href="http://ipython.org/">IPython</a> ，特别是如果您使用Unix系统(<a class="reference external" href="http://ipython.org/">IPython</a> 在Unix下工作的很好)。
详情请参考 <a class="reference external" href="http://ipython.org/install.html">IPython installation guide</a> 。</p>
<div class="section" id="id1">
<h4>启动终端<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h4>
<p>您可以使用 <a class="reference internal" href="index.html#std:command-shell"><tt class="xref std std-command docutils literal"><span class="pre">shell</span></tt></a> 来启动Scrapy终端:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy shell &lt;url&gt;
</pre></div>
</div>
<p><tt class="docutils literal"><span class="pre">&lt;url&gt;</span></tt> 是您要爬取的网页的地址。</p>
</div>
<div class="section" id="id2">
<h4>使用终端<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h4>
<p>Scrapy终端仅仅是一个普通的Python终端(或 <a class="reference external" href="http://ipython.org/">IPython</a> )。其提供了一些额外的快捷方式。</p>
<div class="section" id="shortcut">
<h5>可用的快捷命令(shortcut)<a class="headerlink" href="#shortcut" title="永久链接至标题">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">shelp()</span></tt> - 打印可用对象及快捷命令的帮助列表</li>
<li><tt class="docutils literal"><span class="pre">fetch(request_or_url)</span></tt> - 根据给定的请求(request)或URL获取一个新的response，并更新相关的对象</li>
<li><tt class="docutils literal"><span class="pre">view(response)</span></tt> - 在本机的浏览器打开给定的response。
其会在response的body中添加一个 <a class="reference external" href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/base">&lt;base&gt; tag</a> ，使得外部链接(例如图片及css)能正确显示。
注意，该操作会在本地创建一个临时文件，且该文件不会被自动删除。</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="scrapy">
<h5>可用的Scrapy对象<a class="headerlink" href="#scrapy" title="永久链接至标题">¶</a></h5>
<p>Scrapy终端根据下载的页面会自动创建一些方便使用的对象，例如
<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> 对象及
<a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> 对象(对HTML及XML内容)。</p>
<p>这些对象有:</p>
<blockquote>
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">crawler</span></tt> - 当前 <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><tt class="xref py py-class docutils literal"><span class="pre">Crawler</span></tt></a> 对象.</li>
<li><tt class="docutils literal"><span class="pre">spider</span></tt> - 处理URL的spider。
对当前URL没有处理的Spider时则为一个 <a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> 对象。</li>
<li><tt class="docutils literal"><span class="pre">request</span></tt> - 最近获取到的页面的 <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象。
您可以使用 <a class="reference internal" href="index.html#scrapy.http.Request.replace" title="scrapy.http.Request.replace"><tt class="xref py py-meth docutils literal"><span class="pre">replace()</span></tt></a> 修改该request。或者
使用 <tt class="docutils literal"><span class="pre">fetch</span></tt> 快捷方式来获取新的request。</li>
<li><tt class="docutils literal"><span class="pre">response</span></tt> - 包含最近获取到的页面的 <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> 对象。</li>
<li><tt class="docutils literal"><span class="pre">sel</span></tt> - 根据最近获取到的response构建的 <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> 对象。</li>
<li><tt class="docutils literal"><span class="pre">settings</span></tt> - 当前的 <a class="reference internal" href="index.html#topics-settings"><em>Scrapy settings</em></a></li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="shell-session">
<h4>终端会话(shell session)样例<a class="headerlink" href="#shell-session" title="永久链接至标题">¶</a></h4>
<p>下面给出一个典型的终端会话的例子。
在该例子中，我们首先爬取了 <a class="reference external" href="http://scarpy.org">http://scarpy.org</a> 的页面，而后接着爬取
<a class="reference external" href="http://slashdot.org">http://slashdot.org</a> 的页面。
最后，我们修改了(Slashdot)的请求，将请求设置为POST并重新获取，
得到HTTP 405(不允许的方法)错误。
之后通过Ctrl-D(Unix)或Ctrl-Z(Windows)关闭会话。</p>
<p>需要注意的是，由于爬取的页面不是静态页，内容会随着时间而修改，
因此例子中提取到的数据可能与您尝试的结果不同。
该例子的唯一目的是让您熟悉Scrapy终端。</p>
<p>首先，我们启动终端:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy shell &#39;http://scrapy.org&#39; --nolog
</pre></div>
</div>
<p>接着该终端(使用Scrapy下载器(downloader))获取URL内容并打印可用的对象及快捷命令(注意到以 <tt class="docutils literal"><span class="pre">[s]</span></tt> 开头的行):</p>
<div class="highlight-python"><div class="highlight"><pre>[s] Available Scrapy objects:
[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x1e16b50&gt;
[s]   item       {}
[s]   request    &lt;GET http://scrapy.org&gt;
[s]   response   &lt;200 http://scrapy.org&gt;
[s]   sel        &lt;Selector xpath=None data=u&#39;&lt;html&gt;\n  &lt;head&gt;\n    &lt;meta charset=&quot;utf-8&#39;&gt;
[s]   settings   &lt;scrapy.settings.Settings object at 0x2bfd650&gt;
[s]   spider     &lt;Spider &#39;default&#39; at 0x20c6f50&gt;
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser

&gt;&gt;&gt;
</pre></div>
</div>
<p>之后，您就可以操作这些对象了:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//h2/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="go">u&#39;Welcome to Scrapy&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">fetch</span><span class="p">(</span><span class="s">&quot;http://slashdot.org&quot;</span><span class="p">)</span>
<span class="go">[s] Available Scrapy objects:</span>
<span class="go">[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x1a13b50&gt;</span>
<span class="go">[s]   item       {}</span>
<span class="go">[s]   request    &lt;GET http://slashdot.org&gt;</span>
<span class="go">[s]   response   &lt;200 http://slashdot.org&gt;</span>
<span class="go">[s]   sel        &lt;Selector xpath=None data=u&#39;&lt;html lang=&quot;en&quot;&gt;\n&lt;head&gt;\n\n\n\n\n&lt;script id=&quot;&#39;&gt;</span>
<span class="go">[s]   settings   &lt;scrapy.settings.Settings object at 0x2bfd650&gt;</span>
<span class="go">[s]   spider     &lt;Spider &#39;default&#39; at 0x20c6f50&gt;</span>
<span class="go">[s] Useful shortcuts:</span>
<span class="go">[s]   shelp()           Shell help (print this help)</span>
<span class="go">[s]   fetch(req_or_url) Fetch request (or URL) and update local objects</span>
<span class="go">[s]   view(response)    View response in a browser</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//title/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;Slashdot: News for nerds, stuff that matters&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">request</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s">&quot;POST&quot;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">fetch</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
<span class="go">[s] Available Scrapy objects:</span>
<span class="go">[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x1e16b50&gt;</span>
<span class="gp">...</span>

<span class="go">&gt;&gt;&gt;</span>
</pre></div>
</div>
</div>
<div class="section" id="spidershellresponse">
<span id="topics-shell-inspect-response"></span><h4>在spider中启动shell来查看response<a class="headerlink" href="#spidershellresponse" title="永久链接至标题">¶</a></h4>
<p>有时您想在spider的某个位置中查看被处理的response，
以确认您期望的response到达特定位置。</p>
<p>这可以通过 <tt class="docutils literal"><span class="pre">scrapy.shell.inspect_response</span></tt> 函数来实现。</p>
<p>以下是如何在spider中调用该函数的例子:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&quot;myspider&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">&quot;http://example.com&quot;</span><span class="p">,</span>
        <span class="s">&quot;http://example.org&quot;</span><span class="p">,</span>
        <span class="s">&quot;http://example.net&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c"># We want to inspect one specific response.</span>
        <span class="k">if</span> <span class="s">&quot;.org&quot;</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">scrapy.shell</span> <span class="kn">import</span> <span class="n">inspect_response</span>
            <span class="n">inspect_response</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

        <span class="c"># Rest of parsing code.</span>
</pre></div>
</div>
<p>当运行spider时，您将得到类似下列的输出:</p>
<div class="highlight-python"><div class="highlight"><pre>2014-01-23 17:48:31-0400 [myspider] DEBUG: Crawled (200) &lt;GET http://example.com&gt; (referer: None)
2014-01-23 17:48:31-0400 [myspider] DEBUG: Crawled (200) &lt;GET http://example.org&gt; (referer: None)
[s] Available Scrapy objects:
[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x1e16b50&gt;
...

&gt;&gt;&gt; response.url
&#39;http://example.org&#39;
</pre></div>
</div>
<p>接着测试提取代码:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//h1[@class=&quot;fn&quot;]&#39;</span><span class="p">)</span>
<span class="go">[]</span>
</pre></div>
</div>
<p>呃，看来是没有。您可以在浏览器里查看response的结果，判断是否是您期望的结果:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">view</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<p>最后您可以点击Ctrl-D(Windows下Ctrl-Z)来退出终端，恢复爬取:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="o">^</span><span class="n">D</span>
<span class="go">2014-01-23 17:50:03-0400 [myspider] DEBUG: Crawled (200) &lt;GET http://example.net&gt; (referer: None)</span>
<span class="gp">...</span>
</pre></div>
</div>
<p>注意: 由于该终端屏蔽了Scrapy引擎，您在这个终端中不能使用 <tt class="docutils literal"><span class="pre">fetch</span></tt> 快捷命令(shortcut)。
当您离开终端时，spider会从其停下的地方恢复爬取，正如上面显示的那样。</p>
</div>
</div>
<span id="document-topics/item-pipeline"></span><div class="section" id="item-pipeline">
<span id="topics-item-pipeline"></span><h3>Item Pipeline<a class="headerlink" href="#item-pipeline" title="永久链接至标题">¶</a></h3>
<p>当Item在Spider中被收集之后，它将会被传递到Item Pipeline，一些组件会按照一定的顺序执行对Item的处理。</p>
<p>每个item pipeline组件(有时称之为“Item Pipeline”)是实现了简单方法的Python类。他们接收到Item并通过它执行一些行为，同时也决定此Item是否继续通过pipeline，或是被丢弃而不再进行处理。</p>
<p>以下是item pipeline的一些典型应用：</p>
<ul class="simple">
<li>清理HTML数据</li>
<li>验证爬取的数据(检查item包含某些字段)</li>
<li>查重(并丢弃)</li>
<li>将爬取结果保存到数据库中</li>
</ul>
<div class="section" id="id1">
<h4>编写你自己的item pipeline<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h4>
<p>编写你自己的item pipeline很简单，每个item pipiline组件是一个独立的Python类，同时必须实现以下方法:</p>
<dl class="method">
<dt id="process_item">
<tt class="descname">process_item</tt><big>(</big><em>item</em>, <em>spider</em><big>)</big><a class="headerlink" href="#process_item" title="永久链接至目标">¶</a></dt>
<dd><p>每个item pipeline组件都需要调用该方法，这个方法必须返回一个 <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> (或任何继承类)对象，
或是抛出 <a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><tt class="xref py py-exc docutils literal"><span class="pre">DropItem</span></tt></a> 异常，被丢弃的item将不会被之后的pipeline组件所处理。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>item</strong> (<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> 对象) &#8211; 被爬取的item</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> 对象) &#8211; 爬取该item的spider</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<p>此外,他们也可以实现以下方法:</p>
<dl class="method">
<dt id="open_spider">
<tt class="descname">open_spider</tt><big>(</big><em>spider</em><big>)</big><a class="headerlink" href="#open_spider" title="永久链接至目标">¶</a></dt>
<dd><p>当spider被开启时，这个方法被调用。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> 对象) &#8211; 被开启的spider</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="close_spider">
<tt class="descname">close_spider</tt><big>(</big><em>spider</em><big>)</big><a class="headerlink" href="#close_spider" title="永久链接至目标">¶</a></dt>
<dd><p>当spider被关闭时，这个方法被调用</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> 对象) &#8211; 被关闭的spider</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="id2">
<h4>Item pipeline 样例<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h4>
<div class="section" id="item">
<h5>验证价格，同时丢弃没有价格的item<a class="headerlink" href="#item" title="永久链接至标题">¶</a></h5>
<p>让我们来看一下以下这个假设的pipeline，它为那些不含税(<tt class="docutils literal"><span class="pre">price_excludes_vat</span></tt> 属性)的item调整了 <tt class="docutils literal"><span class="pre">price</span></tt> 属性，同时丢弃了那些没有价格的item:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="kn">import</span> <span class="n">DropItem</span>

<span class="k">class</span> <span class="nc">PricePipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="n">vat_factor</span> <span class="o">=</span> <span class="mf">1.15</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s">&#39;price&#39;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s">&#39;price_excludes_vat&#39;</span><span class="p">]:</span>
                <span class="n">item</span><span class="p">[</span><span class="s">&#39;price&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="s">&#39;price&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">vat_factor</span>
            <span class="k">return</span> <span class="n">item</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span><span class="s">&quot;Missing price in </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">item</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="itemjson">
<h5>将item写入JSON文件<a class="headerlink" href="#itemjson" title="永久链接至标题">¶</a></h5>
<p>以下pipeline将所有(从所有spider中)爬取到的item，存储到一个独立地 <tt class="docutils literal"><span class="pre">items.jl</span></tt> 文件，每行包含一个序列化为JSON格式的item:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">json</span>

<span class="k">class</span> <span class="nc">JsonWriterPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">&#39;items.jl&#39;</span><span class="p">,</span> <span class="s">&#39;wb&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">line</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">item</span><span class="p">))</span> <span class="o">+</span> <span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">JsonWriterPipeline的目的只是为了介绍怎样编写item pipeline，如果你想要将所有爬取的item都保存到同一个JSON文件，
你需要使用 <a class="reference internal" href="index.html#topics-feed-exports"><em>Feed exports</em></a> 。</p>
</div>
</div>
<div class="section" id="id3">
<h5>去重<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h5>
<p>一个用于去重的过滤器，丢弃那些已经被处理过的item。让我们假设我们的item有一个唯一的id，但是我们spider返回的多个item中包含有相同的id:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="kn">import</span> <span class="n">DropItem</span>

<span class="k">class</span> <span class="nc">DuplicatesPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ids_seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s">&#39;id&#39;</span><span class="p">]</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ids_seen</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span><span class="s">&quot;Duplicate item found: </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">item</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ids_seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s">&#39;id&#39;</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id4">
<h4>启用一个Item Pipeline组件<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h4>
<p>为了启用一个Item Pipeline组件，你必须将它的类添加到 <a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES"><tt class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></tt></a> 配置，就像下面这个例子:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">&#39;myproject.pipelines.PricePipeline&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="s">&#39;myproject.pipelines.JsonWriterPipeline&#39;</span><span class="p">:</span> <span class="mi">800</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内。</p>
</div>
</div>
<span id="document-topics/feed-exports"></span><div class="section" id="feed-exports">
<span id="topics-feed-exports"></span><h3>Feed exports<a class="headerlink" href="#feed-exports" title="永久链接至标题">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">0.10 新版功能.</span></p>
</div>
<p>实现爬虫时最经常提到的需求就是能合适的保存爬取到的数据，或者说，生成一个带有爬取数据的&#8221;输出文件&#8221;(通常叫做&#8221;输出feed&#8221;)，来供其他系统使用。</p>
<p>Scrapy自带了Feed输出，并且支持多种序列化格式(serialization format)及存储方式(storage backends)。</p>
<div class="section" id="serialization-formats">
<span id="topics-feed-format"></span><h4>序列化方式(Serialization formats)<a class="headerlink" href="#serialization-formats" title="永久链接至标题">¶</a></h4>
<p>feed输出使用到了
<a class="reference internal" href="index.html#topics-exporters"><em>Item exporters</em></a> 。其自带支持的类型有:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#topics-feed-format-json"><em>JSON</em></a></li>
<li><a class="reference internal" href="index.html#topics-feed-format-jsonlines"><em>JSON lines</em></a></li>
<li><a class="reference internal" href="index.html#topics-feed-format-csv"><em>CSV</em></a></li>
<li><a class="reference internal" href="index.html#topics-feed-format-xml"><em>XML</em></a></li>
</ul>
</div></blockquote>
<p>您也可以通过
<a class="reference internal" href="index.html#std:setting-FEED_EXPORTERS"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_EXPORTERS</span></tt></a> 设置扩展支持的属性。</p>
<div class="section" id="json">
<span id="topics-feed-format-json"></span><h5>JSON<a class="headerlink" href="#json" title="永久链接至标题">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-FEED_FORMAT"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_FORMAT</span></tt></a>: <tt class="docutils literal"><span class="pre">json</span></tt></li>
<li>使用的exporter: <a class="reference internal" href="index.html#scrapy.contrib.exporter.JsonItemExporter" title="scrapy.contrib.exporter.JsonItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">JsonItemExporter</span></tt></a></li>
<li>大数据量情况下使用JSON请参见 <a class="reference internal" href="index.html#json-with-large-data"><em>这个警告</em></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="json-lines">
<span id="topics-feed-format-jsonlines"></span><h5>JSON lines<a class="headerlink" href="#json-lines" title="永久链接至标题">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-FEED_FORMAT"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_FORMAT</span></tt></a>: <tt class="docutils literal"><span class="pre">jsonlines</span></tt></li>
<li>使用的exporter: <a class="reference internal" href="index.html#scrapy.contrib.exporter.JsonLinesItemExporter" title="scrapy.contrib.exporter.JsonLinesItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">JsonLinesItemExporter</span></tt></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="csv">
<span id="topics-feed-format-csv"></span><h5>CSV<a class="headerlink" href="#csv" title="永久链接至标题">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-FEED_FORMAT"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_FORMAT</span></tt></a>: <tt class="docutils literal"><span class="pre">csv</span></tt></li>
<li>使用的exporter: <a class="reference internal" href="index.html#scrapy.contrib.exporter.CsvItemExporter" title="scrapy.contrib.exporter.CsvItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">CsvItemExporter</span></tt></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="xml">
<span id="topics-feed-format-xml"></span><h5>XML<a class="headerlink" href="#xml" title="永久链接至标题">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-FEED_FORMAT"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_FORMAT</span></tt></a>: <tt class="docutils literal"><span class="pre">xml</span></tt></li>
<li>使用的exporter: <a class="reference internal" href="index.html#scrapy.contrib.exporter.XmlItemExporter" title="scrapy.contrib.exporter.XmlItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">XmlItemExporter</span></tt></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="pickle">
<span id="topics-feed-format-pickle"></span><h5>Pickle<a class="headerlink" href="#pickle" title="永久链接至标题">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-FEED_FORMAT"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_FORMAT</span></tt></a>: <tt class="docutils literal"><span class="pre">pickle</span></tt></li>
<li>使用的exporter: <a class="reference internal" href="index.html#scrapy.contrib.exporter.PickleItemExporter" title="scrapy.contrib.exporter.PickleItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">PickleItemExporter</span></tt></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="marshal">
<span id="topics-feed-format-marshal"></span><h5>Marshal<a class="headerlink" href="#marshal" title="永久链接至标题">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-FEED_FORMAT"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_FORMAT</span></tt></a>: <tt class="docutils literal"><span class="pre">marshal</span></tt></li>
<li>使用的exporter: <tt class="xref py py-class docutils literal"><span class="pre">MarshalItemExporter</span></tt></li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="storages">
<span id="topics-feed-storage"></span><h4>存储(Storages)<a class="headerlink" href="#storages" title="永久链接至标题">¶</a></h4>
<p>使用feed输出时您可以通过使用 <a class="reference external" href="http://en.wikipedia.org/wiki/Uniform_Resource_Identifier">URI</a>
(通过 <a class="reference internal" href="index.html#std:setting-FEED_URI"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_URI</span></tt></a> 设置) 来定义存储端。
feed输出支持URI方式支持的多种存储后端类型。</p>
<p>自带支持的存储后端有:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#topics-feed-storage-fs"><em>本地文件系统</em></a></li>
<li><a class="reference internal" href="index.html#topics-feed-storage-ftp"><em>FTP</em></a></li>
<li><a class="reference internal" href="index.html#topics-feed-storage-s3"><em>S3</em></a> (需要 <a class="reference external" href="http://code.google.com/p/boto/">boto</a>)</li>
<li><a class="reference internal" href="index.html#topics-feed-storage-stdout"><em>标准输出</em></a></li>
</ul>
</div></blockquote>
<p>有些存储后端会因所需的外部库未安装而不可用。例如，S3只有在 <a class="reference external" href="http://code.google.com/p/boto/">boto</a> 库安装的情况下才可使用。</p>
</div>
<div class="section" id="uri">
<span id="topics-feed-uri-params"></span><h4>存储URI参数<a class="headerlink" href="#uri" title="永久链接至标题">¶</a></h4>
<p>存储URI也包含参数。当feed被创建时这些参数可以被覆盖:</p>
<blockquote>
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">%(time)s</span></tt> - 当feed被创建时被timestamp覆盖</li>
<li><tt class="docutils literal"><span class="pre">%(name)s</span></tt> - 被spider的名字覆盖</li>
</ul>
</div></blockquote>
<p>其他命名的参数会被spider同名的属性所覆盖。例如，
当feed被创建时， <tt class="docutils literal"><span class="pre">%(site_id)s</span></tt> 将会被
<tt class="docutils literal"><span class="pre">spider.site_id</span></tt> 属性所覆盖。</p>
<p>下面用一些例子来说明:</p>
<blockquote>
<div><ul class="simple">
<li>存储在FTP，每个spider一个目录:<ul>
<li><tt class="docutils literal"><span class="pre">ftp://user:password&#64;ftp.example.com/scraping/feeds/%(name)s/%(time)s.json</span></tt></li>
</ul>
</li>
<li>存储在S3，每一个spider一个目录:<ul>
<li><tt class="docutils literal"><span class="pre">s3://mybucket/scraping/feeds/%(name)s/%(time)s.json</span></tt></li>
</ul>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="storage-backends">
<span id="topics-feed-storage-backends"></span><h4>存储端(Storage backends)<a class="headerlink" href="#storage-backends" title="永久链接至标题">¶</a></h4>
<div class="section" id="topics-feed-storage-fs">
<span id="id1"></span><h5>本地文件系统<a class="headerlink" href="#topics-feed-storage-fs" title="永久链接至标题">¶</a></h5>
<p>将feed存储在本地系统。</p>
<blockquote>
<div><ul class="simple">
<li>URI scheme: <tt class="docutils literal"><span class="pre">file</span></tt></li>
<li>URI样例: <tt class="docutils literal"><span class="pre">file:///tmp/export.csv</span></tt></li>
<li>需要的外部依赖库: none</li>
</ul>
</div></blockquote>
<p>注意: (只有)存储在本地文件系统时，您可以指定一个绝对路径 <tt class="docutils literal"><span class="pre">/tmp/export.csv</span></tt> 并忽略协议(scheme)。不过这仅仅只能在Unix系统中工作。</p>
</div>
<div class="section" id="ftp">
<span id="topics-feed-storage-ftp"></span><h5>FTP<a class="headerlink" href="#ftp" title="永久链接至标题">¶</a></h5>
<p>将feed存储在FTP服务器。</p>
<blockquote>
<div><ul class="simple">
<li>URI scheme: <tt class="docutils literal"><span class="pre">ftp</span></tt></li>
<li>URI样例: <tt class="docutils literal"><span class="pre">ftp://user:pass&#64;ftp.example.com/path/to/export.csv</span></tt></li>
<li>需要的外部依赖库: none</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="s3">
<span id="topics-feed-storage-s3"></span><h5>S3<a class="headerlink" href="#s3" title="永久链接至标题">¶</a></h5>
<p>将feed存储在 <a class="reference external" href="http://aws.amazon.com/s3/">Amazon S3</a> 。</p>
<blockquote>
<div><ul class="simple">
<li>URI scheme: <tt class="docutils literal"><span class="pre">s3</span></tt></li>
<li>URI样例:<ul>
<li><tt class="docutils literal"><span class="pre">s3://mybucket/path/to/export.csv</span></tt></li>
<li><tt class="docutils literal"><span class="pre">s3://aws_key:aws_secret&#64;mybucket/path/to/export.csv</span></tt></li>
</ul>
</li>
<li>需要的外部依赖库: <a class="reference external" href="http://code.google.com/p/boto/">boto</a></li>
</ul>
</div></blockquote>
<p>您可以通过在URI中传递user/pass来完成AWS认证，或者也可以通过下列的设置来完成:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-AWS_ACCESS_KEY_ID"><tt class="xref std std-setting docutils literal"><span class="pre">AWS_ACCESS_KEY_ID</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-AWS_SECRET_ACCESS_KEY"><tt class="xref std std-setting docutils literal"><span class="pre">AWS_SECRET_ACCESS_KEY</span></tt></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="topics-feed-storage-stdout">
<span id="id2"></span><h5>标准输出<a class="headerlink" href="#topics-feed-storage-stdout" title="永久链接至标题">¶</a></h5>
<p>feed输出到Scrapy进程的标准输出。</p>
<blockquote>
<div><ul class="simple">
<li>URI scheme: <tt class="docutils literal"><span class="pre">stdout</span></tt></li>
<li>URI样例: <tt class="docutils literal"><span class="pre">stdout:</span></tt></li>
<li>需要的外部依赖库: none</li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="settings">
<h4>设定(Settings)<a class="headerlink" href="#settings" title="永久链接至标题">¶</a></h4>
<p>这些是配置feed输出的设定:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-FEED_URI"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_URI</span></tt></a> (必须)</li>
<li><a class="reference internal" href="index.html#std:setting-FEED_FORMAT"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_FORMAT</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_STORAGES"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_STORAGES</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_EXPORTERS"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_EXPORTERS</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_STORE_EMPTY"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_STORE_EMPTY</span></tt></a></li>
</ul>
</div></blockquote>
<div class="section" id="feed-uri">
<span id="std:setting-FEED_URI"></span><h5>FEED_URI<a class="headerlink" href="#feed-uri" title="永久链接至标题">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">None</span></tt></p>
<p>输出feed的URI。支持的URI协议请参见
<a class="reference internal" href="index.html#topics-feed-storage-backends"><em>存储端(Storage backends)</em></a> 。</p>
<p>为了启用feed输出，该设定是必须的。</p>
</div>
<div class="section" id="feed-format">
<span id="std:setting-FEED_FORMAT"></span><h5>FEED_FORMAT<a class="headerlink" href="#feed-format" title="永久链接至标题">¶</a></h5>
<p>输出feed的序列化格式。可用的值请参见
<a class="reference internal" href="index.html#topics-feed-format"><em>序列化方式(Serialization formats)</em></a> 。</p>
</div>
<div class="section" id="feed-store-empty">
<span id="std:setting-FEED_STORE_EMPTY"></span><h5>FEED_STORE_EMPTY<a class="headerlink" href="#feed-store-empty" title="永久链接至标题">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>是否输出空feed(没有item的feed)。</p>
</div>
<div class="section" id="feed-storages">
<span id="std:setting-FEED_STORAGES"></span><h5>FEED_STORAGES<a class="headerlink" href="#feed-storages" title="永久链接至标题">¶</a></h5>
<p>Default:: <tt class="docutils literal"><span class="pre">{}</span></tt></p>
<p>包含项目支持的额外feed存储端的字典。
字典的键(key)是URI协议(scheme)，值是存储类(storage class)的路径。</p>
</div>
<div class="section" id="feed-storages-base">
<span id="std:setting-FEED_STORAGES_BASE"></span><h5>FEED_STORAGES_BASE<a class="headerlink" href="#feed-storages-base" title="永久链接至标题">¶</a></h5>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">{</span>
    <span class="s">&#39;&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.contrib.feedexport.FileFeedStorage&#39;</span><span class="p">,</span>
    <span class="s">&#39;file&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.contrib.feedexport.FileFeedStorage&#39;</span><span class="p">,</span>
    <span class="s">&#39;stdout&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.contrib.feedexport.StdoutFeedStorage&#39;</span><span class="p">,</span>
    <span class="s">&#39;s3&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.contrib.feedexport.S3FeedStorage&#39;</span><span class="p">,</span>
    <span class="s">&#39;ftp&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.contrib.feedexport.FTPFeedStorage&#39;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>包含Scrapy内置支持的feed存储端的字典。</p>
</div>
<div class="section" id="feed-exporters">
<span id="std:setting-FEED_EXPORTERS"></span><h5>FEED_EXPORTERS<a class="headerlink" href="#feed-exporters" title="永久链接至标题">¶</a></h5>
<p>Default:: <tt class="docutils literal"><span class="pre">{}</span></tt></p>
<p>包含项目支持的额外输出器(exporter)的字典。
该字典的键(key)是URI协议(scheme)，值是
<a class="reference internal" href="index.html#topics-exporters"><em>Item输出器(exporter)</em></a> 类的路径。</p>
</div>
<div class="section" id="feed-exporters-base">
<span id="std:setting-FEED_EXPORTERS_BASE"></span><h5>FEED_EXPORTERS_BASE<a class="headerlink" href="#feed-exporters-base" title="永久链接至标题">¶</a></h5>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">FEED_EXPORTERS_BASE</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">&#39;json&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.contrib.exporter.JsonItemExporter&#39;</span><span class="p">,</span>
    <span class="s">&#39;jsonlines&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.contrib.exporter.JsonLinesItemExporter&#39;</span><span class="p">,</span>
    <span class="s">&#39;csv&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.contrib.exporter.CsvItemExporter&#39;</span><span class="p">,</span>
    <span class="s">&#39;xml&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.contrib.exporter.XmlItemExporter&#39;</span><span class="p">,</span>
    <span class="s">&#39;marshal&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.contrib.exporter.MarshalItemExporter&#39;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>包含Scrapy内置支持的feed输出器(exporter)的字典。</p>
</div>
</div>
</div>
<span id="document-topics/link-extractors"></span><div class="section" id="link-extractors">
<span id="topics-link-extractors"></span><h3>Link Extractors<a class="headerlink" href="#link-extractors" title="永久链接至标题">¶</a></h3>
<p>Link Extractors 是那些目的仅仅是从网页(<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">scrapy.http.Response</span></tt></a> 对象)中抽取最终将会被follow链接的对象｡</p>
<p>Scrapy默认提供2种可用的 Link Extractor, 但你通过实现一个简单的接口创建自己定制的Link Extractor来满足需求｡</p>
<p>每个LinkExtractor有唯一的公共方法是 <tt class="docutils literal"><span class="pre">extract_links</span></tt> ,它接收一个 <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> 对象,并返回一个 <tt class="xref py py-class docutils literal"><span class="pre">scrapy.link.Link</span></tt> 对象｡Link Extractors,要实例化一次并且 <tt class="docutils literal"><span class="pre">extract_links</span></tt> 方法会根据不同的response调用多次提取链接｡</p>
<p>Link Extractors在 <a class="reference internal" href="index.html#scrapy.contrib.spiders.CrawlSpider" title="scrapy.contrib.spiders.CrawlSpider"><tt class="xref py py-class docutils literal"><span class="pre">CrawlSpider</span></tt></a> 类(在Scrapy可用)中使用, 通过一套规则,但你也可以用它在你的Spider中,即使你不是从 <a class="reference internal" href="index.html#scrapy.contrib.spiders.CrawlSpider" title="scrapy.contrib.spiders.CrawlSpider"><tt class="xref py py-class docutils literal"><span class="pre">CrawlSpider</span></tt></a> 继承的子类, 因为它的目的很简单: 提取链接｡</p>
<div class="section" id="module-scrapy.contrib.linkextractors">
<span id="link-extractor"></span><span id="topics-link-extractors-ref"></span><h4>内置Link Extractor 参考<a class="headerlink" href="#module-scrapy.contrib.linkextractors" title="永久链接至标题">¶</a></h4>
<p>所有与Scrapy绑定且可用的Link Extractors类在 <a class="reference internal" href="index.html#module-scrapy.contrib.linkextractors" title="scrapy.contrib.linkextractors: Link extractors classes"><tt class="xref py py-mod docutils literal"><span class="pre">scrapy.contrib.linkextractors</span></tt></a> 模块提供｡
如果您不知道选择哪个link extractor,使用默认的即可(其实就是LxmlLinkExtractor(参照下面)):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.linkextractors</span> <span class="kn">import</span> <span class="n">LinkExtractor</span>
</pre></div>
</div>
<div class="section" id="module-scrapy.contrib.linkextractors.lxmlhtml">
<span id="lxmllinkextractor"></span><h5>LxmlLinkExtractor<a class="headerlink" href="#module-scrapy.contrib.linkextractors.lxmlhtml" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.linkextractors.lxmlhtml.LxmlLinkExtractor">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.linkextractors.lxmlhtml.</tt><tt class="descname">LxmlLinkExtractor</tt><big>(</big><em>allow=()</em>, <em>deny=()</em>, <em>allow_domains=()</em>, <em>deny_domains=()</em>, <em>deny_extensions=None</em>, <em>restrict_xpaths=()</em>, <em>tags=('a'</em>, <em>'area')</em>, <em>attrs=('href'</em>, <em>)</em>, <em>canonicalize=True</em>, <em>unique=True</em>, <em>process_value=None</em><big>)</big><a class="headerlink" href="#scrapy.contrib.linkextractors.lxmlhtml.LxmlLinkExtractor" title="永久链接至目标">¶</a></dt>
<dd><p>LxmlLinkExtractor is the recommended link extractor with handy filtering
options. It is implemented using lxml&#8217;s robust HTMLParser.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>allow</strong> (<em>a regular expression (or list of)</em>) &#8211; a single regular expression (or list of regular expressions)
that the (absolute) urls must match in order to be extracted. If not
given (or empty), it will match all links.</li>
<li><strong>deny</strong> (<em>a regular expression (or list of)</em>) &#8211; a single regular expression (or list of regular expressions)
that the (absolute) urls must match in order to be excluded (ie. not
extracted). It has precedence over the <tt class="docutils literal"><span class="pre">allow</span></tt> parameter. If not
given (or empty) it won&#8217;t exclude any links.</li>
<li><strong>allow_domains</strong> (<em>str or list</em>) &#8211; a single value or a list of string containing
domains which will be considered for extracting the links</li>
<li><strong>deny_domains</strong> (<em>str or list</em>) &#8211; a single value or a list of strings containing
domains which won&#8217;t be considered for extracting the links</li>
<li><strong>deny_extensions</strong> (<em>list</em>) &#8211; a single value or list of strings containing
extensions that should be ignored when extracting links.
If not given, it will default to the
<tt class="docutils literal"><span class="pre">IGNORED_EXTENSIONS</span></tt> list defined in the <a class="reference external" href="https://github.com/scrapy/scrapy/blob/master/scrapy/linkextractor.py">scrapy.linkextractor</a>
module.</li>
<li><strong>restrict_xpaths</strong> (<em>str or list</em>) &#8211; is a XPath (or list of XPath&#8217;s) which defines
regions inside the response where links should be extracted from.
If given, only the text selected by those XPath will be scanned for
links. See examples below.</li>
<li><strong>tags</strong> (<em>str or list</em>) &#8211; a tag or a list of tags to consider when extracting links.
Defaults to <tt class="docutils literal"><span class="pre">('a',</span> <span class="pre">'area')</span></tt>.</li>
<li><strong>attrs</strong> (<em>list</em>) &#8211; an attribute or list of attributes which should be considered when looking
for links to extract (only for those tags specified in the <tt class="docutils literal"><span class="pre">tags</span></tt>
parameter). Defaults to <tt class="docutils literal"><span class="pre">('href',)</span></tt></li>
<li><strong>canonicalize</strong> (<em>boolean</em>) &#8211; canonicalize each extracted url (using
scrapy.utils.url.canonicalize_url). Defaults to <tt class="docutils literal"><span class="pre">True</span></tt>.</li>
<li><strong>unique</strong> (<em>boolean</em>) &#8211; whether duplicate filtering should be applied to extracted
links.</li>
<li><strong>process_value</strong> (<em>callable</em>) &#8211; see <tt class="docutils literal"><span class="pre">process_value</span></tt> argument of
<tt class="xref py py-class docutils literal"><span class="pre">BaseSgmlLinkExtractor</span></tt> class constructor</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.linkextractors.sgml">
<span id="sgmllinkextractor"></span><h5>SgmlLinkExtractor<a class="headerlink" href="#module-scrapy.contrib.linkextractors.sgml" title="永久链接至标题">¶</a></h5>
<div class="admonition warning">
<p class="first admonition-title">警告</p>
<p class="last">SGMLParser based link extractors are unmantained and its usage is discouraged.
It is recommended to migrate to <tt class="xref py py-class docutils literal"><span class="pre">LxmlLinkExtractor</span></tt> if you are still
using <a class="reference internal" href="index.html#scrapy.contrib.linkextractors.sgml.SgmlLinkExtractor" title="scrapy.contrib.linkextractors.sgml.SgmlLinkExtractor"><tt class="xref py py-class docutils literal"><span class="pre">SgmlLinkExtractor</span></tt></a>.</p>
</div>
<dl class="class">
<dt id="scrapy.contrib.linkextractors.sgml.SgmlLinkExtractor">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.linkextractors.sgml.</tt><tt class="descname">SgmlLinkExtractor</tt><big>(</big><em>allow=()</em>, <em>deny=()</em>, <em>allow_domains=()</em>, <em>deny_domains=()</em>, <em>deny_extensions=None</em>, <em>restrict_xpaths=()</em>, <em>tags=('a'</em>, <em>'area')</em>, <em>attrs=('href')</em>, <em>canonicalize=True</em>, <em>unique=True</em>, <em>process_value=None</em><big>)</big><a class="headerlink" href="#scrapy.contrib.linkextractors.sgml.SgmlLinkExtractor" title="永久链接至目标">¶</a></dt>
<dd><p>SgmlLinkExtractor继承于 <a class="reference internal" href="index.html#scrapy.contrib.linkextractors.sgml.BaseSgmlLinkExtractor" title="scrapy.contrib.linkextractors.sgml.BaseSgmlLinkExtractor"><tt class="xref py py-class docutils literal"><span class="pre">BaseSgmlLinkExtractor</span></tt></a>,其提供了过滤器(filter),以便于提取包括符合正则表达式的链接。
过滤器通过以下构造函数的参数配置:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>allow</strong> (<em>a regular expression (or list of)</em>) &#8211; 必须要匹配这个正则表达式(或正则表达式列表)的URL才会被提取｡如果没有给出(或为空), 它会匹配所有的链接｡</li>
<li><strong>deny</strong> (<em>a regular expression (or list of)</em>) &#8211; 与这个正则表达式(或正则表达式列表)的(绝对)不匹配的URL必须被排除在外(即不提取)｡它的优先级高于 <tt class="docutils literal"><span class="pre">allow</span></tt> 的参数｡如果没有给出(或None), 将不排除任何链接｡</li>
<li><strong>allow_domains</strong> (<em>str or list</em>) &#8211; 单值或者包含字符串域的列表表示会被提取的链接的domains｡</li>
<li><strong>deny_domains</strong> (<em>str or list</em>) &#8211; 单值或包含域名的字符串,将不考虑提取链接的domains｡</li>
<li><strong>deny_extensions</strong> (<em>list</em>) &#8211; 应提取链接时,可以忽略扩展名的列表｡如果没有给出, 它会默认为 <a class="reference external" href="https://github.com/scrapy/scrapy/blob/master/scrapy/linkextractor.py">scrapy.linkextractor</a> 模块中定义的 <tt class="docutils literal"><span class="pre">IGNORED_EXTENSIONS</span></tt> 列表｡</li>
<li><strong>restrict_xpaths</strong> (<em>str or list</em>) &#8211; 一个的XPath (或XPath的列表),它定义了链路应该从提取的响应内的区域｡如果给定的,只有那些XPath的选择的文本将被扫描的链接｡见下面的例子｡</li>
<li><strong>tags</strong> (<em>str or list</em>) &#8211; 提取链接时要考虑的标记或标记列表｡默认为 <tt class="docutils literal"><span class="pre">(</span> <span class="pre">'a'</span> <span class="pre">,</span> <span class="pre">'area')</span></tt> ｡</li>
<li><strong>attrs</strong> (<em>list</em>) &#8211; 提取链接时应该寻找的attrbitues列表(仅在 <tt class="docutils literal"><span class="pre">tag</span></tt> 参数中指定的标签)｡默认为 <tt class="docutils literal"><span class="pre">('href')</span></tt>｡</li>
<li><strong>canonicalize</strong> (<em>boolean</em>) &#8211; 规范化每次提取的URL(使用scrapy.utils.url.canonicalize_url )｡默认为 <tt class="docutils literal"><span class="pre">True</span></tt> ｡</li>
<li><strong>unique</strong> (<em>boolean</em>) &#8211; 重复过滤是否应适用于提取的链接｡</li>
<li><strong>process_value</strong> (<em>callable</em>) &#8211; 见:class:<cite>BaseSgmlLinkExtractor</cite> 类的构造函数 <tt class="docutils literal"><span class="pre">process_value</span></tt> 参数｡</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="basesgmllinkextractor">
<h5>BaseSgmlLinkExtractor<a class="headerlink" href="#basesgmllinkextractor" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.linkextractors.sgml.BaseSgmlLinkExtractor">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.linkextractors.sgml.</tt><tt class="descname">BaseSgmlLinkExtractor</tt><big>(</big><em>tag=&quot;a&quot;</em>, <em>attr=&quot;href&quot;</em>, <em>unique=False</em>, <em>process_value=None</em><big>)</big><a class="headerlink" href="#scrapy.contrib.linkextractors.sgml.BaseSgmlLinkExtractor" title="永久链接至目标">¶</a></dt>
<dd><p>这个Link Extractor的目的只是充当了Sgml Link Extractor的基类｡你应该使用 <a class="reference internal" href="index.html#scrapy.contrib.linkextractors.sgml.SgmlLinkExtractor" title="scrapy.contrib.linkextractors.sgml.SgmlLinkExtractor"><tt class="xref py py-class docutils literal"><span class="pre">SgmlLinkExtractor</span></tt></a>｡</p>
<p>该构造函数的参数是:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>tag</strong> (<em>str or callable</em>) &#8211; 是一个字符串(带标签的名称)或接收一个标签名, 如果链接应该从标签中提取返回 <tt class="docutils literal"><span class="pre">True</span></tt> 的函数或 <tt class="docutils literal"><span class="pre">False</span></tt> 如果他们不应该｡默认为 <tt class="docutils literal"><span class="pre">'a'</span></tt> ｡请求(一旦它被下载)作为其第一个参数｡欲了解更多信息, 请参阅 <a class="reference internal" href="index.html#topics-request-response-ref-request-callback-arguments"><em>Passing additional data to callback functions</em></a>｡</li>
<li><strong>attr</strong> (<em>str or callable</em>) &#8211; 无论是字符串(带有tag属性的名称), 或接收到一个属性名称, 如果链接应该从中提取返回 <tt class="docutils literal"><span class="pre">True</span></tt> 的函数或 <tt class="docutils literal"><span class="pre">False</span></tt> 如果他们不应该｡默认设置为 <tt class="docutils literal"><span class="pre">href</span></tt> ｡</li>
<li><strong>unique</strong> (<em>boolean</em>) &#8211; 是一个布尔值,指定是否重复过滤, 应用于提取链接｡</li>
<li><strong>process_value</strong> (<em>callable</em>) &#8211; <p>它接收来自扫描标签和属性提取每个值, 可以修改该值, 并返回一个新的, 或返回 <tt class="docutils literal"><span class="pre">None</span></tt> 完全忽略链接的功能｡如果没有给出,  <tt class="docutils literal"><span class="pre">process_value</span></tt> 默认是 <tt class="docutils literal"><span class="pre">lambda</span> <span class="pre">x:</span> <span class="pre">x</span></tt>｡</p>
<p>例如,从这段代码中提取链接:</p>
<div class="highlight-html"><div class="highlight"><pre><span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">&quot;javascript:goToPage(&#39;../other/page.html&#39;); return false&quot;</span><span class="nt">&gt;</span>Link text<span class="nt">&lt;/a&gt;</span>
</pre></div>
</div>
<p>你可以使用下面的这个 <tt class="docutils literal"><span class="pre">process_value</span></tt> 函数:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">process_value</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="s">&quot;javascript:goToPage\(&#39;(.*?)&#39;&quot;</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">m</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">m</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-topics/commands"><em>命令行工具(Command line tools)</em></a></dt>
<dd>学习用于管理Scrapy项目的命令行工具</dd>
<dt><a class="reference internal" href="index.html#document-topics/items"><em>Items</em></a></dt>
<dd>定义爬取的数据</dd>
<dt><a class="reference internal" href="index.html#document-topics/spiders"><em>Spiders</em></a></dt>
<dd>编写爬取网站的规则</dd>
<dt><a class="reference internal" href="index.html#document-topics/selectors"><em>选择器(Selectors)</em></a></dt>
<dd>使用XPath提取网页的数据</dd>
<dt><a class="reference internal" href="index.html#document-topics/shell"><em>Scrapy终端(Scrapy shell)</em></a></dt>
<dd>在交互环境中测试提取数据的代码</dd>
<dt><a class="reference internal" href="index.html#document-topics/loaders"><em>Item Loaders</em></a></dt>
<dd>使用爬取到的数据填充item</dd>
<dt><a class="reference internal" href="index.html#document-topics/item-pipeline"><em>Item Pipeline</em></a></dt>
<dd>后处理(Post-process)，存储爬取的数据</dd>
<dt><a class="reference internal" href="index.html#document-topics/feed-exports"><em>Feed exports</em></a></dt>
<dd>以不同格式输出爬取数据到不同的存储端</dd>
<dt><a class="reference internal" href="index.html#document-topics/link-extractors"><em>Link Extractors</em></a></dt>
<dd>方便用于提取后续跟进链接的类。</dd>
</dl>
</div>
<div class="section" id="id5">
<h2>内置服务<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-topics/logging"></span><div class="section" id="logging">
<span id="topics-logging"></span><h3>Logging<a class="headerlink" href="#logging" title="永久链接至标题">¶</a></h3>
<p>Scrapy提供了log功能。您可以通过
<a class="reference internal" href="index.html#module-scrapy.log" title="scrapy.log: Logging facility"><tt class="xref py py-mod docutils literal"><span class="pre">scrapy.log</span></tt></a> 模块使用。当前底层实现使用了 <a class="reference external" href="http://twistedmatrix.com/projects/core/documentation/howto/logging.html">Twisted logging</a> ，不过可能在之后会有所变化。</p>
<p>log服务必须通过显示调用 <a class="reference internal" href="index.html#scrapy.log.start" title="scrapy.log.start"><tt class="xref py py-func docutils literal"><span class="pre">scrapy.log.start()</span></tt></a> 来开启。</p>
<div class="section" id="log-levels">
<span id="topics-logging-levels"></span><h4>Log levels<a class="headerlink" href="#log-levels" title="永久链接至标题">¶</a></h4>
<p>Scrapy提供5层logging级别:</p>
<ol class="arabic simple">
<li><a class="reference internal" href="index.html#scrapy.log.CRITICAL" title="scrapy.log.CRITICAL"><tt class="xref py py-data docutils literal"><span class="pre">CRITICAL</span></tt></a> - 严重错误(critical)</li>
<li><a class="reference internal" href="index.html#scrapy.log.ERROR" title="scrapy.log.ERROR"><tt class="xref py py-data docutils literal"><span class="pre">ERROR</span></tt></a> - 一般错误(regular errors)</li>
<li><a class="reference internal" href="index.html#scrapy.log.WARNING" title="scrapy.log.WARNING"><tt class="xref py py-data docutils literal"><span class="pre">WARNING</span></tt></a> - 警告信息(warning messages)</li>
<li><a class="reference internal" href="index.html#scrapy.log.INFO" title="scrapy.log.INFO"><tt class="xref py py-data docutils literal"><span class="pre">INFO</span></tt></a> - 一般信息(informational messages)</li>
<li><a class="reference internal" href="index.html#scrapy.log.DEBUG" title="scrapy.log.DEBUG"><tt class="xref py py-data docutils literal"><span class="pre">DEBUG</span></tt></a> - 调试信息(debugging messages)</li>
</ol>
</div>
<div class="section" id="log">
<h4>如何设置log级别<a class="headerlink" href="#log" title="永久链接至标题">¶</a></h4>
<p>您可以通过终端选项(command line option) <cite>&#8211;loglevel/-L</cite> 或 <a class="reference internal" href="index.html#std:setting-LOG_LEVEL"><tt class="xref std std-setting docutils literal"><span class="pre">LOG_LEVEL</span></tt></a> 来设置log级别。</p>
</div>
<div class="section" id="log-messages">
<h4>如何记录信息(log messages)<a class="headerlink" href="#log-messages" title="永久链接至标题">¶</a></h4>
<p>下面给出如何使用 <tt class="docutils literal"><span class="pre">WARNING</span></tt> 级别来记录信息的例子:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">log</span>
<span class="n">log</span><span class="o">.</span><span class="n">msg</span><span class="p">(</span><span class="s">&quot;This is a warning&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">log</span><span class="o">.</span><span class="n">WARNING</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="spiderlog-logging-from-spiders">
<h4>在Spider中添加log(Logging from Spiders)<a class="headerlink" href="#spiderlog-logging-from-spiders" title="永久链接至标题">¶</a></h4>
<p>在spider中添加log的推荐方式是使用Spider的
<a class="reference internal" href="index.html#scrapy.spider.Spider.log" title="scrapy.spider.Spider.log"><tt class="xref py py-meth docutils literal"><span class="pre">log()</span></tt></a> 方法。该方法会自动在调用
<a class="reference internal" href="index.html#scrapy.log.msg" title="scrapy.log.msg"><tt class="xref py py-func docutils literal"><span class="pre">scrapy.log.msg()</span></tt></a> 时赋值 <tt class="docutils literal"><span class="pre">spider</span></tt> 参数。其他的参数则直接传递给
<a class="reference internal" href="index.html#scrapy.log.msg" title="scrapy.log.msg"><tt class="xref py py-func docutils literal"><span class="pre">msg()</span></tt></a> 方法。</p>
</div>
<div class="section" id="module-scrapy.log">
<span id="scrapy-log"></span><h4>scrapy.log模块<a class="headerlink" href="#module-scrapy.log" title="永久链接至标题">¶</a></h4>
<dl class="function">
<dt id="scrapy.log.start">
<tt class="descclassname">scrapy.log.</tt><tt class="descname">start</tt><big>(</big><em>logfile=None</em>, <em>loglevel=None</em>, <em>logstdout=None</em><big>)</big><a class="headerlink" href="#scrapy.log.start" title="永久链接至目标">¶</a></dt>
<dd><p>启动log功能。该方法必须在记录(log)任何信息前被调用。否则调用前的信息将会丢失。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>logfile</strong> (<em>str</em>) &#8211; 用于保存log输出的文件路径。如果被忽略，
<a class="reference internal" href="index.html#std:setting-LOG_FILE"><tt class="xref std std-setting docutils literal"><span class="pre">LOG_FILE</span></tt></a> 设置会被使用。
如果两个参数都是 <tt class="docutils literal"><span class="pre">None</span></tt> ，log将会被输出到标准错误流(standard error)。</li>
<li><strong>loglevel</strong> &#8211; 记录的最低的log级别. 可用的值有:
<a class="reference internal" href="index.html#scrapy.log.CRITICAL" title="scrapy.log.CRITICAL"><tt class="xref py py-data docutils literal"><span class="pre">CRITICAL</span></tt></a>, <a class="reference internal" href="index.html#scrapy.log.ERROR" title="scrapy.log.ERROR"><tt class="xref py py-data docutils literal"><span class="pre">ERROR</span></tt></a>, <a class="reference internal" href="index.html#scrapy.log.WARNING" title="scrapy.log.WARNING"><tt class="xref py py-data docutils literal"><span class="pre">WARNING</span></tt></a>, <a class="reference internal" href="index.html#scrapy.log.INFO" title="scrapy.log.INFO"><tt class="xref py py-data docutils literal"><span class="pre">INFO</span></tt></a> and
<a class="reference internal" href="index.html#scrapy.log.DEBUG" title="scrapy.log.DEBUG"><tt class="xref py py-data docutils literal"><span class="pre">DEBUG</span></tt></a>.</li>
<li><strong>logstdout</strong> (<em>boolean</em>) &#8211; 如果为 <tt class="docutils literal"><span class="pre">True</span></tt> ，
所有您的应用的标准输出(包括错误)将会被记录(logged instead)。
例如，如果您调用 &#8220;print &#8216;hello&#8217;&#8221; ，则&#8217;hello&#8217; 会在Scrapy的log中被显示。
如果被忽略，则 <a class="reference internal" href="index.html#std:setting-LOG_STDOUT"><tt class="xref std std-setting docutils literal"><span class="pre">LOG_STDOUT</span></tt></a> 设置会被使用。</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="scrapy.log.msg">
<tt class="descclassname">scrapy.log.</tt><tt class="descname">msg</tt><big>(</big><em>message</em>, <em>level=INFO</em>, <em>spider=None</em><big>)</big><a class="headerlink" href="#scrapy.log.msg" title="永久链接至目标">¶</a></dt>
<dd><p>记录信息(Log a message)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>message</strong> (<em>str</em>) &#8211; log的信息</li>
<li><strong>level</strong> &#8211; 该信息的log级别. 参考
<a class="reference internal" href="index.html#topics-logging-levels"><em>Log levels</em></a>.</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> 对象) &#8211; 记录该信息的spider. 当记录的信息和特定的spider有关联时，该参数必须被使用。</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="data">
<dt id="scrapy.log.CRITICAL">
<tt class="descclassname">scrapy.log.</tt><tt class="descname">CRITICAL</tt><a class="headerlink" href="#scrapy.log.CRITICAL" title="永久链接至目标">¶</a></dt>
<dd><p>严重错误的Log级别</p>
</dd></dl>

<dl class="data">
<dt id="scrapy.log.ERROR">
<tt class="descclassname">scrapy.log.</tt><tt class="descname">ERROR</tt><a class="headerlink" href="#scrapy.log.ERROR" title="永久链接至目标">¶</a></dt>
<dd><p>错误的Log级别
Log level for errors</p>
</dd></dl>

<dl class="data">
<dt id="scrapy.log.WARNING">
<tt class="descclassname">scrapy.log.</tt><tt class="descname">WARNING</tt><a class="headerlink" href="#scrapy.log.WARNING" title="永久链接至目标">¶</a></dt>
<dd><p>警告的Log级别
Log level for warnings</p>
</dd></dl>

<dl class="data">
<dt id="scrapy.log.INFO">
<tt class="descclassname">scrapy.log.</tt><tt class="descname">INFO</tt><a class="headerlink" href="#scrapy.log.INFO" title="永久链接至目标">¶</a></dt>
<dd><p>记录信息的Log级别(生产部署时推荐的Log级别)</p>
</dd></dl>

<dl class="data">
<dt id="scrapy.log.DEBUG">
<tt class="descclassname">scrapy.log.</tt><tt class="descname">DEBUG</tt><a class="headerlink" href="#scrapy.log.DEBUG" title="永久链接至目标">¶</a></dt>
<dd><p>调试信息的Log级别(开发时推荐的Log级别)</p>
</dd></dl>

</div>
<div class="section" id="id1">
<h4>Logging设置<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h4>
<p>以下设置可以被用来配置logging:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-LOG_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">LOG_ENABLED</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_ENCODING"><tt class="xref std std-setting docutils literal"><span class="pre">LOG_ENCODING</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_FILE"><tt class="xref std std-setting docutils literal"><span class="pre">LOG_FILE</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_LEVEL"><tt class="xref std std-setting docutils literal"><span class="pre">LOG_LEVEL</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_STDOUT"><tt class="xref std std-setting docutils literal"><span class="pre">LOG_STDOUT</span></tt></a></li>
</ul>
</div>
</div>
<span id="document-topics/stats"></span><div class="section" id="stats-collection">
<span id="topics-stats"></span><h3>数据收集(Stats Collection)<a class="headerlink" href="#stats-collection" title="永久链接至标题">¶</a></h3>
<p>Scrapy提供了方便的收集数据的机制。数据以key/value方式存储，值大多是计数值。
该机制叫做数据收集器(Stats Collector)，可以通过
<a class="reference internal" href="index.html#topics-api-crawler"><em>Crawler API</em></a> 的属性 <a class="reference internal" href="index.html#scrapy.crawler.Crawler.stats" title="scrapy.crawler.Crawler.stats"><tt class="xref py py-attr docutils literal"><span class="pre">stats</span></tt></a>
来使用。在下面的章节
<a class="reference internal" href="index.html#topics-stats-usecases"><em>常见数据收集器使用方法</em></a> 将给出例子来说明。</p>
<p>无论数据收集(stats collection)开启或者关闭，数据收集器永远都是可用的。
因此您可以import进自己的模块并使用其API(增加值或者设置新的状态键(stat keys))。
该做法是为了简化数据收集的方法: 您不应该使用超过一行代码来收集您的spider，Scrpay扩展或任何您使用数据收集器代码里头的状态。</p>
<p>数据收集器的另一个特性是(在启用状态下)很高效，(在关闭情况下)非常高效(几乎察觉不到)。</p>
<p>数据收集器对每个spider保持一个状态表。当spider启动时，该表自动打开，当spider关闭时，自动关闭。</p>
<div class="section" id="topics-stats-usecases">
<span id="id1"></span><h4>常见数据收集器使用方法<a class="headerlink" href="#topics-stats-usecases" title="永久链接至标题">¶</a></h4>
<p>通过 <a class="reference internal" href="index.html#scrapy.crawler.Crawler.stats" title="scrapy.crawler.Crawler.stats"><tt class="xref py py-attr docutils literal"><span class="pre">stats</span></tt></a> 属性来使用数据收集器。
下面是在扩展中使用状态的例子:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">ExtensionThatAccessStats</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stats</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stats</span> <span class="o">=</span> <span class="n">stats</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">cls</span><span class="p">(</span><span class="n">crawler</span><span class="o">.</span><span class="n">stats</span><span class="p">)</span>
</pre></div>
</div>
<p>设置数据:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">stats</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="s">&#39;hostname&#39;</span><span class="p">,</span> <span class="n">socket</span><span class="o">.</span><span class="n">gethostname</span><span class="p">())</span>
</pre></div>
</div>
<p>增加数据值:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">stats</span><span class="o">.</span><span class="n">inc_value</span><span class="p">(</span><span class="s">&#39;pages_crawled&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>当新的值比原来的值大时设置数据:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">stats</span><span class="o">.</span><span class="n">max_value</span><span class="p">(</span><span class="s">&#39;max_items_scraped&#39;</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
<p>当新的值比原来的值小时设置数据:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">stats</span><span class="o">.</span><span class="n">min_value</span><span class="p">(</span><span class="s">&#39;min_free_memory_percent&#39;</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
<p>获取数据:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">stats</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="s">&#39;pages_crawled&#39;</span><span class="p">)</span>
<span class="go">8</span>
</pre></div>
</div>
<p>获取所有数据:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">stats</span><span class="o">.</span><span class="n">get_stats</span><span class="p">()</span>
<span class="go">{&#39;pages_crawled&#39;: 1238, &#39;start_time&#39;: datetime.datetime(2009, 7, 14, 21, 47, 28, 977139)}</span>
</pre></div>
</div>
</div>
<div class="section" id="id2">
<h4>可用的数据收集器<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h4>
<p>除了基本的 <tt class="xref py py-class docutils literal"><span class="pre">StatsCollector</span></tt> ，Scrapy也提供了基于 <tt class="xref py py-class docutils literal"><span class="pre">StatsCollector</span></tt> 的数据收集器。
您可以通过 <a class="reference internal" href="index.html#std:setting-STATS_CLASS"><tt class="xref std std-setting docutils literal"><span class="pre">STATS_CLASS</span></tt></a> 设置来选择。默认使用的是
<tt class="xref py py-class docutils literal"><span class="pre">MemoryStatsCollector</span></tt> 。</p>
<span class="target" id="module-scrapy.statscol"></span><div class="section" id="memorystatscollector">
<h5>MemoryStatsCollector<a class="headerlink" href="#memorystatscollector" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.statscol.MemoryStatsCollector">
<em class="property">class </em><tt class="descclassname">scrapy.statscol.</tt><tt class="descname">MemoryStatsCollector</tt><a class="headerlink" href="#scrapy.statscol.MemoryStatsCollector" title="永久链接至目标">¶</a></dt>
<dd><p>一个简单的数据收集器。其在spider运行完毕后将其数据保存在内存中。数据可以通过
<a class="reference internal" href="index.html#scrapy.statscol.MemoryStatsCollector.spider_stats" title="scrapy.statscol.MemoryStatsCollector.spider_stats"><tt class="xref py py-attr docutils literal"><span class="pre">spider_stats</span></tt></a> 属性访问。该属性是一个以spider名字为键(key)的字典。</p>
<p>这是Scrapy的默认选择。</p>
<dl class="attribute">
<dt id="scrapy.statscol.MemoryStatsCollector.spider_stats">
<tt class="descname">spider_stats</tt><a class="headerlink" href="#scrapy.statscol.MemoryStatsCollector.spider_stats" title="永久链接至目标">¶</a></dt>
<dd><p>保存了每个spider最近一次爬取的状态的字典(dict)。该字典以spider名字为键，值也是字典。</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="dummystatscollector">
<h5>DummyStatsCollector<a class="headerlink" href="#dummystatscollector" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.statscol.DummyStatsCollector">
<em class="property">class </em><tt class="descclassname">scrapy.statscol.</tt><tt class="descname">DummyStatsCollector</tt><a class="headerlink" href="#scrapy.statscol.DummyStatsCollector" title="永久链接至目标">¶</a></dt>
<dd><p>该数据收集器并不做任何事情但非常高效(因为什么都不做(写文档的人真调皮o(╯□╰)o))。
您可以通过设置 <a class="reference internal" href="index.html#std:setting-STATS_CLASS"><tt class="xref std std-setting docutils literal"><span class="pre">STATS_CLASS</span></tt></a> 启用这个收集器，来关闭数据收集，提高效率。
不过，数据收集的性能负担相较于Scrapy其他的处理(例如分析页面)来说是非常小的。</p>
</dd></dl>

</div>
</div>
</div>
<span id="document-topics/email"></span><div class="section" id="module-scrapy.mail">
<span id="email"></span><span id="topics-email"></span><h3>发送email<a class="headerlink" href="#module-scrapy.mail" title="永久链接至标题">¶</a></h3>
<p>虽然Python通过 <a class="reference external" href="http://docs.python.org/library/smtplib.html">smtplib</a> 库使得发送email变得很简单，Scrapy仍然提供了自己的实现。
该功能十分易用，同时由于采用了 <a class="reference external" href="http://twistedmatrix.com/documents/current/core/howto/defer-intro.html">Twisted非阻塞式(non-blocking)IO</a> ，其避免了对爬虫的非阻塞式IO的影响。
另外，其也提供了简单的API来发送附件。
通过一些 <a class="reference internal" href="index.html#topics-email-settings"><em>settings</em></a> 设置，您可以很简单的进行配置。</p>
<div class="section" id="id1">
<h4>简单例子<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h4>
<p>有两种方法可以创建邮件发送器(mail sender)。
您可以通过标准构造器(constructor)创建:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.mail</span> <span class="kn">import</span> <span class="n">MailSender</span>
<span class="n">mailer</span> <span class="o">=</span> <span class="n">MailSender</span><span class="p">()</span>
</pre></div>
</div>
<p>或者您可以传递一个Scrapy设置对象，其会参考
<a class="reference internal" href="index.html#topics-email-settings"><em>settings</em></a>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">mailer</span> <span class="o">=</span> <span class="n">MailSender</span><span class="o">.</span><span class="n">from_settings</span><span class="p">(</span><span class="n">settings</span><span class="p">)</span>
</pre></div>
</div>
<p>这是如何来发送邮件了(不包括附件):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">mailer</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">to</span><span class="o">=</span><span class="p">[</span><span class="s">&quot;someone@example.com&quot;</span><span class="p">],</span> <span class="n">subject</span><span class="o">=</span><span class="s">&quot;Some subject&quot;</span><span class="p">,</span> <span class="n">body</span><span class="o">=</span><span class="s">&quot;Some body&quot;</span><span class="p">,</span> <span class="n">cc</span><span class="o">=</span><span class="p">[</span><span class="s">&quot;another@example.com&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="section" id="mailsender">
<h4>MailSender类参考手册<a class="headerlink" href="#mailsender" title="永久链接至标题">¶</a></h4>
<p>在Scrapy中发送email推荐使用MailSender。其同框架中其他的部分一样，使用了
<a class="reference external" href="http://twistedmatrix.com/documents/current/core/howto/defer-intro.html">Twisted非阻塞式(non-blocking)IO</a> 。</p>
<dl class="class">
<dt id="scrapy.mail.MailSender">
<em class="property">class </em><tt class="descclassname">scrapy.mail.</tt><tt class="descname">MailSender</tt><big>(</big><em>smtphost=None</em>, <em>mailfrom=None</em>, <em>smtpuser=None</em>, <em>smtppass=None</em>, <em>smtpport=None</em><big>)</big><a class="headerlink" href="#scrapy.mail.MailSender" title="永久链接至目标">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>smtphost</strong> (<em>str</em>) &#8211; 发送email的SMTP主机(host)。如果忽略，则使用 <a class="reference internal" href="index.html#std:setting-MAIL_HOST"><tt class="xref std std-setting docutils literal"><span class="pre">MAIL_HOST</span></tt></a> 。</li>
<li><strong>mailfrom</strong> (<em>str</em>) &#8211; 用于发送email的地址(address)(填入 <tt class="docutils literal"><span class="pre">From:</span></tt>) 。
如果忽略，则使用 <a class="reference internal" href="index.html#std:setting-MAIL_FROM"><tt class="xref std std-setting docutils literal"><span class="pre">MAIL_FROM</span></tt></a> 。</li>
<li><strong>smtpuser</strong> &#8211; SMTP用户。如果忽略,则使用 <a class="reference internal" href="index.html#std:setting-MAIL_USER"><tt class="xref std std-setting docutils literal"><span class="pre">MAIL_USER</span></tt></a> 。
如果未给定，则将不会进行SMTP认证(authentication)。</li>
<li><strong>smtppass</strong> (<em>str</em>) &#8211; SMTP认证的密码</li>
<li><strong>smtpport</strong> (<em>boolean</em>) &#8211; SMTP连接的短裤</li>
<li><strong>smtptls</strong> &#8211; 强制使用STARTTLS</li>
<li><strong>smtpssl</strong> &#8211; 强制使用SSL连接</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="classmethod">
<dt id="scrapy.mail.MailSender.from_settings">
<em class="property">classmethod </em><tt class="descname">from_settings</tt><big>(</big><em>settings</em><big>)</big><a class="headerlink" href="#scrapy.mail.MailSender.from_settings" title="永久链接至目标">¶</a></dt>
<dd><p>使用Scrapy设置对象来初始化对象。其会参考
<a class="reference internal" href="index.html#topics-email-settings"><em>这些Scrapy设置</em></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><strong>settings</strong> (<a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><tt class="xref py py-class docutils literal"><span class="pre">scrapy.settings.Settings</span></tt></a> object) &#8211; the e-mail recipients</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.mail.MailSender.send">
<tt class="descname">send</tt><big>(</big><em>to</em>, <em>subject</em>, <em>body</em>, <em>cc=None</em>, <em>attachs=()</em>, <em>mimetype='text/plain'</em><big>)</big><a class="headerlink" href="#scrapy.mail.MailSender.send" title="永久链接至目标">¶</a></dt>
<dd><p>发送email到给定的接收者。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>to</strong> (<em>list</em>) &#8211; email接收者</li>
<li><strong>subject</strong> (<em>str</em>) &#8211; email内容</li>
<li><strong>cc</strong> (<em>list</em>) &#8211; 抄送的人</li>
<li><strong>body</strong> (<em>str</em>) &#8211; email的内容</li>
<li><strong>attachs</strong> (<em>iterable</em>) &#8211; 可迭代的元组 <tt class="docutils literal"><span class="pre">(attach_name,</span> <span class="pre">mimetype,</span> <span class="pre">file_object)</span></tt>。
<tt class="docutils literal"><span class="pre">attach_name</span></tt> 是一个在email的附件中显示的名字的字符串，
<tt class="docutils literal"><span class="pre">mimetype</span></tt> 是附件的mime类型，
<tt class="docutils literal"><span class="pre">file_object</span></tt> 是包含附件内容的可读的文件对象。</li>
<li><strong>mimetype</strong> (<em>str</em>) &#8211; email的mime类型</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="mail">
<span id="topics-email-settings"></span><h4>Mail设置<a class="headerlink" href="#mail" title="永久链接至标题">¶</a></h4>
<p>这些设置定义了
<a class="reference internal" href="index.html#scrapy.mail.MailSender" title="scrapy.mail.MailSender"><tt class="xref py py-class docutils literal"><span class="pre">MailSender</span></tt></a> 构造器的默认值。其使得在您不编写任何一行代码的情况下，为您的项目配置实现email通知的功能。</p>
<div class="section" id="mail-from">
<span id="std:setting-MAIL_FROM"></span><h5>MAIL_FROM<a class="headerlink" href="#mail-from" title="永久链接至标题">¶</a></h5>
<p>默认值: <tt class="docutils literal"><span class="pre">'scrapy&#64;localhost'</span></tt></p>
<p>用于发送email的地址(address)(填入 <tt class="docutils literal"><span class="pre">From:</span></tt>) 。</p>
</div>
<div class="section" id="mail-host">
<span id="std:setting-MAIL_HOST"></span><h5>MAIL_HOST<a class="headerlink" href="#mail-host" title="永久链接至标题">¶</a></h5>
<p>默认值: <tt class="docutils literal"><span class="pre">'localhost'</span></tt></p>
<p>发送email的SMTP主机(host)。</p>
</div>
<div class="section" id="mail-port">
<span id="std:setting-MAIL_PORT"></span><h5>MAIL_PORT<a class="headerlink" href="#mail-port" title="永久链接至标题">¶</a></h5>
<p>默认值: <tt class="docutils literal"><span class="pre">25</span></tt></p>
<p>发用邮件的SMTP端口。</p>
</div>
<div class="section" id="mail-user">
<span id="std:setting-MAIL_USER"></span><h5>MAIL_USER<a class="headerlink" href="#mail-user" title="永久链接至标题">¶</a></h5>
<p>默认值: <tt class="docutils literal"><span class="pre">None</span></tt></p>
<p>SMTP用户。如果未给定，则将不会进行SMTP认证(authentication)。</p>
</div>
<div class="section" id="mail-pass">
<span id="std:setting-MAIL_PASS"></span><h5>MAIL_PASS<a class="headerlink" href="#mail-pass" title="永久链接至标题">¶</a></h5>
<p>默认值: <tt class="docutils literal"><span class="pre">None</span></tt></p>
<p>用于SMTP认证，与 <a class="reference internal" href="index.html#std:setting-MAIL_USER"><tt class="xref std std-setting docutils literal"><span class="pre">MAIL_USER</span></tt></a> 配套的密码。</p>
</div>
<div class="section" id="mail-tls">
<span id="std:setting-MAIL_TLS"></span><h5>MAIL_TLS<a class="headerlink" href="#mail-tls" title="永久链接至标题">¶</a></h5>
<p>默认值: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>强制使用STARTTLS。STARTTLS能使得在已经存在的不安全连接上，通过使用SSL/TLS来实现安全连接。</p>
</div>
<div class="section" id="mail-ssl">
<span id="std:setting-MAIL_SSL"></span><h5>MAIL_SSL<a class="headerlink" href="#mail-ssl" title="永久链接至标题">¶</a></h5>
<p>默认值: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>强制使用SSL加密连接。</p>
</div>
</div>
</div>
<span id="document-topics/telnetconsole"></span><div class="section" id="module-scrapy.telnet">
<span id="telnet-telnet-console"></span><span id="topics-telnetconsole"></span><h3>Telnet终端(Telnet Console)<a class="headerlink" href="#module-scrapy.telnet" title="永久链接至标题">¶</a></h3>
<p>Scrapy提供了内置的telnet终端，以供检查，控制Scrapy运行的进程。
telnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以在其中做任何事。</p>
<p>telnet终端是一个
<a class="reference internal" href="index.html#topics-extensions-ref"><em>自带的Scrapy扩展</em></a> 。
该扩展默认为启用，不过您也可以关闭。
关于扩展的更多内容请参考
<a class="reference internal" href="index.html#topics-extensions-ref-telnetconsole"><em>Telnet console 扩展</em></a> 。</p>
<div class="section" id="telnet">
<h4>如何访问telnet终端<a class="headerlink" href="#telnet" title="永久链接至标题">¶</a></h4>
<p>telnet终端监听设置中定义的
<a class="reference internal" href="index.html#std:setting-TELNETCONSOLE_PORT"><tt class="xref std std-setting docutils literal"><span class="pre">TELNETCONSOLE_PORT</span></tt></a> ，默认为 <tt class="docutils literal"><span class="pre">6023</span></tt> 。
访问telnet请输入:</p>
<div class="highlight-none"><div class="highlight"><pre>telnet localhost 6023
&gt;&gt;&gt;
</pre></div>
</div>
<p>Windows及大多数Linux发行版都自带了所需的telnet程序。</p>
</div>
<div class="section" id="id1">
<h4>telnet终端中可用的变量<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h4>
<p>telnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以做任何事情，甚至是导入新终端。</p>
<p>telnet为了方便提供了一些默认定义的变量:</p>
<table border="1" class="docutils">
<colgroup>
<col width="19%" />
<col width="81%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">快捷名称</th>
<th class="head">描述</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><tt class="docutils literal"><span class="pre">crawler</span></tt></td>
<td>Scrapy Crawler (<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><tt class="xref py py-class docutils literal"><span class="pre">scrapy.crawler.Crawler</span></tt></a> 对象)</td>
</tr>
<tr class="row-odd"><td><tt class="docutils literal"><span class="pre">engine</span></tt></td>
<td>Crawler.engine属性</td>
</tr>
<tr class="row-even"><td><tt class="docutils literal"><span class="pre">spider</span></tt></td>
<td>当前激活的爬虫(spider)</td>
</tr>
<tr class="row-odd"><td><tt class="docutils literal"><span class="pre">slot</span></tt></td>
<td>the engine slot</td>
</tr>
<tr class="row-even"><td><tt class="docutils literal"><span class="pre">extensions</span></tt></td>
<td>扩展管理器(manager) (Crawler.extensions属性)</td>
</tr>
<tr class="row-odd"><td><tt class="docutils literal"><span class="pre">stats</span></tt></td>
<td>状态收集器 (Crawler.stats属性)</td>
</tr>
<tr class="row-even"><td><tt class="docutils literal"><span class="pre">settings</span></tt></td>
<td>Scrapy设置(setting)对象 (Crawler.settings属性)</td>
</tr>
<tr class="row-odd"><td><tt class="docutils literal"><span class="pre">est</span></tt></td>
<td>打印引擎状态的报告</td>
</tr>
<tr class="row-even"><td><tt class="docutils literal"><span class="pre">prefs</span></tt></td>
<td>针对内存调试 (参考 <a class="reference internal" href="index.html#topics-leaks"><em>调试内存溢出</em></a>)</td>
</tr>
<tr class="row-odd"><td><tt class="docutils literal"><span class="pre">p</span></tt></td>
<td><a class="reference external" href="http://docs.python.org/library/pprint.html#pprint.pprint">pprint.pprint</a> 函数的简写</td>
</tr>
<tr class="row-even"><td><tt class="docutils literal"><span class="pre">hpy</span></tt></td>
<td>针对内存调试 (参考 <a class="reference internal" href="index.html#topics-leaks"><em>调试内存溢出</em></a>)</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="telnet-console-usage-examples">
<h4>Telnet console usage examples<a class="headerlink" href="#telnet-console-usage-examples" title="永久链接至标题">¶</a></h4>
<p>下面是使用telnet终端的一些例子:</p>
<div class="section" id="id2">
<h5>查看引擎状态<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h5>
<p>在终端中您可以使用Scrapy引擎的 <tt class="docutils literal"><span class="pre">est()</span></tt> 方法来快速查看状态:</p>
<div class="highlight-none"><div class="highlight"><pre>telnet localhost 6023
&gt;&gt;&gt; est()
Execution engine status

time()-engine.start_time                        : 8.62972998619
engine.has_capacity()                           : False
len(engine.downloader.active)                   : 16
engine.scraper.is_idle()                        : False
engine.spider.name                              : followall
engine.spider_is_idle(engine.spider)            : False
engine.slot.closing                             : False
len(engine.slot.inprogress)                     : 16
len(engine.slot.scheduler.dqs or [])            : 0
len(engine.slot.scheduler.mqs)                  : 92
len(engine.scraper.slot.queue)                  : 0
len(engine.scraper.slot.active)                 : 0
engine.scraper.slot.active_size                 : 0
engine.scraper.slot.itemproc_size               : 0
engine.scraper.slot.needs_backout()             : False
</pre></div>
</div>
</div>
<div class="section" id="scrapy">
<h5>暂停，恢复和停止Scrapy引擎<a class="headerlink" href="#scrapy" title="永久链接至标题">¶</a></h5>
<p>暂停:</p>
<div class="highlight-none"><div class="highlight"><pre>telnet localhost 6023
&gt;&gt;&gt; engine.pause()
&gt;&gt;&gt;
</pre></div>
</div>
<p>恢复:</p>
<div class="highlight-none"><div class="highlight"><pre>telnet localhost 6023
&gt;&gt;&gt; engine.unpause()
&gt;&gt;&gt;
</pre></div>
</div>
<p>停止:</p>
<div class="highlight-none"><div class="highlight"><pre>telnet localhost 6023
&gt;&gt;&gt; engine.stop()
Connection closed by foreign host.
</pre></div>
</div>
</div>
</div>
<div class="section" id="id3">
<h4>Telnet终端信号<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h4>
<span class="target" id="std:signal-update_telnet_vars"></span><dl class="function">
<dt id="scrapy.telnet.update_telnet_vars">
<tt class="descclassname">scrapy.telnet.</tt><tt class="descname">update_telnet_vars</tt><big>(</big><em>telnet_vars</em><big>)</big><a class="headerlink" href="#scrapy.telnet.update_telnet_vars" title="永久链接至目标">¶</a></dt>
<dd><p>在telnet终端开启前发送该信号。您可以挂载(hook up)该信号来添加，移除或更新
telnet本地命名空间可用的变量。
您可以通过在您的处理函数(handler)中更新 <tt class="docutils literal"><span class="pre">telnet_vars</span></tt> 字典来实现该修改。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><strong>telnet_vars</strong> (<em>dict</em>) &#8211; telnet变量的字典</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="id4">
<h4>Telnet设定<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h4>
<p>以下是终端的一些设定:</p>
<div class="section" id="telnetconsole-port">
<span id="std:setting-TELNETCONSOLE_PORT"></span><h5>TELNETCONSOLE_PORT<a class="headerlink" href="#telnetconsole-port" title="永久链接至标题">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">[6023,</span> <span class="pre">6073]</span></tt></p>
<p>telnet终端使用的端口范围。如果设为 <tt class="docutils literal"><span class="pre">None</span></tt> 或 <tt class="docutils literal"><span class="pre">0</span></tt> ，
则动态分配端口。</p>
</div>
<div class="section" id="telnetconsole-host">
<span id="std:setting-TELNETCONSOLE_HOST"></span><h5>TELNETCONSOLE_HOST<a class="headerlink" href="#telnetconsole-host" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">'127.0.0.1'</span></tt></p>
<p>telnet终端监听的接口(interface)。</p>
</div>
</div>
</div>
<span id="document-topics/webservice"></span><div class="section" id="web-service">
<span id="topics-webservice"></span><h3>Web Service<a class="headerlink" href="#web-service" title="永久链接至标题">¶</a></h3>
<p>Scrapy提供用于监控及控制运行中的爬虫的web服务(service)。
服务通过 <a class="reference external" href="http://www.jsonrpc.org/">JSON-RPC 2.0</a> 协议提供大部分的资源，不过也有些(只读)资源仅仅输出JSON数据。</p>
<p>Scrapy为管理Scrapy进程提供了一个可扩展的web服务。
您可以通过 <a class="reference internal" href="index.html#std:setting-WEBSERVICE_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">WEBSERVICE_ENABLED</span></tt></a> 来启用服务。
服务将会监听 <a class="reference internal" href="index.html#std:setting-WEBSERVICE_PORT"><tt class="xref std std-setting docutils literal"><span class="pre">WEBSERVICE_PORT</span></tt></a> 的端口，并将记录写入到
<a class="reference internal" href="index.html#std:setting-WEBSERVICE_LOGFILE"><tt class="xref std std-setting docutils literal"><span class="pre">WEBSERVICE_LOGFILE</span></tt></a> 指定的文件中。</p>
<p>web服务是默认启用的 <a class="reference internal" href="index.html#topics-extensions-ref"><em>内置Scrapy扩展</em></a> ，
不过如果您运行的环境内存紧张的话，也可以关闭该扩展。</p>
<div class="section" id="web-service-resources">
<span id="topics-webservice-resources"></span><h4>Web Service资源(resources)<a class="headerlink" href="#web-service-resources" title="永久链接至标题">¶</a></h4>
<p>web service提供多种资源，定义在
<tt class="xref std std-setting docutils literal"><span class="pre">WEBSERVICE_RESOURCES</span></tt> 设置中。 每个资源提供了不同的功能。参考
<a class="reference internal" href="index.html#topics-webservice-resources-ref"><em>可用JSON-RPC对象</em></a> 来查看默认可用的资源。</p>
<p>虽然您可以使用任何协议来实现您的资源，但有两种资源是和Scrapy绑定的:</p>
<ul class="simple">
<li>Simple JSON resources - 只读，输出JSON数据</li>
<li>JSON-RPC resources - 通过使用 <a class="reference external" href="http://www.jsonrpc.org/">JSON-RPC 2.0</a> 协议支持对一些Scrapy对象的直接访问</li>
</ul>
<span class="target" id="module-scrapy.contrib.webservice"></span><div class="section" id="json-rpc">
<span id="topics-webservice-resources-ref"></span><h5>可用JSON-RPC对象<a class="headerlink" href="#json-rpc" title="永久链接至标题">¶</a></h5>
<p>Scrapy默认支持以下JSON-RPC资源:</p>
<div class="section" id="module-scrapy.contrib.webservice.crawler">
<span id="crawler-json-rpc"></span><span id="topics-webservice-crawler"></span><h6>Crawler JSON-RPC资源<a class="headerlink" href="#module-scrapy.contrib.webservice.crawler" title="永久链接至标题">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.webservice.crawler.CrawlerResource">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.webservice.crawler.</tt><tt class="descname">CrawlerResource</tt><a class="headerlink" href="#scrapy.contrib.webservice.crawler.CrawlerResource" title="永久链接至目标">¶</a></dt>
<dd><p>提供对主Crawler对象的访问，来控制Scrapy进程。</p>
<p>默认访问地址: <a class="reference external" href="http://localhost:6080/crawler">http://localhost:6080/crawler</a></p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.webservice.stats">
<span id="stats-collector-json-rpc"></span><h6>状态收集器(Stats Collector)JSON-RPC资源<a class="headerlink" href="#module-scrapy.contrib.webservice.stats" title="永久链接至标题">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.webservice.stats.StatsResource">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.webservice.stats.</tt><tt class="descname">StatsResource</tt><a class="headerlink" href="#scrapy.contrib.webservice.stats.StatsResource" title="永久链接至目标">¶</a></dt>
<dd><p>提供对crawler使用的状态收集器(Stats Collector)的访问。</p>
<p>默认访问地址: <a class="reference external" href="http://localhost:6080/stats">http://localhost:6080/stats</a></p>
</dd></dl>

</div>
<div class="section" id="spider-manager-json-rpc">
<h6>爬虫管理器(Spider Manager)JSON-RPC资源<a class="headerlink" href="#spider-manager-json-rpc" title="永久链接至标题">¶</a></h6>
<p>您可以通过
<a class="reference internal" href="index.html#topics-webservice-crawler"><em>Crawler JSON-RPC资源</em></a> 来访问
爬虫管理器JSON-RPC资源。地址为:
<a class="reference external" href="http://localhost:6080/crawler/spiders">http://localhost:6080/crawler/spiders</a></p>
</div>
<div class="section" id="extension-manager-json-rpc">
<h6>扩展管理器(Extension Manager)JSON-RPC资源<a class="headerlink" href="#extension-manager-json-rpc" title="永久链接至标题">¶</a></h6>
<p>您可以通过
<a class="reference internal" href="index.html#topics-webservice-crawler"><em>Crawler JSON-RPC资源</em></a> 来访问
扩展管理器JSON-RPC资源。地址为:</p>
</div>
</div>
<div class="section" id="json">
<h5>可用JSON资源<a class="headerlink" href="#json" title="永久链接至标题">¶</a></h5>
<p>Scrapy默认提供下列JSON资源:</p>
<div class="section" id="module-scrapy.contrib.webservice.enginestatus">
<span id="id1"></span><h6>引擎状态JSON资源<a class="headerlink" href="#module-scrapy.contrib.webservice.enginestatus" title="永久链接至标题">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.webservice.enginestatus.EngineStatusResource">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.webservice.enginestatus.</tt><tt class="descname">EngineStatusResource</tt><a class="headerlink" href="#scrapy.contrib.webservice.enginestatus.EngineStatusResource" title="永久链接至目标">¶</a></dt>
<dd><p>提供了对引擎状态数据的访问</p>
<p>默认访问地址: <a class="reference external" href="http://localhost:6080/enginestatus">http://localhost:6080/enginestatus</a></p>
</dd></dl>

</div>
</div>
</div>
<div class="section" id="web">
<h4>Web服务设置<a class="headerlink" href="#web" title="永久链接至标题">¶</a></h4>
<p>您可以通过下列选项来设置web服务:</p>
<div class="section" id="webservice-enabled">
<span id="std:setting-WEBSERVICE_ENABLED"></span><h5>WEBSERVICE_ENABLED<a class="headerlink" href="#webservice-enabled" title="永久链接至标题">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>布尔值。确定web服务是否启用(以及说该扩展是否启用)。</p>
</div>
<div class="section" id="webservice-logfile">
<span id="std:setting-WEBSERVICE_LOGFILE"></span><h5>WEBSERVICE_LOGFILE<a class="headerlink" href="#webservice-logfile" title="永久链接至标题">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">None</span></tt></p>
<p>记录对该web服务的http请求的文件。如果未设置，则记录将写到标准scrapy的log中。</p>
</div>
<div class="section" id="webservice-port">
<span id="std:setting-WEBSERVICE_PORT"></span><h5>WEBSERVICE_PORT<a class="headerlink" href="#webservice-port" title="永久链接至标题">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">[6080,</span> <span class="pre">7030]</span></tt></p>
<p>web服务的端口范围。如果设置为 <tt class="docutils literal"><span class="pre">None</span></tt> 或 <tt class="docutils literal"><span class="pre">0</span></tt> ，则使用动态分配的端口。</p>
</div>
<div class="section" id="webservice-host">
<span id="std:setting-WEBSERVICE_HOST"></span><h5>WEBSERVICE_HOST<a class="headerlink" href="#webservice-host" title="永久链接至标题">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">'127.0.0.1'</span></tt></p>
<p>web服务监听的接口(interface)</p>
</div>
<div class="section" id="webservice-resources">
<h5>WEBSERVICE_RESOURCES<a class="headerlink" href="#webservice-resources" title="永久链接至标题">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">{}</span></tt></p>
<p>您的项目所启用的web服务资源的列表(list)。请参考
<a class="reference internal" href="index.html#topics-webservice-resources"><em>Web Service资源(resources)</em></a> 。该列表将会添加/覆盖
<tt class="xref std std-setting docutils literal"><span class="pre">WEBSERVICE_RESOURCES_BASE</span></tt> 中定义的Scrpay默认启用的资源的值。</p>
</div>
<div class="section" id="webservice-resources-base">
<h5>WEBSERVICE_RESOURCES_BASE<a class="headerlink" href="#webservice-resources-base" title="永久链接至标题">¶</a></h5>
<p>Default:</p>
<div class="highlight-none"><div class="highlight"><pre>{
    &#39;scrapy.contrib.webservice.crawler.CrawlerResource&#39;: 1,
    &#39;scrapy.contrib.webservice.enginestatus.EngineStatusResource&#39;: 1,
    &#39;scrapy.contrib.webservice.stats.StatsResource&#39;: 1,
}
</pre></div>
</div>
<p>Scrapy默认提供的web服务资源的列表。
您不应该对您的项目修改这个设置，而是修改 <tt class="xref std std-setting docutils literal"><span class="pre">WEBSERVICE_RESOURCES</span></tt> 。
如果您想要关闭某些资源，您可以在
<tt class="xref std std-setting docutils literal"><span class="pre">WEBSERVICE_RESOURCES</span></tt> 设置其的值为 <tt class="docutils literal"><span class="pre">None</span></tt> 。</p>
</div>
</div>
<div class="section" id="web-resource">
<h4>编写web服务资源(resource)<a class="headerlink" href="#web-resource" title="永久链接至标题">¶</a></h4>
<p>web服务资源的实现采用了Twisted Web API。
Twisted web及Twisted web资源的更多详情请参考 <a class="reference external" href="http://jcalderone.livejournal.com/50562.html">Twisted Web guide</a> 。</p>
<p>编写web服务资源您需要继承 <tt class="xref py py-class docutils literal"><span class="pre">JsonResource</span></tt> 或 <tt class="xref py py-class docutils literal"><span class="pre">JsonRpcResource</span></tt>
并实现 <tt class="xref py py-class docutils literal"><span class="pre">renderGET</span></tt> 方法。</p>
<dl class="class">
<dt id="scrapy.contrib.webservice.enginestatus.scrapy.webservice.JsonResource">
<em class="property">class </em><tt class="descclassname">scrapy.webservice.</tt><tt class="descname">JsonResource</tt><a class="headerlink" href="#scrapy.contrib.webservice.enginestatus.scrapy.webservice.JsonResource" title="永久链接至目标">¶</a></dt>
<dd><p><a class="reference external" href="http://twistedmatrix.com/documents/10.0.0/api/twisted.web.resource.Resource.html">twisted.web.resource.Resource</a> 的子类，实现了一个JSON web服务资源。参考</p>
<dl class="attribute">
<dt id="scrapy.contrib.webservice.enginestatus.scrapy.webservice.JsonResource.ws_name">
<tt class="descname">ws_name</tt><a class="headerlink" href="#scrapy.contrib.webservice.enginestatus.scrapy.webservice.JsonResource.ws_name" title="永久链接至目标">¶</a></dt>
<dd><p>Scrapy web服务的名字，同时也是该资源监听的路径。举个例子，
假设Scrapy web服务监听 <a class="reference external" href="http://localhost:6080/">http://localhost:6080/</a> ，<tt class="docutils literal"><span class="pre">ws_name</span></tt> 是
<tt class="docutils literal"><span class="pre">'resource1'</span></tt> ，则该资源的URL为:</p>
<blockquote>
<div><a class="reference external" href="http://localhost:6080/resource1/">http://localhost:6080/resource1/</a></div></blockquote>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="scrapy.contrib.webservice.enginestatus.scrapy.webservice.JsonRpcResource">
<em class="property">class </em><tt class="descclassname">scrapy.webservice.</tt><tt class="descname">JsonRpcResource</tt><big>(</big><em>crawler</em>, <em>target=None</em><big>)</big><a class="headerlink" href="#scrapy.contrib.webservice.enginestatus.scrapy.webservice.JsonRpcResource" title="永久链接至目标">¶</a></dt>
<dd><p><tt class="xref py py-class docutils literal"><span class="pre">JsonResource</span></tt> 的子类，实现了JSON-RPC资源。
JSON-RPC资源为Python(Scrapy)对象做了一层JSON-RPC API封装。
被封装的资源必须通过
<a class="reference internal" href="index.html#scrapy.contrib.webservice.enginestatus.scrapy.webservice.JsonRpcResource.get_target" title="scrapy.contrib.webservice.enginestatus.scrapy.webservice.JsonRpcResource.get_target"><tt class="xref py py-meth docutils literal"><span class="pre">get_target()</span></tt></a> 方法返回。该方法默认返回构造器传入的目标(target)。</p>
<dl class="method">
<dt id="scrapy.contrib.webservice.enginestatus.scrapy.webservice.JsonRpcResource.get_target">
<tt class="descname">get_target</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.contrib.webservice.enginestatus.scrapy.webservice.JsonRpcResource.get_target" title="永久链接至目标">¶</a></dt>
<dd><p>返回JSON-RPC所封装的对象。默认情况下，返回构造器传入的对象。</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="id2">
<h4>web服务资源例子<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h4>
<div class="section" id="statsresource-json-rpc-resource">
<h5>StatsResource (JSON-RPC resource)<a class="headerlink" href="#statsresource-json-rpc-resource" title="永久链接至标题">¶</a></h5>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.webservice import JsonRpcResource

class StatsResource(JsonRpcResource):

    ws_name = &#39;stats&#39;

    def __init__(self, crawler):
        JsonRpcResource.__init__(self, crawler, crawler.stats)
</pre></div>
</div>
</div>
<div class="section" id="enginestatusresource-json-resource">
<h5>EngineStatusResource (JSON resource)<a class="headerlink" href="#enginestatusresource-json-resource" title="永久链接至标题">¶</a></h5>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.webservice import JsonResource
from scrapy.utils.engine import get_engine_status

class EngineStatusResource(JsonResource):

    ws_name = &#39;enginestatus&#39;

    def __init__(self, crawler, spider_name=None):
        JsonResource.__init__(self, crawler)
        self._spider_name = spider_name
        self.isLeaf = spider_name is not None

    def render_GET(self, txrequest):
        status = get_engine_status(self.crawler.engine)
        if self._spider_name is None:
            return status
        for sp, st in status[&#39;spiders&#39;].items():
            if sp.name == self._spider_name:
                return st

    def getChild(self, name, txrequest):
        return EngineStatusResource(name, self.crawler)
</pre></div>
</div>
</div>
</div>
<div class="section" id="example-of-web-service-client">
<h4>Example of web service client<a class="headerlink" href="#example-of-web-service-client" title="永久链接至标题">¶</a></h4>
<div class="section" id="scrapy-ws-py-script">
<h5>scrapy-ws.py script<a class="headerlink" href="#scrapy-ws-py-script" title="永久链接至标题">¶</a></h5>
<div class="highlight-none"><div class="highlight"><pre>#!/usr/bin/env python
&quot;&quot;&quot;
Example script to control a Scrapy server using its JSON-RPC web service.

It only provides a reduced functionality as its main purpose is to illustrate
how to write a web service client. Feel free to improve or write you own.

Also, keep in mind that the JSON-RPC API is not stable. The recommended way for
controlling a Scrapy server is through the execution queue (see the &quot;queue&quot;
command).

&quot;&quot;&quot;

from __future__ import print_function
import sys, optparse, urllib, json
from urlparse import urljoin

from scrapy.utils.jsonrpc import jsonrpc_client_call, JsonRpcError

def get_commands():
    return {
        &#39;help&#39;: cmd_help,
        &#39;stop&#39;: cmd_stop,
        &#39;list-available&#39;: cmd_list_available,
        &#39;list-running&#39;: cmd_list_running,
        &#39;list-resources&#39;: cmd_list_resources,
        &#39;get-global-stats&#39;: cmd_get_global_stats,
        &#39;get-spider-stats&#39;: cmd_get_spider_stats,
    }

def cmd_help(args, opts):
    &quot;&quot;&quot;help - list available commands&quot;&quot;&quot;
    print(&quot;Available commands:&quot;)
    for _, func in sorted(get_commands().items()):
        print(&quot;  &quot;, func.__doc__)

def cmd_stop(args, opts):
    &quot;&quot;&quot;stop &lt;spider&gt; - stop a running spider&quot;&quot;&quot;
    jsonrpc_call(opts, &#39;crawler/engine&#39;, &#39;close_spider&#39;, args[0])

def cmd_list_running(args, opts):
    &quot;&quot;&quot;list-running - list running spiders&quot;&quot;&quot;
    for x in json_get(opts, &#39;crawler/engine/open_spiders&#39;):
        print(x)

def cmd_list_available(args, opts):
    &quot;&quot;&quot;list-available - list name of available spiders&quot;&quot;&quot;
    for x in jsonrpc_call(opts, &#39;crawler/spiders&#39;, &#39;list&#39;):
        print(x)

def cmd_list_resources(args, opts):
    &quot;&quot;&quot;list-resources - list available web service resources&quot;&quot;&quot;
    for x in json_get(opts, &#39;&#39;)[&#39;resources&#39;]:
        print(x)

def cmd_get_spider_stats(args, opts):
    &quot;&quot;&quot;get-spider-stats &lt;spider&gt; - get stats of a running spider&quot;&quot;&quot;
    stats = jsonrpc_call(opts, &#39;stats&#39;, &#39;get_stats&#39;, args[0])
    for name, value in stats.items():
        print(&quot;%-40s %s&quot; % (name, value))

def cmd_get_global_stats(args, opts):
    &quot;&quot;&quot;get-global-stats - get global stats&quot;&quot;&quot;
    stats = jsonrpc_call(opts, &#39;stats&#39;, &#39;get_stats&#39;)
    for name, value in stats.items():
        print(&quot;%-40s %s&quot; % (name, value))

def get_wsurl(opts, path):
    return urljoin(&quot;http://%s:%s/&quot;% (opts.host, opts.port), path)

def jsonrpc_call(opts, path, method, *args, **kwargs):
    url = get_wsurl(opts, path)
    return jsonrpc_client_call(url, method, *args, **kwargs)

def json_get(opts, path):
    url = get_wsurl(opts, path)
    return json.loads(urllib.urlopen(url).read())

def parse_opts():
    usage = &quot;%prog [options] &lt;command&gt; [arg] ...&quot;
    description = &quot;Scrapy web service control script. Use &#39;%prog help&#39; &quot; \
        &quot;to see the list of available commands.&quot;
    op = optparse.OptionParser(usage=usage, description=description)
    op.add_option(&quot;-H&quot;, dest=&quot;host&quot;, default=&quot;localhost&quot;, \
        help=&quot;Scrapy host to connect to&quot;)
    op.add_option(&quot;-P&quot;, dest=&quot;port&quot;, type=&quot;int&quot;, default=6080, \
        help=&quot;Scrapy port to connect to&quot;)
    opts, args = op.parse_args()
    if not args:
        op.print_help()
        sys.exit(2)
    cmdname, cmdargs, opts = args[0], args[1:], opts
    commands = get_commands()
    if cmdname not in commands:
        sys.stderr.write(&quot;Unknown command: %s\n\n&quot; % cmdname)
        cmd_help(None, None)
        sys.exit(1)
    return commands[cmdname], cmdargs, opts

def main():
    cmd, args, opts = parse_opts()
    try:
        cmd(args, opts)
    except IndexError:
        print(cmd.__doc__)
    except JsonRpcError as e:
        print(str(e))
        if e.data:
            print(&quot;Server Traceback below:&quot;)
            print(e.data)


if __name__ == &#39;__main__&#39;:
    main()
</pre></div>
</div>
</div>
</div>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-topics/logging"><em>Logging</em></a></dt>
<dd>了解Scrapy提供的logging功能。</dd>
<dt><a class="reference internal" href="index.html#document-topics/stats"><em>数据收集(Stats Collection)</em></a></dt>
<dd>收集爬虫运行数据</dd>
<dt><a class="reference internal" href="index.html#document-topics/email"><em>发送email</em></a></dt>
<dd>当特定事件发生时发送邮件通知</dd>
<dt><a class="reference internal" href="index.html#document-topics/telnetconsole"><em>Telnet终端(Telnet Console)</em></a></dt>
<dd>使用内置的Python终端检查运行中的crawler(爬虫)</dd>
<dt><a class="reference internal" href="index.html#document-topics/webservice"><em>Web Service</em></a></dt>
<dd>使用web service对您的爬虫进行监控和管理</dd>
</dl>
</div>
<div class="section" id="id6">
<h2>解决特定问题<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-faq"></span><div class="section" id="faq">
<span id="id1"></span><h3>常见问题(FAQ)<a class="headerlink" href="#faq" title="永久链接至标题">¶</a></h3>
<div class="section" id="scrapybeautifulsouplxml">
<h4>Scrapy相BeautifulSoup或lxml比较,如何呢？<a class="headerlink" href="#scrapybeautifulsouplxml" title="永久链接至标题">¶</a></h4>
<p><a class="reference external" href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> 及 <a class="reference external" href="http://lxml.de/">lxml</a> 是HTML和XML的分析库。Scrapy则是
编写爬虫，爬取网页并获取数据的应用框架(application framework)。</p>
<p>Scrapy提供了内置的机制来提取数据(叫做
<a class="reference internal" href="index.html#topics-selectors"><em>选择器(selectors)</em></a>)。
但如果您觉得使用更为方便，也可以使用 <a class="reference external" href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> (或 <a class="reference external" href="http://lxml.de/">lxml</a>)。
总之，它们仅仅是分析库，可以在任何Python代码中被导入及使用。</p>
<p>换句话说，拿Scrapy与 <a class="reference external" href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> (或 <a class="reference external" href="http://lxml.de/">lxml</a>) 比较就好像是拿
<a class="reference external" href="http://jinja.pocoo.org/2/">jinja2</a> 与 <a class="reference external" href="http://www.djangoproject.com">Django</a> 相比。</p>
</div>
<div class="section" id="scrapypython">
<span id="faq-python-versions"></span><h4>Scrapy支持那些Python版本？<a class="headerlink" href="#scrapypython" title="永久链接至标题">¶</a></h4>
<p>Scrapy仅仅支持Python 2.7。
Python2.6的支持从Scrapy 0.20开始被废弃了。</p>
</div>
<div class="section" id="scrapypython-3">
<h4>Scrapy支持Python 3么？<a class="headerlink" href="#scrapypython-3" title="永久链接至标题">¶</a></h4>
<p>不。但是Python 3.3+的支持已经在计划中了。
现在，Scrapy支持Python 2.7。</p>
<div class="admonition seealso">
<p class="first admonition-title">参见</p>
<p class="last"><a class="reference internal" href="index.html#faq-python-versions"><em>Scrapy支持那些Python版本？</em></a>.</p>
</div>
</div>
<div class="section" id="scrapydjango-x">
<h4>Scrapy是否从Django中&#8221;剽窃&#8221;了X呢？<a class="headerlink" href="#scrapydjango-x" title="永久链接至标题">¶</a></h4>
<p>也许吧，不过我们不喜欢这个词。我们认为 <a class="reference external" href="http://www.djangoproject.com">Django</a> 是一个很好的开源项目，同时也是
一个很好的参考对象，所以我们把其作为Scrapy的启发对象。</p>
<p>我们坚信，如果有些事情已经做得很好了，那就没必要再重复制造轮子。这个想法，作为
开源项目及免费软件的基石之一，不仅仅针对软件，也包括文档，过程，政策等等。
所以，与其自行解决每个问题，我们选择从其他已经很好地解决问题的项目中复制想法(copy idea)
，并把注意力放在真正需要解决的问题上。</p>
<p>如果Scrapy能启发其他的项目，我们将为此而自豪。欢迎来抄(steal)我们！</p>
</div>
<div class="section" id="scrapyhttp">
<h4>Scrapy支持HTTP代理么？<a class="headerlink" href="#scrapyhttp" title="永久链接至标题">¶</a></h4>
<p>是的。(从Scrapy 0.8开始)通过HTTP代理下载中间件对HTTP代理提供了支持。参考
<a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware" title="scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">HttpProxyMiddleware</span></tt></a>.</p>
</div>
<div class="section" id="item">
<h4>如何爬取属性在不同页面的item呢？<a class="headerlink" href="#item" title="永久链接至标题">¶</a></h4>
<p>参考 <a class="reference internal" href="index.html#topics-request-response-ref-request-callback-arguments"><em>Passing additional data to callback functions</em></a>.</p>
</div>
<div class="section" id="scrapy-importerror-nomodule-named-win32api">
<h4>Scrapy退出，ImportError: Nomodule named win32api<a class="headerlink" href="#scrapy-importerror-nomodule-named-win32api" title="永久链接至标题">¶</a></h4>
<p><a class="reference external" href="http://twistedmatrix.com/trac/ticket/3707">这是个Twisted bug</a> ，您需要安装 <a class="reference external" href="http://sourceforge.net/projects/pywin32/">pywin32</a> 。</p>
</div>
<div class="section" id="spider">
<h4>我要如何在spider里模拟用户登录呢?<a class="headerlink" href="#spider" title="永久链接至标题">¶</a></h4>
<p>参考 <a class="reference internal" href="index.html#topics-request-response-ref-request-userlogin"><em>使用FormRequest.from_response()方法模拟用户登录</em></a>.</p>
</div>
<div class="section" id="scrapy">
<h4>Scrapy是以广度优先还是深度优先进行爬取的呢？<a class="headerlink" href="#scrapy" title="永久链接至标题">¶</a></h4>
<p>默认情况下，Scrapy使用 <a class="reference external" href="http://en.wikipedia.org/wiki/LIFO">LIFO</a> 队列来存储等待的请求。简单的说，就是
<a class="reference external" href="http://en.wikipedia.org/wiki/Depth-first_search">深度优先顺序</a> 。深度优先对大多数情况下是更方便的。如果您想以
<a class="reference external" href="http://en.wikipedia.org/wiki/Breadth-first_search">广度优先顺序</a> 进行爬取，你可以设置以下的设定:</p>
<div class="highlight-none"><div class="highlight"><pre>DEPTH_PRIORITY = 1
SCHEDULER_DISK_QUEUE = &#39;scrapy.squeue.PickleFifoDiskQueue&#39;
SCHEDULER_MEMORY_QUEUE = &#39;scrapy.squeue.FifoMemoryQueue&#39;
</pre></div>
</div>
</div>
<div class="section" id="id3">
<h4>我的Scrapy爬虫有内存泄露，怎么办?<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h4>
<p>参考 <a class="reference internal" href="index.html#topics-leaks"><em>调试内存溢出</em></a>.</p>
<p>另外，Python自己也有内存泄露，在
<a class="reference internal" href="index.html#topics-leaks-without-leaks"><em>Leaks without leaks</em></a> 有所描述。</p>
</div>
<div class="section" id="id4">
<h4>如何让Scrapy减少内存消耗?<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h4>
<p>参考上一个问题</p>
</div>
<div class="section" id="spiderhttp">
<h4>我能在spider中使用基本HTTP认证么？<a class="headerlink" href="#spiderhttp" title="永久链接至标题">¶</a></h4>
<p>可以。参考 <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware" title="scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">HttpAuthMiddleware</span></tt></a>.</p>
</div>
<div class="section" id="id5">
<h4>为什么Scrapy下载了英文的页面，而不是我的本国语言？<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h4>
<p>尝试通过覆盖 <a class="reference internal" href="index.html#std:setting-DEFAULT_REQUEST_HEADERS"><tt class="xref std std-setting docutils literal"><span class="pre">DEFAULT_REQUEST_HEADERS</span></tt></a> 设置来修改默认的 <a class="reference external" href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.4">Accept-Language</a> 请求头。</p>
</div>
<div class="section" id="id6">
<h4>我能在哪里找到Scrapy项目的例子？<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h4>
<p>参考 <a class="reference internal" href="index.html#intro-examples"><em>例子</em></a>.</p>
</div>
<div class="section" id="scrapy-spider">
<h4>我能在不创建Scrapy项目的情况下运行一个爬虫(spider)么？<a class="headerlink" href="#scrapy-spider" title="永久链接至标题">¶</a></h4>
<p>是的。您可以使用 <a class="reference internal" href="index.html#std:command-runspider"><tt class="xref std std-command docutils literal"><span class="pre">runspider</span></tt></a> 命令。例如，如果您有个
spider写在 <tt class="docutils literal"><span class="pre">my_spider.py</span></tt> 文件中，您可以运行:</p>
<div class="highlight-none"><div class="highlight"><pre>scrapy runspider my_spider.py
</pre></div>
</div>
<p>详情请参考 <a class="reference internal" href="index.html#std:command-runspider"><tt class="xref std std-command docutils literal"><span class="pre">runspider</span></tt></a> 命令。</p>
</div>
<div class="section" id="filtered-offsite-request">
<h4>我收到了 &#8220;Filtered offsite request&#8221; 消息。如何修复？<a class="headerlink" href="#filtered-offsite-request" title="永久链接至标题">¶</a></h4>
<p>这些消息(以 <tt class="docutils literal"><span class="pre">DEBUG</span></tt> 所记录)并不意味着有问题，所以你可以不修复它们。</p>
<p>这些消息由Offsite Spider中间件(Middleware)所抛出。
该(默认启用的)中间件筛选出了不属于当前spider的站点请求。</p>
<p>更多详情请参见:
<a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware" title="scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">OffsiteMiddleware</span></tt></a>.</p>
</div>
<div class="section" id="id7">
<h4>发布Scrapy爬虫到生产环境的推荐方式？<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h4>
<p>参见 <a class="reference internal" href="index.html#topics-scrapyd"><em>Scrapyd</em></a>.</p>
</div>
<div class="section" id="large-exports-json">
<h4>我能对大数据(large exports)使用JSON么？<a class="headerlink" href="#large-exports-json" title="永久链接至标题">¶</a></h4>
<p>这取决于您的输出有多大。参考
<a class="reference internal" href="index.html#scrapy.contrib.exporter.JsonItemExporter" title="scrapy.contrib.exporter.JsonItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">JsonItemExporter</span></tt></a> 文档中的
<a class="reference internal" href="index.html#json-with-large-data"><em>这个警告</em></a></p>
</div>
<div class="section" id="signal-handler-twisted">
<h4>我能在信号处理器(signal handler)中返回(Twisted)引用么？<a class="headerlink" href="#signal-handler-twisted" title="永久链接至标题">¶</a></h4>
<p>有些信号支持从处理器中返回引用，有些不行。参考
<a class="reference internal" href="index.html#topics-signals-ref"><em>内置信号参考手册(Built-in signals reference)</em></a> 来了解详情。</p>
</div>
<div class="section" id="reponse999">
<h4>reponse返回的状态值999代表了什么?<a class="headerlink" href="#reponse999" title="永久链接至标题">¶</a></h4>
<p>999是雅虎用来控制请求量所定义的返回值。
试着减慢爬取速度，将spider的下载延迟改为 <tt class="docutils literal"><span class="pre">2</span></tt> 或更高:</p>
<div class="highlight-none"><div class="highlight"><pre>class MySpider(CrawlSpider):

    name = &#39;myspider&#39;

    download_delay = 2

    # [ ... rest of the spider code ... ]
</pre></div>
</div>
<p>或在 <a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></tt></a> 中设置项目的全局下载延迟。</p>
</div>
<div class="section" id="spider-pdb-set-trace">
<h4>我能在spider中调用 <tt class="docutils literal"><span class="pre">pdb.set_trace()</span></tt> 来调试么？<a class="headerlink" href="#spider-pdb-set-trace" title="永久链接至标题">¶</a></h4>
<p>可以，但你也可以使用Scrapy终端。这能让你快速分析(甚至修改)
spider处理返回的返回(response)。通常来说，比老旧的 <tt class="docutils literal"><span class="pre">pdb.set_trace()</span></tt> 有用多了。</p>
<p>更多详情请参考 <a class="reference internal" href="index.html#topics-shell-inspect-response"><em>在spider中启动shell来查看response</em></a>.</p>
</div>
<div class="section" id="item-dump-json-csv-xml">
<h4>将所有爬取到的item转存(dump)到JSON/CSV/XML文件的最简单的方法?<a class="headerlink" href="#item-dump-json-csv-xml" title="永久链接至标题">¶</a></h4>
<p>dump到JSON文件:</p>
<div class="highlight-none"><div class="highlight"><pre>scrapy crawl myspider -o items.json
</pre></div>
</div>
<p>dump到CSV文件:</p>
<div class="highlight-none"><div class="highlight"><pre>scrapy crawl myspider -o items.csv
</pre></div>
</div>
<p>dump到XML文件:</p>
<div class="highlight-none"><div class="highlight"><pre>scrapy crawl myspider -o items.xml
</pre></div>
</div>
<p>更多详情请参考 <a class="reference internal" href="index.html#topics-feed-exports"><em>Feed exports</em></a></p>
</div>
<div class="section" id="viewstate">
<h4>在某些表单中巨大神秘的 <tt class="docutils literal"><span class="pre">__VIEWSTATE</span></tt> 参数是什么？<a class="headerlink" href="#viewstate" title="永久链接至标题">¶</a></h4>
<p><tt class="docutils literal"><span class="pre">__VIEWSTATE</span></tt> 参数存在于ASP.NET/VB.NET建立的站点中。关于这个参数的作用请参考
<a class="reference external" href="http://search.cpan.org/~ecarroll/HTML-TreeBuilderX-ASP_NET-0.09/lib/HTML/TreeBuilderX/ASP_NET.pm">这篇文章</a> 。这里有一个爬取这种站点的
<a class="reference external" href="http://github.com/AmbientLighter/rpn-fas/blob/master/fas/spiders/rnp.py">样例爬虫</a> 。</p>
</div>
<div class="section" id="xml-csv">
<h4>分析大XML/CSV数据源的最好方法是?<a class="headerlink" href="#xml-csv" title="永久链接至标题">¶</a></h4>
<p>使用XPath选择器来分析大数据源可能会有问题。选择器需要在内存中对数据建立完整的
DOM树，这过程速度很慢且消耗大量内存。</p>
<p>为了避免一次性读取整个数据源，您可以使用
<tt class="docutils literal"><span class="pre">scrapy.utils.iterators</span></tt> 中的 <tt class="docutils literal"><span class="pre">xmliter</span></tt> 及 <tt class="docutils literal"><span class="pre">csviter</span></tt> 方法。
实际上，这也是feed spider(参考 <a class="reference internal" href="index.html#topics-spiders"><em>Spiders</em></a>)中的处理方法。</p>
</div>
<div class="section" id="scrapycookies">
<h4>Scrapy自动管理cookies么？<a class="headerlink" href="#scrapycookies" title="永久链接至标题">¶</a></h4>
<p>是的，Scrapy接收并保持服务器返回来的cookies，在之后的请求会发送回去，就像正常的网页浏览器做的那样。</p>
<p>更多详情请参考 <a class="reference internal" href="index.html#topics-request-response"><em>Requests and Responses</em></a> 及 <a class="reference internal" href="index.html#cookies-mw"><em>CookiesMiddleware</em></a> 。</p>
</div>
<div class="section" id="scrapyscrapy">
<h4>如何才能看到Scrapy发出及接收到的Scrapy呢？<a class="headerlink" href="#scrapyscrapy" title="永久链接至标题">¶</a></h4>
<p>启用 <a class="reference internal" href="index.html#std:setting-COOKIES_DEBUG"><tt class="xref std std-setting docutils literal"><span class="pre">COOKIES_DEBUG</span></tt></a> 选项。</p>
</div>
<div class="section" id="id10">
<h4>要怎么停止爬虫呢?<a class="headerlink" href="#id10" title="永久链接至标题">¶</a></h4>
<p>在回调函数中raise <a class="reference internal" href="index.html#scrapy.exceptions.CloseSpider" title="scrapy.exceptions.CloseSpider"><tt class="xref py py-exc docutils literal"><span class="pre">CloseSpider</span></tt></a> 异常。
更多详情请参见: <a class="reference internal" href="index.html#scrapy.exceptions.CloseSpider" title="scrapy.exceptions.CloseSpider"><tt class="xref py py-exc docutils literal"><span class="pre">CloseSpider</span></tt></a> 。</p>
</div>
<div class="section" id="scrapy-bot-ban">
<h4>如何避免我的Scrapy机器人(bot)被禁止(ban)呢？<a class="headerlink" href="#scrapy-bot-ban" title="永久链接至标题">¶</a></h4>
<p>参考 <a class="reference internal" href="index.html#bans"><em>避免被禁止(ban)</em></a>.</p>
</div>
<div class="section" id="spider-arguments-settings-spider">
<h4>我应该使用spider参数(arguments)还是设置(settings)来配置spider呢？<a class="headerlink" href="#spider-arguments-settings-spider" title="永久链接至标题">¶</a></h4>
<p><a class="reference internal" href="index.html#spiderargs"><em>spider参数</em></a> 及 <a class="reference internal" href="index.html#topics-settings"><em>设置(settings)</em></a> 都可以用来配置您的spider。
没有什么强制的规则来限定要使用哪个，但设置(settings)更适合那些一旦设置就不怎么会修改的参数，
而spider参数则意味着修改更为频繁，在每次spider运行都有修改，甚至是spider运行所必须的元素
(例如，设置spider的起始url)。</p>
<p>这里以例子来说明这个问题。假设您有一个spider需要登录某个网站来
爬取数据，并且仅仅想爬取特定网站的特定部分(每次都不一定相同)。
在这个情况下，认证的信息将写在设置中，而爬取的特定部分的url将是spider参数。</p>
</div>
<div class="section" id="xmlxpathitem">
<h4>我爬取了一个XML文档但是XPath选择器不返回任何的item<a class="headerlink" href="#xmlxpathitem" title="永久链接至标题">¶</a></h4>
<p>也许您需要移除命名空间(namespace)。参见 <a class="reference internal" href="index.html#removing-namespaces"><em>移除命名空间</em></a>.</p>
</div>
<div class="section" id="name-crawler">
<h4>我得到错误: &#8220;不能导入name crawler“<a class="headerlink" href="#name-crawler" title="永久链接至标题">¶</a></h4>
<p>这是由于Scrapy修改，去掉了单例模式(singletons)所引起的。
这个错误一般是由从 <tt class="docutils literal"><span class="pre">scrapy.project</span></tt> 导入 <tt class="docutils literal"><span class="pre">crawler</span></tt> 的模块引起的(扩展，中间件，pipeline或spider)。
例如:</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.project import crawler

class SomeExtension(object):
    def __init__(self):
        self.crawler = crawler
        # ...
</pre></div>
</div>
<p>这种访问crawler对象的方式已经被舍弃了，新的代码应该使用
<tt class="docutils literal"><span class="pre">from_crawler</span></tt> 类方法来移植，例如:</p>
<div class="highlight-none"><div class="highlight"><pre>class SomeExtension(object):

    @classmethod
    def from_crawler(cls, crawler):
        o = cls()
        o.crawler = crawler
        return o
</pre></div>
</div>
<p>Scrapy终端工具(command line tool)针对旧的导入机制提供了一些支持(给出了废弃警告)，
但如果您以不同方式使用Scrapy(例如，作为类库)，该机制可能会失效。</p>
</div>
</div>
<span id="document-topics/debug"></span><div class="section" id="debugging-spiders">
<span id="topics-debug"></span><h3>调试(Debugging)Spiders<a class="headerlink" href="#debugging-spiders" title="永久链接至标题">¶</a></h3>
<p>本篇介绍了调试spider的常用技术。
考虑下面的spider:</p>
<div class="highlight-none"><div class="highlight"><pre>import scrapy
from myproject.items import MyItem

class MySpider(scrapy.Spider):
    name = &#39;myspider&#39;
    start_urls = (
        &#39;http://example.com/page1&#39;,
        &#39;http://example.com/page2&#39;,
        )

    def parse(self, response):
        # collect `item_urls`
        for item_url in item_urls:
            yield scrapy.Request(item_url, self.parse_item)

    def parse_item(self, response):
        item = MyItem()
        # populate `item` fields
        # and extract item_details_url
        yield scrapy.Request(item_details_url, self.parse_details, meta={&#39;item&#39;: item})

    def parse_details(self, response):
        item = response.meta[&#39;item&#39;]
        # populate more `item` fields
        return item
</pre></div>
</div>
<p>简单地说，该spider分析了两个包含item的页面(start_urls)。Item有详情页面，
所以我们使用 <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 的 <tt class="docutils literal"><span class="pre">meta</span></tt> 功能来传递已经部分获取的item。</p>
<div class="section" id="parse">
<h4>Parse命令<a class="headerlink" href="#parse" title="永久链接至标题">¶</a></h4>
<p>检查spier输出的最基本方法是使用
<a class="reference internal" href="index.html#std:command-parse"><tt class="xref std std-command docutils literal"><span class="pre">parse</span></tt></a> 命令。这能让你在函数层(method level)上检查spider
各个部分的效果。其十分灵活并且易用，不过不能在代码中调试。</p>
<p>查看特定url爬取到的item:</p>
<div class="highlight-none"><div class="highlight"><pre>$ scrapy parse --spider=myspider -c parse_item -d 2 &lt;item_url&gt;
[ ... scrapy log lines crawling example.com spider ... ]

&gt;&gt;&gt; STATUS DEPTH LEVEL 2 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[{&#39;url&#39;: &lt;item_url&gt;}]

# Requests  -----------------------------------------------------------------
[]
</pre></div>
</div>
<p>使用 <tt class="docutils literal"><span class="pre">--verbose</span></tt> 或 <tt class="docutils literal"><span class="pre">-v</span></tt> 选项，查看各个层次的状态:</p>
<div class="highlight-none"><div class="highlight"><pre>$ scrapy parse --spider=myspider -c parse_item -d 2 -v &lt;item_url&gt;
[ ... scrapy log lines crawling example.com spider ... ]

&gt;&gt;&gt; DEPTH LEVEL: 1 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[]

# Requests  -----------------------------------------------------------------
[&lt;GET item_details_url&gt;]


&gt;&gt;&gt; DEPTH LEVEL: 2 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[{&#39;url&#39;: &lt;item_url&gt;}]

# Requests  -----------------------------------------------------------------
[]
</pre></div>
</div>
<p>检查从单个start_url爬取到的item也是很简单的:</p>
<div class="highlight-none"><div class="highlight"><pre>$ scrapy parse --spider=myspider -d 3 &#39;http://example.com/page1&#39;
</pre></div>
</div>
</div>
<div class="section" id="scrapy-shell">
<h4>Scrapy终端(Shell)<a class="headerlink" href="#scrapy-shell" title="永久链接至标题">¶</a></h4>
<p>尽管 <a class="reference internal" href="index.html#std:command-parse"><tt class="xref std std-command docutils literal"><span class="pre">parse</span></tt></a> 命令对检查spider的效果十分有用，但除了显示收到的response及输出外，
其对检查回调函数内部的过程并没有提供什么便利。
如何调试 <tt class="docutils literal"><span class="pre">parse_detail</span></tt> 没有收到item的情况呢？</p>
<p>幸运的是，救世主 <a class="reference internal" href="index.html#std:command-shell"><tt class="xref std std-command docutils literal"><span class="pre">shell</span></tt></a> 出现了(参考
<a class="reference internal" href="index.html#topics-shell-inspect-response"><em>在spider中启动shell来查看response</em></a>):</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.shell import inspect_response

def parse_details(self, response):
    item = response.meta.get(&#39;item&#39;, None)
    if item:
        # populate more `item` fields
        return item
    else:
        inspect_response(response, self)
</pre></div>
</div>
<p>参考 <a class="reference internal" href="index.html#topics-shell-inspect-response"><em>在spider中启动shell来查看response</em></a> 。</p>
</div>
<div class="section" id="id1">
<h4>在浏览器中打开<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h4>
<p>有时候您想查看某个response在浏览器中显示的效果，这是可以使用
<tt class="docutils literal"><span class="pre">open_in_browser</span></tt> 功能。下面是使用的例子:</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.utils.response import open_in_browser

def parse_details(self, response):
    if &quot;item name&quot; not in response.body:
        open_in_browser(response)
</pre></div>
</div>
<p><tt class="docutils literal"><span class="pre">open_in_browser</span></tt> 将会使用Scrapy获取到的response来打开浏览器，并且调整
<a class="reference external" href="http://www.w3schools.com/tags/tag_base.asp">base tag</a> 使得图片及样式(style)能正常显示。</p>
</div>
<div class="section" id="logging">
<h4>Logging<a class="headerlink" href="#logging" title="永久链接至标题">¶</a></h4>
<p>记录(logging)是另一个获取到spider运行信息的方法。虽然不是那么方便，
但好处是log的内容在以后的运行中也可以看到:</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy import log

def parse_details(self, response):
    item = response.meta.get(&#39;item&#39;, None)
    if item:
        # populate more `item` fields
        return item
    else:
        self.log(&#39;No item received for %s&#39; % response.url,
            level=log.WARNING)
</pre></div>
</div>
<p>更多内容请参见 <a class="reference internal" href="index.html#topics-logging"><em>Logging</em></a> 部分。</p>
</div>
</div>
<span id="document-topics/contracts"></span><div class="section" id="spiders-contracts">
<span id="topics-contracts"></span><h3>Spiders Contracts<a class="headerlink" href="#spiders-contracts" title="永久链接至标题">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">0.15 新版功能.</span></p>
</div>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">这是一个新引入(Scrapy 0.15)的特性，在后续的功能/API更新中可能有所改变，查看
<a class="reference internal" href="index.html#news"><em>release notes</em></a> 来了解更新。</p>
</div>
<p>测试spider是一件挺烦人的事情，尤其是只能编写单元测试(unit test)没有其他办法时，就更恼人了。
Scrapy通过合同(contract)的方式来提供了测试spider的集成方法。</p>
<p>您可以硬编码(hardcode)一个样例(sample)url，
设置多个条件来测试回调函数处理repsponse的结果，来测试spider的回调函数。
每个contract包含在文档字符串(docstring)里，以 <tt class="docutils literal"><span class="pre">&#64;</span></tt> 开头。
查看下面的例子:</p>
<div class="highlight-none"><div class="highlight"><pre>def parse(self, response):
    &quot;&quot;&quot; This function parses a sample response. Some contracts are mingled
    with this docstring.

    @url http://www.amazon.com/s?field-keywords=selfish+gene
    @returns items 1 16
    @returns requests 0 0
    @scrapes Title Author Year Price
    &quot;&quot;&quot;
</pre></div>
</div>
<p>该回调函数使用了三个内置的contract来测试:</p>
<span class="target" id="module-scrapy.contracts.default"></span><dl class="class">
<dt id="scrapy.contracts.default.UrlContract">
<em class="property">class </em><tt class="descclassname">scrapy.contracts.default.</tt><tt class="descname">UrlContract</tt><a class="headerlink" href="#scrapy.contracts.default.UrlContract" title="永久链接至目标">¶</a></dt>
<dd><p>该constract(<tt class="docutils literal"><span class="pre">&#64;url</span></tt>)设置了用于检查spider的其他constract状态的样例url。
该contract是必须的，所有缺失该contract的回调函数在测试时将会被忽略:</p>
<div class="highlight-none"><div class="highlight"><pre>@url url
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="scrapy.contracts.default.ReturnsContract">
<em class="property">class </em><tt class="descclassname">scrapy.contracts.default.</tt><tt class="descname">ReturnsContract</tt><a class="headerlink" href="#scrapy.contracts.default.ReturnsContract" title="永久链接至目标">¶</a></dt>
<dd><p>该contract(<tt class="docutils literal"><span class="pre">&#64;returns</span></tt>)设置spider返回的items和requests的上界和下界。
上界是可选的:</p>
<div class="highlight-none"><div class="highlight"><pre>@returns item(s)|request(s) [min [max]]
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="scrapy.contracts.default.ScrapesContract">
<em class="property">class </em><tt class="descclassname">scrapy.contracts.default.</tt><tt class="descname">ScrapesContract</tt><a class="headerlink" href="#scrapy.contracts.default.ScrapesContract" title="永久链接至目标">¶</a></dt>
<dd><p>该contract(<tt class="docutils literal"><span class="pre">&#64;scrapes</span></tt>)检查回调函数返回的所有item是否有特定的fields:</p>
<div class="highlight-none"><div class="highlight"><pre>@scrapes field_1 field_2 ...
</pre></div>
</div>
</dd></dl>

<p>使用 <a class="reference internal" href="index.html#std:command-check"><tt class="xref std std-command docutils literal"><span class="pre">check</span></tt></a> 命令来运行contract检查。</p>
<div class="section" id="contracts">
<h4>自定义Contracts<a class="headerlink" href="#contracts" title="永久链接至标题">¶</a></h4>
<p>如果您想要比内置scrapy contract更为强大的功能，可以在您的项目里创建并设置您自己的
contract，并使用 <a class="reference internal" href="index.html#std:setting-SPIDER_CONTRACTS"><tt class="xref std std-setting docutils literal"><span class="pre">SPIDER_CONTRACTS</span></tt></a> 设置来加载:</p>
<div class="highlight-none"><div class="highlight"><pre>SPIDER_CONTRACTS = {
    &#39;myproject.contracts.ResponseCheck&#39;: 10,
    &#39;myproject.contracts.ItemValidate&#39;: 10,
}
</pre></div>
</div>
<p>每个contract必须继承 <a class="reference internal" href="index.html#scrapy.contracts.Contract" title="scrapy.contracts.Contract"><tt class="xref py py-class docutils literal"><span class="pre">scrapy.contracts.Contract</span></tt></a> 并覆盖下列三个方法:</p>
<span class="target" id="module-scrapy.contracts"></span><dl class="class">
<dt id="scrapy.contracts.Contract">
<em class="property">class </em><tt class="descclassname">scrapy.contracts.</tt><tt class="descname">Contract</tt><big>(</big><em>method</em>, <em>*args</em><big>)</big><a class="headerlink" href="#scrapy.contracts.Contract" title="永久链接至目标">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>method</strong> (<em>function</em>) &#8211; contract所关联的回调函数</li>
<li><strong>args</strong> (<em>list</em>) &#8211; 传入docstring的(以空格区分的)argument列表(list)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="scrapy.contracts.Contract.adjust_request_args">
<tt class="descname">adjust_request_args</tt><big>(</big><em>args</em><big>)</big><a class="headerlink" href="#scrapy.contracts.Contract.adjust_request_args" title="永久链接至目标">¶</a></dt>
<dd><p>接收一个 <tt class="docutils literal"><span class="pre">字典(dict)</span></tt> 作为参数。该参数包含了所有 <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象
参数的默认值。该方法必须返回相同或修改过的字典。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contracts.Contract.pre_process">
<tt class="descname">pre_process</tt><big>(</big><em>response</em><big>)</big><a class="headerlink" href="#scrapy.contracts.Contract.pre_process" title="永久链接至目标">¶</a></dt>
<dd><p>该函数在sample request接收到response后，传送给回调函数前被调用，运行测试。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contracts.Contract.post_process">
<tt class="descname">post_process</tt><big>(</big><em>output</em><big>)</big><a class="headerlink" href="#scrapy.contracts.Contract.post_process" title="永久链接至目标">¶</a></dt>
<dd><p>该函数处理回调函数的输出。迭代器(Iterators)在传输给该函数前会被列表化(listified)。</p>
</dd></dl>

</dd></dl>

<p>该样例contract在response接收时检查了是否有自定义header。
在失败时Raise <tt class="xref py py-class docutils literal"><span class="pre">scrapy.exceptions.ContractFaild</span></tt> 来展现错误:</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.contracts import Contract
from scrapy.exceptions import ContractFail

class HasHeaderContract(Contract):
    &quot;&quot;&quot; Demo contract which checks the presence of a custom header
        @has_header X-CustomHeader
    &quot;&quot;&quot;

    name = &#39;has_header&#39;

    def pre_process(self, response):
        for header in self.args:
            if header not in response.headers:
                raise ContractFail(&#39;X-CustomHeader not present&#39;)
</pre></div>
</div>
</div>
</div>
<span id="document-topics/practices"></span><div class="section" id="common-practices">
<span id="topics-practices"></span><h3>实践经验(Common Practices)<a class="headerlink" href="#common-practices" title="永久链接至标题">¶</a></h3>
<p>本章节记录了使用Scrapy的一些实践经验(common practices)。
这包含了很多使用不会包含在其他特定章节的的内容。</p>
<div class="section" id="scrapy">
<span id="run-from-script"></span><h4>在脚本中运行Scrapy<a class="headerlink" href="#scrapy" title="永久链接至标题">¶</a></h4>
<p>除了常用的 <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">crawl</span></tt> 来启动Scrapy，您也可以使用 <a class="reference internal" href="index.html#topics-api"><em>API</em></a> 在脚本中启动Scrapy。</p>
<p>需要注意的是，Scrapy是在Twisted异步网络库上构建的，
因此其必须在Twisted reactor里运行。</p>
<p>另外，在spider运行结束后，您必须自行关闭Twisted reactor。
这可以通过设置 <tt class="docutils literal"><span class="pre">signals.spider_closed</span></tt> 信号的处理器(handler)来实现。</p>
<p>下面给出了如何实现的例子，使用 <a class="reference external" href="https://github.com/scrapinghub/testspiders">testspiders</a> 项目作为例子。</p>
<div class="highlight-none"><div class="highlight"><pre>from twisted.internet import reactor
from scrapy.crawler import Crawler
from scrapy import log, signals
from testspiders.spiders.followall import FollowAllSpider
from scrapy.utils.project import get_project_settings

spider = FollowAllSpider(domain=&#39;scrapinghub.com&#39;)
settings = get_project_settings()
crawler = Crawler(settings)
crawler.signals.connect(reactor.stop, signal=signals.spider_closed)
crawler.configure()
crawler.crawl(spider)
crawler.start()
log.start()
reactor.run() # the script will block here until the spider_closed signal was sent
</pre></div>
</div>
<div class="admonition seealso">
<p class="first admonition-title">参见</p>
<p class="last"><a class="reference external" href="http://twistedmatrix.com/documents/current/core/howto/reactor-basics.html">Twisted Reactor Overview</a>.</p>
</div>
</div>
<div class="section" id="spider">
<h4>同一进程运行多个spider<a class="headerlink" href="#spider" title="永久链接至标题">¶</a></h4>
<p>默认情况下，当您执行 <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">crawl</span></tt> 时，Scrapy每个进程运行一个spider。
当然，Scrapy通过
<a class="reference internal" href="index.html#topics-api"><em>内部(internal)API</em></a>
也支持单进程多个spider。</p>
<p>下面以 <a class="reference external" href="https://github.com/scrapinghub/testspiders">testspiders</a> 作为例子:</p>
<div class="highlight-none"><div class="highlight"><pre>from twisted.internet import reactor
from scrapy.crawler import Crawler
from scrapy import log
from testspiders.spiders.followall import FollowAllSpider
from scrapy.utils.project import get_project_settings

def setup_crawler(domain):
    spider = FollowAllSpider(domain=domain)
    settings = get_project_settings()
    crawler = Crawler(settings)
    crawler.configure()
    crawler.crawl(spider)
    crawler.start()

for domain in [&#39;scrapinghub.com&#39;, &#39;insophia.com&#39;]:
    setup_crawler(domain)
log.start()
reactor.run()
</pre></div>
</div>
<div class="admonition seealso">
<p class="first admonition-title">参见</p>
<p class="last"><a class="reference internal" href="index.html#run-from-script"><em>在脚本中运行Scrapy</em></a>.</p>
</div>
</div>
<div class="section" id="distributed-crawls">
<span id="id1"></span><h4>分布式爬虫(Distributed crawls)<a class="headerlink" href="#distributed-crawls" title="永久链接至标题">¶</a></h4>
<p>Scrapy并没有提供内置的机制支持分布式(多服务器)爬取。不过还是有办法进行分布式爬取，
取决于您要怎么分布了。</p>
<p>如果您有很多spider，那分布负载最简单的办法就是启动多个Scrapyd，并分配到不同机器上。</p>
<p>如果想要在多个机器上运行一个单独的spider，那您可以将要爬取的url进行分块，并发送给spider。
例如:</p>
<p>首先，准备要爬取的url列表，并分配到不同的文件url里:</p>
<div class="highlight-none"><div class="highlight"><pre>http://somedomain.com/urls-to-crawl/spider1/part1.list
http://somedomain.com/urls-to-crawl/spider1/part2.list
http://somedomain.com/urls-to-crawl/spider1/part3.list
</pre></div>
</div>
<p>接着在3个不同的Scrapd服务器中启动spider。spider会接收一个(spider)参数 <tt class="docutils literal"><span class="pre">part</span></tt> ，
该参数表示要爬取的分块:</p>
<div class="highlight-none"><div class="highlight"><pre>curl http://scrapy1.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=1
curl http://scrapy2.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=2
curl http://scrapy3.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=3
</pre></div>
</div>
</div>
<div class="section" id="ban">
<span id="bans"></span><h4>避免被禁止(ban)<a class="headerlink" href="#ban" title="永久链接至标题">¶</a></h4>
<p>有些网站实现了特定的机制，以一定规则来避免被爬虫爬取。
与这些规则打交道并不容易，需要技巧，有时候也需要些特别的基础。
如果有疑问请考虑联系 <a class="reference external" href="http://scrapy.org/support/">商业支持</a> 。</p>
<p>下面是些处理这些站点的建议(tips):</p>
<ul class="simple">
<li>使用user agent池，轮流选择之一来作为user agent。池中包含常见的浏览器的user agent(google一下一大堆)</li>
<li>禁止cookies(参考 <a class="reference internal" href="index.html#std:setting-COOKIES_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">COOKIES_ENABLED</span></tt></a>)，有些站点会使用cookies来发现爬虫的轨迹。</li>
<li>设置下载延迟(2或更高)。参考 <a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></tt></a> 设置。</li>
<li>如果可行，使用 <a class="reference external" href="http://www.googleguide.com/cached_pages.html">Google cache</a> 来爬取数据，而不是直接访问站点。</li>
<li>使用IP池。例如免费的 <a class="reference external" href="https://www.torproject.org/">Tor项目</a> 或付费服务(<a class="reference external" href="http://proxymesh.com/">ProxyMesh</a>)。</li>
<li>使用高度分布式的下载器(downloader)来绕过禁止(ban)，您就只需要专注分析处理页面。这样的例子有:
<a class="reference external" href="http://crawlera.com">Crawlera</a></li>
</ul>
<p>如果您仍然无法避免被ban，考虑联系
<a class="reference external" href="http://scrapy.org/support/">商业支持</a>.</p>
</div>
<div class="section" id="item">
<span id="dynamic-item-classes"></span><h4>动态创建Item类<a class="headerlink" href="#item" title="永久链接至标题">¶</a></h4>
<p>对于有些应用，item的结构由用户输入或者其他变化的情况所控制。您可以动态创建class。</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.item import DictItem, Field

def create_item_class(class_name, field_list):
    field_dict = {}
    for field_name in field_list:
        field_dict[field_name] = Field()

    return type(class_name, (DictItem,), field_dict)
</pre></div>
</div>
</div>
</div>
<span id="document-topics/broad-crawls"></span><div class="section" id="broad-crawls">
<span id="topics-broad-crawls"></span><h3>通用爬虫(Broad Crawls)<a class="headerlink" href="#broad-crawls" title="永久链接至标题">¶</a></h3>
<p>Scrapy默认对特定爬取进行优化。这些站点一般被一个单独的Scrapy spider进行处理，
不过这并不是必须或要求的(例如，也有通用的爬虫能处理任何给定的站点)。</p>
<p>除了这种爬取完某个站点或没有更多请求就停止的&#8221;专注的爬虫&#8221;，还有一种通用的爬取类型，其能爬取大量(甚至是无限)的网站，
仅仅受限于时间或其他的限制。
这种爬虫叫做&#8221;通用爬虫(broad crawls)&#8221;，一般用于搜索引擎。</p>
<p>通用爬虫一般有以下通用特性:</p>
<ul class="simple">
<li>其爬取大量(一般来说是无限)的网站而不是特定的一些网站。</li>
<li>其不会将整个网站都爬取完毕，因为这十分不实际(或者说是不可能)完成的。相反，其会限制爬取的时间及数量。</li>
<li>其在逻辑上十分简单(相较于具有很多提取规则的复杂的spider)，数据会在另外的阶段进行后处理(post-processed)</li>
<li>其并行爬取大量网站以避免被某个网站的限制所限制爬取的速度(为表示尊重，每个站点爬取速度很慢但同时爬取很多站点)。</li>
</ul>
<p>正如上面所述，Scrapy默认设置是对特定爬虫做了优化，而不是通用爬虫。不过，
鉴于其使用了异步架构，Scrapy对通用爬虫也十分适用。
本篇文章总结了一些将Scrapy作为通用爬虫所需要的技巧，
以及相应针对通用爬虫的Scrapy设定的一些建议。</p>
<div class="section" id="id1">
<h4>增加并发<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h4>
<p>并发是指同时处理的request的数量。其有全局限制和局部(每个网站)的限制。</p>
<p>Scrapy默认的全局并发限制对同时爬取大量网站的情况并不适用，因此您需要增加这个值。
增加多少取决于您的爬虫能占用多少CPU。
一般开始可以设置为 <tt class="docutils literal"><span class="pre">100</span></tt> 。不过最好的方式是做一些测试，获得Scrapy进程占取CPU与并发数的关系。
为了优化性能，您应该选择一个能使CPU占用率在80%-90%的并发数。</p>
<p>增加全局并发数:</p>
<div class="highlight-none"><div class="highlight"><pre>CONCURRENT_REQUESTS = 100
</pre></div>
</div>
</div>
<div class="section" id="log">
<h4>降低log级别<a class="headerlink" href="#log" title="永久链接至标题">¶</a></h4>
<p>当进行通用爬取时，一般您所注意的仅仅是爬取的速率以及遇到的错误。
Scrapy使用 <tt class="docutils literal"><span class="pre">INFO</span></tt> log级别来报告这些信息。为了减少CPU使用率(及记录log存储的要求),
在生产环境中进行通用爬取时您不应该使用 <tt class="docutils literal"><span class="pre">DEBUG</span></tt> log级别。
不过在开发的时候使用 <tt class="docutils literal"><span class="pre">DEBUG</span></tt> 应该还能接受。</p>
<p>设置Log级别:</p>
<div class="highlight-none"><div class="highlight"><pre>LOG_LEVEL = &#39;INFO&#39;
</pre></div>
</div>
</div>
<div class="section" id="cookies">
<h4>禁止cookies<a class="headerlink" href="#cookies" title="永久链接至标题">¶</a></h4>
<p>除非您 <em>真的</em> 需要，否则请禁止cookies。在进行通用爬取时cookies并不需要，
(搜索引擎则忽略cookies)。禁止cookies能减少CPU使用率及Scrapy爬虫在内存中记录的踪迹，提高性能。</p>
<p>禁止cookies:</p>
<div class="highlight-none"><div class="highlight"><pre>COOKIES_ENABLED = False
</pre></div>
</div>
</div>
<div class="section" id="id2">
<h4>禁止重试<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h4>
<p>对失败的HTTP请求进行重试会减慢爬取的效率，尤其是当站点响应很慢(甚至失败)时，
访问这样的站点会造成超时并重试多次。这是不必要的，同时也占用了爬虫爬取其他站点的能力。</p>
<p>禁止重试:</p>
<div class="highlight-none"><div class="highlight"><pre>RETRY_ENABLED = False
</pre></div>
</div>
</div>
<div class="section" id="id3">
<h4>减小下载超时<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h4>
<p>如果您对一个非常慢的连接进行爬取(一般对通用爬虫来说并不重要)，
减小下载超时能让卡住的连接能被快速的放弃并解放处理其他站点的能力。</p>
<p>减小下载超时:</p>
<div class="highlight-none"><div class="highlight"><pre>DOWNLOAD_TIMEOUT = 15
</pre></div>
</div>
</div>
<div class="section" id="id4">
<h4>禁止重定向<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h4>
<p>除非您对跟进重定向感兴趣，否则请考虑关闭重定向。
当进行通用爬取时，一般的做法是保存重定向的地址，并在之后的爬取进行解析。
这保证了每批爬取的request数目在一定的数量，
否则重定向循环可能会导致爬虫在某个站点耗费过多资源。</p>
<p>关闭重定向:</p>
<div class="highlight-none"><div class="highlight"><pre>REDIRECT_ENABLED = False
</pre></div>
</div>
</div>
<div class="section" id="ajax-crawlable-pages">
<h4>启用 &#8220;Ajax Crawlable Pages&#8221; 爬取<a class="headerlink" href="#ajax-crawlable-pages" title="永久链接至标题">¶</a></h4>
<p>有些站点(基于2013年的经验数据，之多有1%)声明其为 <a class="reference external" href="https://developers.google.com/webmasters/ajax-crawling/docs/getting-started">ajax crawlable</a> 。
这意味着该网站提供了原本只有ajax获取到的数据的纯HTML版本。
网站通过两种方法声明:</p>
<ol class="arabic simple">
<li>在url中使用 <tt class="docutils literal"><span class="pre">#!</span></tt> - 这是默认的方式;</li>
<li>使用特殊的meta标签 - 这在&#8221;main&#8221;, &#8220;index&#8221; 页面中使用。</li>
</ol>
<p>Scrapy自动解决(1)；解决(2)您需要启用
<a class="reference internal" href="index.html#ajaxcrawl-middleware"><em>AjaxCrawlMiddleware</em></a>:</p>
<div class="highlight-none"><div class="highlight"><pre>AJAXCRAWL_ENABLED = True
</pre></div>
</div>
<p>通用爬取经常抓取大量的 &#8220;index&#8221; 页面；
AjaxCrawlMiddleware能帮助您正确地爬取。
由于有些性能问题，且对于特定爬虫没有什么意义，该中间默认关闭。</p>
</div>
</div>
<span id="document-topics/firefox"></span><div class="section" id="firefox">
<span id="topics-firefox"></span><h3>借助Firefox来爬取<a class="headerlink" href="#firefox" title="永久链接至标题">¶</a></h3>
<p>这里介绍一些使用Firefox进行爬取的点子及建议，以及一些帮助爬取的Firefox实用插件。</p>
<div class="section" id="dom">
<span id="topics-firefox-livedom"></span><h4>在浏览器中检查DOM的注意事项<a class="headerlink" href="#dom" title="永久链接至标题">¶</a></h4>
<p>Firefox插件操作的是活动的浏览器DOM(live browser DOM)，这意味着当您检查网页源码的时候，
其已经不是原始的HTML，而是经过浏览器清理并执行一些Javascript代码后的结果。
Firefox是个典型的例子，其会在table中添加 <tt class="docutils literal"><span class="pre">&lt;tbody&gt;</span></tt> 元素。
而Scrapy相反，其并不修改原始的HTML，因此如果在XPath表达式中使用
<tt class="docutils literal"><span class="pre">&lt;tbody&gt;</span></tt> ，您将获取不到任何数据。</p>
<p>所以，当XPath配合Firefox使用时您需要记住以下几点:</p>
<ul class="simple">
<li>当检查DOM来查找Scrapy使用的XPath时，禁用Firefox的Javascrpit。</li>
<li>永远不要用完整的XPath路径。使用相对及基于属性(例如 <tt class="docutils literal"><span class="pre">id</span></tt> ， <tt class="docutils literal"><span class="pre">class</span></tt> ， <tt class="docutils literal"><span class="pre">width</span></tt> 等)的路径
或者具有区别性的特性例如 <tt class="docutils literal"><span class="pre">contains(&#64;href,</span> <span class="pre">'image')</span></tt> 。</li>
<li>永远不要在XPath表达式中加入 <tt class="docutils literal"><span class="pre">&lt;tbody&gt;</span></tt> 元素，除非您知道您在做什么</li>
</ul>
</div>
<div class="section" id="topics-firefox-addons">
<span id="id1"></span><h4>对爬取有帮助的实用Firefox插件<a class="headerlink" href="#topics-firefox-addons" title="永久链接至标题">¶</a></h4>
<div class="section" id="firebug">
<h5>Firebug<a class="headerlink" href="#firebug" title="永久链接至标题">¶</a></h5>
<p><a class="reference external" href="http://getfirebug.com">Firebug</a> 是一个在web开发者间很著名的工具，其对抓取也十分有用。
尤其是 <a class="reference external" href="http://www.youtube.com/watch?v=-pT_pDe54aA">检查元素(Inspect Element)</a> 特性对构建抓取数据的XPath十分方便。
当移动鼠标在页面元素时，您能查看相应元素的HTML源码。</p>
<p>查看 <a class="reference internal" href="index.html#topics-firebug"><em>使用Firebug进行爬取</em></a> ，了解如何配合Scrapy使用Firebug的详细教程。</p>
</div>
<div class="section" id="xpather">
<h5>XPather<a class="headerlink" href="#xpather" title="永久链接至标题">¶</a></h5>
<p><a class="reference external" href="https://addons.mozilla.org/firefox/addon/1192">XPather</a> 能让你在页面上直接测试XPath表达式。</p>
</div>
<div class="section" id="xpath-checker">
<h5>XPath Checker<a class="headerlink" href="#xpath-checker" title="永久链接至标题">¶</a></h5>
<p><a class="reference external" href="https://addons.mozilla.org/firefox/addon/1095">XPath Checker</a> 是另一个用于测试XPath表达式的Firefox插件。</p>
</div>
<div class="section" id="tamper-data">
<h5>Tamper Data<a class="headerlink" href="#tamper-data" title="永久链接至标题">¶</a></h5>
<p><a class="reference external" href="http://addons.mozilla.org/firefox/addon/966">Tamper Data</a> 是一个允许您查看及修改Firefox发送的header的插件。Firebug能查看HTTP header，但无法修改。</p>
</div>
<div class="section" id="firecookie">
<h5>Firecookie<a class="headerlink" href="#firecookie" title="永久链接至标题">¶</a></h5>
<p><a class="reference external" href="https://addons.mozilla.org/firefox/addon/6683">Firecookie</a> 使得查看及管理cookie变得简单。您可以使用这个插件来创建新的cookie，
删除存在的cookie，查看当前站点的cookie，管理cookie的权限及其他功能。</p>
</div>
</div>
</div>
<span id="document-topics/firebug"></span><div class="section" id="firebug">
<span id="topics-firebug"></span><h3>使用Firebug进行爬取<a class="headerlink" href="#firebug" title="永久链接至标题">¶</a></h3>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">本教程所使用的样例站Google Directory已经 <a class="reference external" href="http://searchenginewatch.com/article/2096661/Google-Directory-Has-Been-Shut-Down">被Google关闭</a> 了。不过教程中的概念任然适用。
如果您打算使用一个新的网站来更新本教程，您的贡献是再欢迎不过了。
详细信息请参考 <a class="reference internal" href="index.html#topics-contributing"><em>Contributing to Scrapy</em></a> 。</p>
</div>
<div class="section" id="id1">
<h4>介绍<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h4>
<p>本文档介绍了如何适用 <a class="reference external" href="http://getfirebug.com">Firebug</a> (一个Firefox的插件)来使得爬取更为简单，有趣。
更多有意思的Firefox插件请参考 <a class="reference internal" href="index.html#topics-firefox-addons"><em>对爬取有帮助的实用Firefox插件</em></a> 。
使用Firefox插件检查页面需要有些注意事项: <a class="reference internal" href="index.html#topics-firefox-livedom"><em>在浏览器中检查DOM的注意事项</em></a> 。</p>
<p>在本样例中将展现如何使用 <a class="reference external" href="http://getfirebug.com">Firebug</a> 从 <a class="reference external" href="http://directory.google.com/">Google Directory</a> 来爬取数据。
<a class="reference external" href="http://directory.google.com/">Google Directory</a> 包含了 <a class="reference internal" href="index.html#intro-tutorial"><em>入门教程</em></a> 里所使用的
<a class="reference external" href="http://www.dmoz.org">Open Directory Project</a> 中一样的数据，不过有着不同的结构。</p>
<p>Firebug提供了非常实用的 <a class="reference external" href="http://www.youtube.com/watch?v=-pT_pDe54aA">检查元素</a> 功能。该功能允许您将鼠标悬浮在不同的页面元素上，
显示相应元素的HTML代码。否则，您只能十分痛苦的在HTML的body中手动搜索标签。</p>
<p>在下列截图中，您将看到 <a class="reference external" href="http://www.youtube.com/watch?v=-pT_pDe54aA">检查元素</a> 的执行效果。</p>
<a class="reference internal image-reference" href="_images/firebug1.png"><img alt="Inspecting elements with Firebug" src="_images/firebug1.png" style="width: 913px; height: 600px;" /></a>
<p>首先我们能看到目录根据种类进行分类的同时，还划分了子类。</p>
<p>不过，看起来子类还有更多的子类，而不仅仅是页面显示的这些，所以我们接着查找:</p>
<a class="reference internal image-reference" href="_images/firebug2.png"><img alt="Inspecting elements with Firebug" src="_images/firebug2.png" style="width: 819px; height: 629px;" /></a>
<p>正如路径的概念那样，子类包含了其他子类的链接，同时也链接到实际的网站中。</p>
</div>
<div class="section" id="follow">
<h4>获取到跟进(follow)的链接<a class="headerlink" href="#follow" title="永久链接至标题">¶</a></h4>
<p>查看路径的URL，我们可以看到URL的通用模式(pattern):</p>
<blockquote>
<div><a class="reference external" href="http://directory.google.com/Category/Subcategory/Another_Subcategory">http://directory.google.com/Category/Subcategory/Another_Subcategory</a></div></blockquote>
<p>了解到这个消息，我们可以构建一个跟进的链接的正则表达式:</p>
<div class="highlight-none"><div class="highlight"><pre>directory\.google\.com/[A-Z][a-zA-Z_/]+$
</pre></div>
</div>
<p>因此，根据这个表达式，我们创建第一个爬取规则:</p>
<div class="highlight-none"><div class="highlight"><pre>Rule(LinkExtractor(allow=&#39;directory.google.com/[A-Z][a-zA-Z_/]+$&#39;, ),
    &#39;parse_category&#39;,
    follow=True,
),
</pre></div>
</div>
<p><a class="reference internal" href="index.html#scrapy.contrib.spiders.Rule" title="scrapy.contrib.spiders.Rule"><tt class="xref py py-class docutils literal"><span class="pre">Rule</span></tt></a> 对象指导基于
<a class="reference internal" href="index.html#scrapy.contrib.spiders.CrawlSpider" title="scrapy.contrib.spiders.CrawlSpider"><tt class="xref py py-class docutils literal"><span class="pre">CrawlSpider</span></tt></a> 的spider如何跟进目录链接。
<tt class="docutils literal"><span class="pre">parse_category</span></tt> 是spider的方法，用于从页面中处理也提取数据。</p>
<p>spider的代码如下:</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.contrib.linkextractors import LinkExtractor
from scrapy.contrib.spiders import CrawlSpider, Rule

class GoogleDirectorySpider(CrawlSpider):
    name = &#39;directory.google.com&#39;
    allowed_domains = [&#39;directory.google.com&#39;]
    start_urls = [&#39;http://directory.google.com/&#39;]

    rules = (
        Rule(LinkExtractor(allow=&#39;directory\.google\.com/[A-Z][a-zA-Z_/]+$&#39;),
            &#39;parse_category&#39;, follow=True,
        ),
    )

    def parse_category(self, response):
        # write the category page data extraction code here
        pass
</pre></div>
</div>
</div>
<div class="section" id="id4">
<h4>提取数据<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h4>
<p>现在我们来编写提取数据的代码。</p>
<p>在Firebug的帮助下，我们将查看一些包含网站链接的网页(以 <a class="reference external" href="http://directory.google.com/Top/Arts/Awards/">http://directory.google.com/Top/Arts/Awards/</a> 为例)，
找到使用 <a class="reference internal" href="index.html#topics-selectors"><em>Selectors</em></a> 提取链接的方法。
我们也将使用 <a class="reference internal" href="index.html#topics-shell"><em>Scrapy shell</em></a> 来测试得到的XPath表达式，确保表达式工作符合预期。</p>
<a class="reference internal image-reference" href="_images/firebug3.png"><img alt="Inspecting elements with Firebug" src="_images/firebug3.png" style="width: 965px; height: 751px;" /></a>
<p>正如您所看到的那样，页面的标记并不是十分明显: 元素并不包含
<tt class="docutils literal"><span class="pre">id</span></tt> ， <tt class="docutils literal"><span class="pre">class</span></tt> 或任何可以区分的属性。所以我们将使用等级槽(rank bar)作为指示点来选择提取的数据，创建XPath。</p>
<p>使用Firebug，我们可以看到每个链接都在 <tt class="docutils literal"><span class="pre">td</span></tt> 标签中。该标签存在于同时(在另一个 <tt class="docutils literal"><span class="pre">td</span></tt>)包含链接的等级槽(ranking bar)的 <tt class="docutils literal"><span class="pre">tr</span></tt> 中。</p>
<p>所以我们选择等级槽(ranking bar)，接着找到其父节点(<tt class="docutils literal"><span class="pre">tr</span></tt>)，最后是(包含我们要爬取数据的)链接的 <tt class="docutils literal"><span class="pre">td</span></tt> 。</p>
<p>对应的XPath:</p>
<div class="highlight-none"><div class="highlight"><pre>//td[descendant::a[contains(@href, &quot;#pagerank&quot;)]]/following-sibling::td//a
</pre></div>
</div>
<p>使用 <a class="reference internal" href="index.html#topics-shell"><em>Scrapy终端</em></a> 来测试这些复杂的XPath表达式，确保其工作符合预期。</p>
<p>简单来说，该表达式会查找等级槽的 <tt class="docutils literal"><span class="pre">td</span></tt> 元素，接着选择所有 <tt class="docutils literal"><span class="pre">td</span></tt> 元素，该元素拥有子孙 <tt class="docutils literal"><span class="pre">a</span></tt> 元素，且 <tt class="docutils literal"><span class="pre">a</span></tt> 元素的属性 <tt class="docutils literal"><span class="pre">href</span></tt> 包含字符串
<tt class="docutils literal"><span class="pre">#pagerank</span></tt> 。</p>
<p>当然，这不是唯一的XPath，也许也不是选择数据的最简单的那个。
其他的方法也可能是，例如，选择灰色的链接的 <tt class="docutils literal"><span class="pre">font</span></tt> 标签。</p>
<p>最终，我们编写 <tt class="docutils literal"><span class="pre">parse_category()</span></tt> 方法:</p>
<div class="highlight-none"><div class="highlight"><pre>def parse_category(self, response):

    # The path to website links in directory page
    links = response.xpath(&#39;//td[descendant::a[contains(@href, &quot;#pagerank&quot;)]]/following-sibling::td/font&#39;)

    for link in links:
        item = DirectoryItem()
        item[&#39;name&#39;] = link.xpath(&#39;a/text()&#39;).extract()
        item[&#39;url&#39;] = link.xpath(&#39;a/@href&#39;).extract()
        item[&#39;description&#39;] = link.xpath(&#39;font[2]/text()&#39;).extract()
        yield item
</pre></div>
</div>
<p>注意，您可能会遇到有些在Firebug找到，但是在原始HTML中找不到的元素，
例如典型的 <tt class="docutils literal"><span class="pre">&lt;tbody&gt;</span></tt> 元素，
或者Firebug检查活动DOM(live DOM)所看到的元素，但元素由javascript动态生成，并不在HTML源码中。
(原文语句乱了,上面为意译- -:
or tags which Therefer   in page HTML
sources may on Firebug inspects the live DOM
)</p>
</div>
</div>
<span id="document-topics/leaks"></span><div class="section" id="topics-leaks">
<span id="id1"></span><h3>调试内存溢出<a class="headerlink" href="#topics-leaks" title="永久链接至标题">¶</a></h3>
<p>在Scrapy中，类似Requests, Response及Items的对象具有有限的生命周期:
他们被创建，使用，最后被销毁。</p>
<p>这些对象中，Request的生命周期应该是最长的，其会在调度队列(Scheduler queue)中一直等待，直到被处理。
更多内容请参考 <a class="reference internal" href="index.html#topics-architecture"><em>架构概览</em></a> 。</p>
<p>由于这些Scrapy对象拥有很长的生命，因此将这些对象存储在内存而没有正确释放的危险总是存在。
而这导致了所谓的&#8221;内存泄露&#8221;。</p>
<p>为了帮助调试内存泄露，Scrapy提供了跟踪对象引用的机制，叫做 <a class="reference internal" href="index.html#topics-leaks-trackrefs"><em>trackref</em></a> ，
或者您也可以使用第三方提供的更先进内存调试库 <a class="reference internal" href="index.html#topics-leaks-guppy"><em>Guppy</em></a>
(更多内容请查看下面)。而这都必须在 <a class="reference internal" href="index.html#topics-telnetconsole"><em>Telnet终端</em></a> 中使用。</p>
<div class="section" id="id2">
<h4>内存泄露的常见原因<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h4>
<p>内存泄露经常是由于Scrapy开发者在Requests中(有意或无意)传递对象的引用(例如，使用
<a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">meta</span></tt></a> 属性或request回调函数)，使得该对象的生命周期与
Request的生命周期所绑定。这是目前为止最常见的内存泄露的原因，
同时对新手来说也是一个比较难调试的问题。</p>
<p>在大项目中，spider是由不同的人所编写的。而这其中有的spider可能是有&#8221;泄露的&#8221;，
当所有的爬虫同时运行时，这些影响了其他(写好)的爬虫，最终，影响了整个爬取进程。</p>
<p>与此同时，在不限制框架的功能的同时避免造成这些造成泄露的原因是十分困难的。因此，
我们决定不限制这些功能而是提供调试这些泄露的实用工具。这些工具回答了一个问题:
<em>哪个spider在泄露</em> 。</p>
<p>内存泄露可能存在与一个您编写的中间件，管道(pipeline) 或扩展，在代码中您没有正确释放
(之前分配的)资源。例如，您在 <a class="reference internal" href="index.html#std:signal-spider_opened"><tt class="xref std std-signal docutils literal"><span class="pre">spider_opened</span></tt></a> 中分配资源但在
<a class="reference internal" href="index.html#std:signal-spider_closed"><tt class="xref std std-signal docutils literal"><span class="pre">spider_closed</span></tt></a> 中没有释放它们。</p>
</div>
<div class="section" id="trackref">
<span id="topics-leaks-trackrefs"></span><h4>使用 <tt class="docutils literal"><span class="pre">trackref</span></tt> 调试内存泄露<a class="headerlink" href="#trackref" title="永久链接至标题">¶</a></h4>
<p><tt class="docutils literal"><span class="pre">trackref</span></tt> 是Scrapy提供用于调试大部分内存泄露情况的模块。
简单来说，其追踪了所有活动(live)的Request, Request, Item及Selector对象的引用。</p>
<p>您可以进入telnet终端并通过 <tt class="docutils literal"><span class="pre">prefs()</span></tt> 功能来检查多少(上面所提到的)活跃(alive)对象。
<tt class="docutils literal"><span class="pre">pref()</span></tt> 是 <a class="reference internal" href="index.html#scrapy.utils.trackref.print_live_refs" title="scrapy.utils.trackref.print_live_refs"><tt class="xref py py-func docutils literal"><span class="pre">print_live_refs()</span></tt></a> 功能的引用:</p>
<div class="highlight-none"><div class="highlight"><pre>telnet localhost 6023

&gt;&gt;&gt; prefs()
Live References

ExampleSpider                       1   oldest: 15s ago
HtmlResponse                       10   oldest: 1s ago
Selector                            2   oldest: 0s ago
FormRequest                       878   oldest: 7s ago
</pre></div>
</div>
<p>正如所见，报告也展现了每个类中最老的对象的时间(age)。</p>
<p>如果您有内存泄露，那您能找到哪个spider正在泄露的机会是查看最老的request或response。
您可以使用 <a class="reference internal" href="index.html#scrapy.utils.trackref.get_oldest" title="scrapy.utils.trackref.get_oldest"><tt class="xref py py-func docutils literal"><span class="pre">get_oldest()</span></tt></a> 方法来获取每个类中最老的对象，
正如此所示(在终端中)(原文档没有样例)。</p>
<div class="section" id="id3">
<h5>哪些对象被追踪了?<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h5>
<p><tt class="docutils literal"><span class="pre">trackref</span></tt> 追踪的对象包括以下类(及其子类)的对象:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">scrapy.http.Request</span></tt></li>
<li><tt class="docutils literal"><span class="pre">scrapy.http.Response</span></tt></li>
<li><tt class="docutils literal"><span class="pre">scrapy.item.Item</span></tt></li>
<li><tt class="docutils literal"><span class="pre">scrapy.selector.Selector</span></tt></li>
<li><tt class="docutils literal"><span class="pre">scrapy.spider.Spider</span></tt></li>
</ul>
</div>
<div class="section" id="id4">
<h5>真实例子<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h5>
<p>让我们来看一个假设的具有内存泄露的准确例子。</p>
<p>假如我们有些spider的代码中有一行类似于这样的代码:</p>
<div class="highlight-none"><div class="highlight"><pre>return Request(&quot;http://www.somenastyspider.com/product.php?pid=%d&quot; % product_id,
    callback=self.parse, meta={referer: response}&quot;)
</pre></div>
</div>
<p>代码中在request中传递了一个response的引用，使得reponse的生命周期与request所绑定，
进而造成了内存泄露。</p>
<p>让我们来看看如何使用 <tt class="docutils literal"><span class="pre">trackref</span></tt> 工具来发现哪一个是有问题的spider(当然是在不知道任何的前提的情况下)。</p>
<p>当crawler运行了一小阵子后，我们发现内存占用增长了很多。
这时候我们进入telnet终端，查看活跃(live)的引用:</p>
<div class="highlight-none"><div class="highlight"><pre>&gt;&gt;&gt; prefs()
Live References

SomenastySpider                     1   oldest: 15s ago
HtmlResponse                     3890   oldest: 265s ago
Selector                            2   oldest: 0s ago
Request                          3878   oldest: 250s ago
</pre></div>
</div>
<p>上面具有非常多的活跃(且运行时间很长)的response，而其比Request的时间还要长的现象肯定是有问题的。
因此，查看最老的response:</p>
<div class="highlight-none"><div class="highlight"><pre>&gt;&gt;&gt; from scrapy.utils.trackref import get_oldest
&gt;&gt;&gt; r = get_oldest(&#39;HtmlResponse&#39;)
&gt;&gt;&gt; r.url
&#39;http://www.somenastyspider.com/product.php?pid=123&#39;
</pre></div>
</div>
<p>就这样，通过查看最老的response的URL，我们发现其属于 <tt class="docutils literal"><span class="pre">somenastyspider.com</span></tt> spider。
现在我们可以查看该spider的代码并发现导致泄露的那行代码(在request中传递response的引用)。</p>
<p>如果您想要遍历所有而不是最老的对象，您可以使用 <tt class="xref py py-func docutils literal"><span class="pre">iter_all()</span></tt> 方法:</p>
<div class="highlight-none"><div class="highlight"><pre>&gt;&gt;&gt; from scrapy.utils.trackref import iter_all
&gt;&gt;&gt; [r.url for r in iter_all(&#39;HtmlResponse&#39;)]
[&#39;http://www.somenastyspider.com/product.php?pid=123&#39;,
 &#39;http://www.somenastyspider.com/product.php?pid=584&#39;,
...
</pre></div>
</div>
</div>
<div class="section" id="spider">
<h5>很多spider?<a class="headerlink" href="#spider" title="永久链接至标题">¶</a></h5>
<p>如果您的项目有很多的spider，<tt class="docutils literal"><span class="pre">prefs()</span></tt> 的输出会变得很难阅读。针对于此，
该方法具有 <tt class="docutils literal"><span class="pre">ignore</span></tt> 参数，用于忽略特定的类(及其子类)。例如:</p>
<div class="highlight-none"><div class="highlight"><pre>&gt;&gt;&gt; from scrapy.spider import Spider
&gt;&gt;&gt; prefs(ignore=Spider)
</pre></div>
</div>
<p>将不会展现任何spider的活跃引用。</p>
<span class="target" id="module-scrapy.utils.trackref"></span></div>
<div class="section" id="scrapy-utils-trackref">
<h5>scrapy.utils.trackref模块<a class="headerlink" href="#scrapy-utils-trackref" title="永久链接至标题">¶</a></h5>
<p>以下是 <a class="reference internal" href="index.html#module-scrapy.utils.trackref" title="scrapy.utils.trackref: Track references of live objects"><tt class="xref py py-mod docutils literal"><span class="pre">trackref</span></tt></a> 模块中可用的方法。</p>
<dl class="class">
<dt id="scrapy.utils.trackref.object_ref">
<em class="property">class </em><tt class="descclassname">scrapy.utils.trackref.</tt><tt class="descname">object_ref</tt><a class="headerlink" href="#scrapy.utils.trackref.object_ref" title="永久链接至目标">¶</a></dt>
<dd><p>如果您想通过 <tt class="docutils literal"><span class="pre">trackref</span></tt> 模块追踪活跃的实例，继承该类(而不是对象)。</p>
</dd></dl>

<dl class="function">
<dt id="scrapy.utils.trackref.print_live_refs">
<tt class="descclassname">scrapy.utils.trackref.</tt><tt class="descname">print_live_refs</tt><big>(</big><em>class_name</em>, <em>ignore=NoneType</em><big>)</big><a class="headerlink" href="#scrapy.utils.trackref.print_live_refs" title="永久链接至目标">¶</a></dt>
<dd><p>打印活跃引用的报告，以类名分类。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><strong>ignore</strong> (<em>类或者类的元组</em>) &#8211; 如果给定，所有指定类(或者类的元组)的对象将会被忽略。</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="scrapy.utils.trackref.get_oldest">
<tt class="descclassname">scrapy.utils.trackref.</tt><tt class="descname">get_oldest</tt><big>(</big><em>class_name</em><big>)</big><a class="headerlink" href="#scrapy.utils.trackref.get_oldest" title="永久链接至目标">¶</a></dt>
<dd><p>返回给定类名的最老活跃(alive)对象，如果没有则返回 <tt class="docutils literal"><span class="pre">None</span></tt> 。首先使用
<a class="reference internal" href="index.html#scrapy.utils.trackref.print_live_refs" title="scrapy.utils.trackref.print_live_refs"><tt class="xref py py-func docutils literal"><span class="pre">print_live_refs()</span></tt></a> 来获取每个类所跟踪的所有活跃(live)对象的列表。</p>
</dd></dl>

<dl class="function">
<dt id="scrapy.utils.trackref.iter_all">
<tt class="descclassname">scrapy.utils.trackref.</tt><tt class="descname">iter_all</tt><big>(</big><em>class_name</em><big>)</big><a class="headerlink" href="#scrapy.utils.trackref.iter_all" title="永久链接至目标">¶</a></dt>
<dd><p>返回一个能给定类名的所有活跃对象的迭代器，如果没有则返回 <tt class="docutils literal"><span class="pre">None</span></tt> 。首先使用
<a class="reference internal" href="index.html#scrapy.utils.trackref.print_live_refs" title="scrapy.utils.trackref.print_live_refs"><tt class="xref py py-func docutils literal"><span class="pre">print_live_refs()</span></tt></a> 来获取每个类所跟踪的所有活跃(live)对象的列表。</p>
</dd></dl>

</div>
</div>
<div class="section" id="guppy">
<span id="topics-leaks-guppy"></span><h4>使用Guppy调试内存泄露<a class="headerlink" href="#guppy" title="永久链接至标题">¶</a></h4>
<p><tt class="docutils literal"><span class="pre">trackref</span></tt> 提供了追踪内存泄露非常方便的机制，其仅仅追踪了比较可能导致内存泄露的对象
(Requests, Response, Items及Selectors)。然而，内存泄露也有可能来自其他(更为隐蔽的)对象。
如果是因为这个原因，通过 <tt class="docutils literal"><span class="pre">trackref</span></tt> 则无法找到泄露点，您仍然有其他工具: <a class="reference external" href="http://pypi.python.org/pypi/guppy">Guppy library</a> 。</p>
<p>如果使用 <tt class="docutils literal"><span class="pre">setuptools</span></tt> , 您可以通过下列命令安装Guppy:</p>
<div class="highlight-none"><div class="highlight"><pre>easy_install guppy
</pre></div>
</div>
<p>telnet终端也提供了快捷方式(<tt class="docutils literal"><span class="pre">hpy</span></tt>)来访问Guppy堆对象(heap objects)。
下面给出了查看堆中所有可用的Python对象的例子:</p>
<div class="highlight-none"><div class="highlight"><pre>&gt;&gt;&gt; x = hpy.heap()
&gt;&gt;&gt; x.bytype
Partition of a set of 297033 objects. Total size = 52587824 bytes.
 Index  Count   %     Size   % Cumulative  % Type
     0  22307   8 16423880  31  16423880  31 dict
     1 122285  41 12441544  24  28865424  55 str
     2  68346  23  5966696  11  34832120  66 tuple
     3    227   0  5836528  11  40668648  77 unicode
     4   2461   1  2222272   4  42890920  82 type
     5  16870   6  2024400   4  44915320  85 function
     6  13949   5  1673880   3  46589200  89 types.CodeType
     7  13422   5  1653104   3  48242304  92 list
     8   3735   1  1173680   2  49415984  94 _sre.SRE_Pattern
     9   1209   0   456936   1  49872920  95 scrapy.http.headers.Headers
&lt;1676 more rows. Type e.g. &#39;_.more&#39; to view.&gt;
</pre></div>
</div>
<p>您可以看到大部分的空间被字典所使用。接着，如果您想要查看哪些属性引用了这些字典，
您可以:</p>
<div class="highlight-none"><div class="highlight"><pre>&gt;&gt;&gt; x.bytype[0].byvia
Partition of a set of 22307 objects. Total size = 16423880 bytes.
 Index  Count   %     Size   % Cumulative  % Referred Via:
     0  10982  49  9416336  57   9416336  57 &#39;.__dict__&#39;
     1   1820   8  2681504  16  12097840  74 &#39;.__dict__&#39;, &#39;.func_globals&#39;
     2   3097  14  1122904   7  13220744  80
     3    990   4   277200   2  13497944  82 &quot;[&#39;cookies&#39;]&quot;
     4    987   4   276360   2  13774304  84 &quot;[&#39;cache&#39;]&quot;
     5    985   4   275800   2  14050104  86 &quot;[&#39;meta&#39;]&quot;
     6    897   4   251160   2  14301264  87 &#39;[2]&#39;
     7      1   0   196888   1  14498152  88 &quot;[&#39;moduleDict&#39;]&quot;, &quot;[&#39;modules&#39;]&quot;
     8    672   3   188160   1  14686312  89 &quot;[&#39;cb_kwargs&#39;]&quot;
     9     27   0   155016   1  14841328  90 &#39;[1]&#39;
&lt;333 more rows. Type e.g. &#39;_.more&#39; to view.&gt;
</pre></div>
</div>
<p>如上所示，Guppy模块十分强大，不过也需要一些关于Python内部的知识。关于Guppy的更多内容请参考
<a class="reference external" href="http://guppy-pe.sourceforge.net/">Guppy documentation</a>.</p>
</div>
<div class="section" id="leaks-without-leaks">
<span id="topics-leaks-without-leaks"></span><h4>Leaks without leaks<a class="headerlink" href="#leaks-without-leaks" title="永久链接至标题">¶</a></h4>
<p>有时候，您可能会注意到Scrapy进程的内存占用只在增长，从不下降。不幸的是，
有时候这并不是Scrapy或者您的项目在泄露内存。这是由于一个已知(但不有名)的Python问题。
Python在某些情况下可能不会返回已经释放的内存到操作系统。关于这个问题的更多内容请看:</p>
<ul class="simple">
<li><a class="reference external" href="http://evanjones.ca/python-memory.html">Python Memory Management</a></li>
<li><a class="reference external" href="http://evanjones.ca/python-memory-part2.html">Python Memory Management Part 2</a></li>
<li><a class="reference external" href="http://evanjones.ca/python-memory-part3.html">Python Memory Management Part 3</a></li>
</ul>
<p>改进方案由Evan Jones提出，在 <a class="reference external" href="http://evanjones.ca/memoryallocator/">这篇文章</a> 中详细介绍，在Python 2.5中合并。
不过这仅仅减小了这个问题，并没有完全修复。引用这片文章:</p>
<blockquote>
<div><em>不幸的是，这个patch仅仅会释放没有在其内部分配对象的区域(arena)。这意味着
碎片化是一个大问题。某个应用可以拥有很多空闲内存，分布在所有的区域(arena)中，
但是没法释放任何一个。这个问题存在于所有内存分配器中。解决这个问题的唯一办法是
转化到一个更为紧凑(compact)的垃圾回收器，其能在内存中移动对象。
这需要对Python解析器做一个显著的修改。</em></div></blockquote>
<p>这个问题将会在未来Scrapy发布版本中得到解决。我们打算转化到一个新的进程模型，
并在可回收的子进程池中运行spider。</p>
</div>
</div>
<span id="document-topics/images"></span><div class="section" id="topics-images">
<span id="id1"></span><h3>下载项目图片<a class="headerlink" href="#topics-images" title="永久链接至标题">¶</a></h3>
<p>Scrapy提供了一个 <a class="reference internal" href="index.html#document-topics/item-pipeline"><em>item pipeline</em></a> ，来下载属于某个特定项目的图片，比如，当你抓取产品时，也想把它们的图片下载到本地。</p>
<p>这条管道，被称作图片管道，在 <a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline" title="scrapy.contrib.pipeline.images.ImagesPipeline"><tt class="xref py py-class docutils literal"><span class="pre">ImagesPipeline</span></tt></a> 类中实现，提供了一个方便并具有额外特性的方法，来下载并本地存储图片:</p>
<ul class="simple">
<li>将所有下载的图片转换成通用的格式（JPG）和模式（RGB）</li>
<li>避免重新下载最近已经下载过的图片</li>
<li>缩略图生成</li>
<li>检测图像的宽/高，确保它们满足最小限制</li>
</ul>
<p>这个管道也会为那些当前安排好要下载的图片保留一个内部队列，并将那些到达的包含相同图片的项目连接到那个队列中。
这可以避免多次下载几个项目共享的同一个图片。</p>
<p><a class="reference external" href="https://github.com/python-imaging/Pillow">Pillow</a> 是用来生成缩略图，并将图片归一化为JPEG/RGB格式，因此为了使用图片管道，你需要安装这个库。
<a class="reference external" href="http://www.pythonware.com/products/pil/">Python Imaging Library</a> (PIL) 在大多数情况下是有效的，但众所周知，在一些设置里会出现问题，因此我们推荐使用 <a class="reference external" href="https://github.com/python-imaging/Pillow">Pillow</a> 而不是 <a class="reference external" href="PythonImagingLibrary">PIL</a>.</p>
<div class="section" id="id2">
<h4>使用图片管道<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h4>
<p>当使用 <a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline" title="scrapy.contrib.pipeline.images.ImagesPipeline"><tt class="xref py py-class docutils literal"><span class="pre">ImagesPipeline</span></tt></a> ，典型的工作流程如下所示:</p>
<ol class="arabic simple">
<li>在一个爬虫里，你抓取一个项目，把其中图片的URL放入 <tt class="docutils literal"><span class="pre">image_urls</span></tt> 组内。</li>
<li>项目从爬虫内返回，进入项目管道。</li>
<li>当项目进入 <a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline" title="scrapy.contrib.pipeline.images.ImagesPipeline"><tt class="xref py py-class docutils literal"><span class="pre">ImagesPipeline</span></tt></a>，<tt class="docutils literal"><span class="pre">image_urls</span></tt> 组内的URLs将被Scrapy的调度器和下载器（这意味着调度器和下载器的中间件可以复用）安排下载，当优先级更高，会在其他页面被抓取前处理。项目会在这个特定的管道阶段保持“locker”的状态，直到完成图片的下载（或者由于某些原因未完成下载）。</li>
<li>当图片下载完，另一个组(<tt class="docutils literal"><span class="pre">images</span></tt>)将被更新到结构中。这个组将包含一个字典列表，其中包括下载图片的信息，比如下载路径、源抓取地址（从 <tt class="docutils literal"><span class="pre">image_urls</span></tt> 组获得）和图片的校验码。
<tt class="docutils literal"><span class="pre">images</span></tt> 列表中的图片顺序将和源 <tt class="docutils literal"><span class="pre">image_urls</span></tt> 组保持一致。如果某个图片下载失败，将会记录下错误信息，图片也不会出现在 <tt class="docutils literal"><span class="pre">images</span></tt> 组中。</li>
</ol>
</div>
<div class="section" id="id3">
<h4>使用样例<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h4>
<p>为了使用图片管道，你仅需要 <a class="reference internal" href="index.html#topics-images-enabling"><em>启动它</em></a> 并用 <tt class="docutils literal"><span class="pre">image_urls</span></tt> 和 <tt class="docutils literal"><span class="pre">images</span></tt> 定义一个项目:</p>
<div class="highlight-none"><div class="highlight"><pre>import scrapy

class MyItem(scrapy.Item):

    # ... other item fields ...
    image_urls = scrapy.Field()
    images = scrapy.Field()
</pre></div>
</div>
<p>如果你需要更加复杂的功能，想重写定制图片管道行为，参见 <a class="reference internal" href="index.html#topics-images-override"><em>实现定制图片管道</em></a> 。</p>
</div>
<div class="section" id="topics-images-enabling">
<span id="id4"></span><h4>开启你的图片管道<a class="headerlink" href="#topics-images-enabling" title="永久链接至标题">¶</a></h4>
<p id="std:setting-IMAGES_STORE">为了开启你的图片管道，你首先需要在项目中添加它 <a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES"><tt class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></tt></a> setting:</p>
<div class="highlight-none"><div class="highlight"><pre>ITEM_PIPELINES = {&#39;scrapy.contrib.pipeline.images.ImagesPipeline&#39;: 1}
</pre></div>
</div>
<p>并将 <a class="reference internal" href="index.html#std:setting-IMAGES_STORE"><tt class="xref std std-setting docutils literal"><span class="pre">IMAGES_STORE</span></tt></a> 设置为一个有效的文件夹，用来存储下载的图片。否则管道将保持禁用状态，即使你在 <a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES"><tt class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></tt></a> 设置中添加了它。</p>
<p>比如:</p>
<div class="highlight-none"><div class="highlight"><pre>IMAGES_STORE = &#39;/path/to/valid/dir&#39;
</pre></div>
</div>
</div>
<div class="section" id="id5">
<h4>图片存储<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h4>
<p>文件系统是当前官方唯一支持的存储系统，但也支持（非公开的） <a class="reference external" href="https://s3.amazonaws.com/">Amazon S3</a> 。</p>
<div class="section" id="id6">
<h5>文件系统存储<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h5>
<p>图片存储在文件中（一个图片一个文件），并使用它们URL的 <a class="reference external" href="http://en.wikipedia.org/wiki/SHA_hash_functions">SHA1 hash</a> 作为文件名。</p>
<p>比如，对下面的图片URL:</p>
<div class="highlight-none"><div class="highlight"><pre>http://www.example.com/image.jpg
</pre></div>
</div>
<p>它的 <cite>SHA1 hash</cite> 值为:</p>
<div class="highlight-none"><div class="highlight"><pre>3afec3b4765f8f0a07b78f98c07b83f013567a0a
</pre></div>
</div>
<p>将被下载并存为下面的文件:</p>
<div class="highlight-none"><div class="highlight"><pre>&lt;IMAGES_STORE&gt;/full/3afec3b4765f8f0a07b78f98c07b83f013567a0a.jpg
</pre></div>
</div>
<p>其中:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">&lt;IMAGES_STORE&gt;</span></tt> 是定义在 <a class="reference internal" href="index.html#std:setting-IMAGES_STORE"><tt class="xref std std-setting docutils literal"><span class="pre">IMAGES_STORE</span></tt></a> 设置里的文件夹</li>
<li><tt class="docutils literal"><span class="pre">full</span></tt> 是用来区分图片和缩略图（如果使用的话）的一个子文件夹。详情参见 <a class="reference internal" href="index.html#topics-images-thumbnails"><em>缩略图生成</em></a>.</li>
</ul>
</div>
</div>
<div class="section" id="id7">
<h4>额外的特性<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h4>
<div class="section" id="id8">
<h5>图片失效<a class="headerlink" href="#id8" title="永久链接至标题">¶</a></h5>
<p id="std:setting-IMAGES_EXPIRES">图像管道避免下载最近已经下载的图片。使用 <a class="reference internal" href="index.html#std:setting-IMAGES_EXPIRES"><tt class="xref std std-setting docutils literal"><span class="pre">IMAGES_EXPIRES</span></tt></a> 设置可以调整失效期限，可以用天数来指定:</p>
<div class="highlight-none"><div class="highlight"><pre># 90天的图片失效期限
IMAGES_EXPIRES = 90
</pre></div>
</div>
</div>
<div class="section" id="topics-images-thumbnails">
<span id="id9"></span><h5>缩略图生成<a class="headerlink" href="#topics-images-thumbnails" title="永久链接至标题">¶</a></h5>
<p>图片管道可以自动创建下载图片的缩略图。</p>
<p id="std:setting-IMAGES_THUMBS">为了使用这个特性，你需要设置 <a class="reference internal" href="index.html#std:setting-IMAGES_THUMBS"><tt class="xref std std-setting docutils literal"><span class="pre">IMAGES_THUMBS</span></tt></a> 字典，其关键字为缩略图名字，值为它们的大小尺寸。</p>
<p>比如:</p>
<div class="highlight-none"><div class="highlight"><pre>IMAGES_THUMBS = {
    &#39;small&#39;: (50, 50),
    &#39;big&#39;: (270, 270),
}
</pre></div>
</div>
<p>当你使用这个特性时，图片管道将使用下面的格式来创建各个特定尺寸的缩略图:</p>
<div class="highlight-none"><div class="highlight"><pre>&lt;IMAGES_STORE&gt;/thumbs/&lt;size_name&gt;/&lt;image_id&gt;.jpg
</pre></div>
</div>
<p>其中:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">&lt;size_name&gt;</span></tt> 是 <a class="reference internal" href="index.html#std:setting-IMAGES_THUMBS"><tt class="xref std std-setting docutils literal"><span class="pre">IMAGES_THUMBS</span></tt></a> 字典关键字（<tt class="docutils literal"><span class="pre">small</span></tt>， <tt class="docutils literal"><span class="pre">big</span></tt> ，等）</li>
<li><tt class="docutils literal"><span class="pre">&lt;image_id&gt;</span></tt> 是图像url的 <a class="reference external" href="http://en.wikipedia.org/wiki/SHA_hash_functions">SHA1 hash</a></li>
</ul>
<p>例如使用 <tt class="docutils literal"><span class="pre">small</span></tt> 和 <tt class="docutils literal"><span class="pre">big</span></tt> 缩略图名字的图片文件:</p>
<div class="highlight-none"><div class="highlight"><pre>&lt;IMAGES_STORE&gt;/full/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
&lt;IMAGES_STORE&gt;/thumbs/small/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
&lt;IMAGES_STORE&gt;/thumbs/big/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
</pre></div>
</div>
<p>第一个是从网站下载的完整图片。</p>
</div>
<div class="section" id="id10">
<h5>滤出小图片<a class="headerlink" href="#id10" title="永久链接至标题">¶</a></h5>
<span class="target" id="std:setting-IMAGES_MIN_HEIGHT"></span><p id="std:setting-IMAGES_MIN_WIDTH">你可以丢掉那些过小的图片，只需在:setting:<cite>IMAGES_MIN_HEIGHT</cite> 和 <a class="reference internal" href="index.html#std:setting-IMAGES_MIN_WIDTH"><tt class="xref std std-setting docutils literal"><span class="pre">IMAGES_MIN_WIDTH</span></tt></a> 设置中指定最小允许的尺寸。</p>
<p>比如:</p>
<div class="highlight-none"><div class="highlight"><pre>IMAGES_MIN_HEIGHT = 110
IMAGES_MIN_WIDTH = 110
</pre></div>
</div>
<p>注意：这些尺寸一点也不影响缩略图的生成。</p>
<p>默认情况下，没有尺寸限制，因此所有图片都将处理。</p>
</div>
</div>
<div class="section" id="module-scrapy.contrib.pipeline.images">
<span id="id11"></span><span id="topics-images-override"></span><h4>实现定制图片管道<a class="headerlink" href="#module-scrapy.contrib.pipeline.images" title="永久链接至标题">¶</a></h4>
<p>下面是你可以在定制的图片管道里重写的方法：</p>
<dl class="class">
<dt id="scrapy.contrib.pipeline.images.ImagesPipeline">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.pipeline.images.</tt><tt class="descname">ImagesPipeline</tt><a class="headerlink" href="#scrapy.contrib.pipeline.images.ImagesPipeline" title="永久链接至目标">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.contrib.pipeline.images.ImagesPipeline.get_media_requests">
<tt class="descname">get_media_requests</tt><big>(</big><em>item</em>, <em>info</em><big>)</big><a class="headerlink" href="#scrapy.contrib.pipeline.images.ImagesPipeline.get_media_requests" title="永久链接至目标">¶</a></dt>
<dd><p>在工作流程中可以看到，管道会得到图片的URL并从项目中下载。为了这么做，你需要重写 <a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline.get_media_requests" title="scrapy.contrib.pipeline.images.ImagesPipeline.get_media_requests"><tt class="xref py py-meth docutils literal"><span class="pre">get_media_requests()</span></tt></a> 方法，并对各个图片URL返回一个Request:</p>
<div class="highlight-none"><div class="highlight"><pre>def get_media_requests(self, item, info):
    for image_url in item[&#39;image_urls&#39;]:
        yield scrapy.Request(image_url)
</pre></div>
</div>
<p>这些请求将被管道处理，当它们完成下载后，结果将以2-元素的元组列表形式传送到 <a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline.item_completed" title="scrapy.contrib.pipeline.images.ImagesPipeline.item_completed"><tt class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></tt></a> 方法:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">success</span></tt> 是一个布尔值，当图片成功下载时为 <tt class="docutils literal"><span class="pre">True</span></tt> ，因为某个原因下载失败为``False``</li>
<li><tt class="docutils literal"><span class="pre">image_info_or_error</span></tt> 是一个包含下列关键字的字典（如果成功为 <tt class="docutils literal"><span class="pre">True</span></tt> ）或者出问题时为 <a class="reference external" href="http://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Twisted Failure</a> 。<ul>
<li><tt class="docutils literal"><span class="pre">url</span></tt> - 图片下载的url。这是从 <a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline.get_media_requests" title="scrapy.contrib.pipeline.images.ImagesPipeline.get_media_requests"><tt class="xref py py-meth docutils literal"><span class="pre">get_media_requests()</span></tt></a> 方法返回请求的url。</li>
<li><tt class="docutils literal"><span class="pre">path</span></tt> - 图片存储的路径（类似 <a class="reference internal" href="index.html#std:setting-IMAGES_STORE"><tt class="xref std std-setting docutils literal"><span class="pre">IMAGES_STORE</span></tt></a>）</li>
<li><tt class="docutils literal"><span class="pre">checksum</span></tt> - 图片内容的 <a class="reference external" href="http://en.wikipedia.org/wiki/MD5">MD5 hash</a></li>
</ul>
</li>
</ul>
<p><a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline.item_completed" title="scrapy.contrib.pipeline.images.ImagesPipeline.item_completed"><tt class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></tt></a> 接收的元组列表需要保证与 <a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline.get_media_requests" title="scrapy.contrib.pipeline.images.ImagesPipeline.get_media_requests"><tt class="xref py py-meth docutils literal"><span class="pre">get_media_requests()</span></tt></a> 方法返回请求的顺序相一致。下面是 <tt class="docutils literal"><span class="pre">results</span></tt> 参数的一个典型值:</p>
<div class="highlight-none"><div class="highlight"><pre>[(True,
  {&#39;checksum&#39;: &#39;2b00042f7481c7b056c4b410d28f33cf&#39;,
   &#39;path&#39;: &#39;full/7d97e98f8af710c7e7fe703abc8f639e0ee507c4.jpg&#39;,
   &#39;url&#39;: &#39;http://www.example.com/images/product1.jpg&#39;}),
 (True,
  {&#39;checksum&#39;: &#39;b9628c4ab9b595f72f280b90c4fd093d&#39;,
   &#39;path&#39;: &#39;full/1ca5879492b8fd606df1964ea3c1e2f4520f076f.jpg&#39;,
   &#39;url&#39;: &#39;http://www.example.com/images/product2.jpg&#39;}),
 (False,
  Failure(...))]
</pre></div>
</div>
<p>默认 <a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline.get_media_requests" title="scrapy.contrib.pipeline.images.ImagesPipeline.get_media_requests"><tt class="xref py py-meth docutils literal"><span class="pre">get_media_requests()</span></tt></a> 方法返回 <tt class="docutils literal"><span class="pre">None</span></tt> ，这意味着项目中没有图片可下载。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.pipeline.images.ImagesPipeline.item_completed">
<tt class="descname">item_completed</tt><big>(</big><em>results</em>, <em>items</em>, <em>info</em><big>)</big><a class="headerlink" href="#scrapy.contrib.pipeline.images.ImagesPipeline.item_completed" title="永久链接至目标">¶</a></dt>
<dd><p>当一个单独项目中的所有图片请求完成时（要么完成下载，要么因为某种原因下载失败）， <a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline.item_completed" title="scrapy.contrib.pipeline.images.ImagesPipeline.item_completed"><tt class="xref py py-meth docutils literal"><span class="pre">ImagesPipeline.item_completed()</span></tt></a> 方法将被调用。</p>
<blockquote>
<div><a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline.item_completed" title="scrapy.contrib.pipeline.images.ImagesPipeline.item_completed"><tt class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></tt></a> 方法需要返回一个输出，其将被送到随后的项目管道阶段，因此你需要返回（或者丢弃）项目，如你在任意管道里所做的一样。</div></blockquote>
<p>这里是一个 <a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline.item_completed" title="scrapy.contrib.pipeline.images.ImagesPipeline.item_completed"><tt class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></tt></a> 方法的例子，其中我们将下载的图片路径（传入到results中）存储到 <tt class="docutils literal"><span class="pre">image_paths</span></tt> 项目组中，如果其中没有图片，我们将丢弃项目:</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.exceptions import DropItem

def item_completed(self, results, item, info):
    image_paths = [x[&#39;path&#39;] for ok, x in results if ok]
    if not image_paths:
        raise DropItem(&quot;Item contains no images&quot;)
    item[&#39;image_paths&#39;] = image_paths
    return item
</pre></div>
</div>
<p>默认情况下， <a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline.item_completed" title="scrapy.contrib.pipeline.images.ImagesPipeline.item_completed"><tt class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></tt></a> 方法返回项目。</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="id12">
<h4>定制图片管道的例子<a class="headerlink" href="#id12" title="永久链接至标题">¶</a></h4>
<p>下面是一个图片管道的完整例子，其方法如上所示:</p>
<div class="highlight-none"><div class="highlight"><pre>import scrapy
from scrapy.contrib.pipeline.images import ImagesPipeline
from scrapy.exceptions import DropItem

class MyImagesPipeline(ImagesPipeline):

    def get_media_requests(self, item, info):
        for image_url in item[&#39;image_urls&#39;]:
            yield scrapy.Request(image_url)

    def item_completed(self, results, item, info):
        image_paths = [x[&#39;path&#39;] for ok, x in results if ok]
        if not image_paths:
            raise DropItem(&quot;Item contains no images&quot;)
        item[&#39;image_paths&#39;] = image_paths
        return item
</pre></div>
</div>
</div>
</div>
<span id="document-topics/ubuntu"></span><div class="section" id="ubuntu">
<span id="topics-ubuntu"></span><h3>Ubuntu 软件包<a class="headerlink" href="#ubuntu" title="永久链接至标题">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">0.10 新版功能.</span></p>
</div>
<p><a class="reference external" href="http://scrapinghub.com/">Scrapinghub</a> 发布的apt-get可获取版本通常比Ubuntu里更新，并且在比 <a class="reference external" href="https://github.com/scrapy/scrapy">Github 仓库</a>
(master &amp; stable branches) 稳定的同时还包括了最新的漏洞修复。</p>
<p>用法:</p>
<ol class="arabic">
<li><p class="first">把Scrapy签名的GPG密钥添加到APT的钥匙环中:</p>
<div class="highlight-none"><div class="highlight"><pre>sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 627220E7
</pre></div>
</div>
</li>
<li><p class="first">执行如下命令，创建 <cite>/etc/apt/sources.list.d/scrapy.list</cite> 文件:</p>
<div class="highlight-none"><div class="highlight"><pre>echo &#39;deb http://archive.scrapy.org/ubuntu scrapy main&#39; | sudo tee /etc/apt/sources.list.d/scrapy.list
</pre></div>
</div>
</li>
<li><p class="first">更新包列表并安装 scrapy-0.24:</p>
<pre class="literal-block">
sudo apt-get update &amp;&amp; sudo apt-get install scrapy-0.24
</pre>
</li>
</ol>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">如果你要升级Scrapy，请重复步骤3。</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">警告</p>
<p class="last">debian官方源提供的 <cite>python-scrapy</cite> 是一个非常老的版本且不再获得Scrapy团队支持。</p>
</div>
</div>
<span id="document-topics/scrapyd"></span><div class="section" id="scrapyd">
<span id="topics-scrapyd"></span><h3>Scrapyd<a class="headerlink" href="#scrapyd" title="永久链接至标题">¶</a></h3>
<p>Scrapyd被移动成为一个单独的项目。
其文档当前被托管在:</p>
<blockquote>
<div><a class="reference external" href="http://scrapyd.readthedocs.org/">http://scrapyd.readthedocs.org/</a></div></blockquote>
</div>
<span id="document-topics/autothrottle"></span><div class="section" id="autothrottle">
<h3>自动限速(AutoThrottle)扩展<a class="headerlink" href="#autothrottle" title="永久链接至标题">¶</a></h3>
<p>该扩展能根据Scrapy服务器及您爬取的网站的负载自动限制爬取速度。</p>
<div class="section" id="id1">
<h4>设计目标<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h4>
<ol class="arabic simple">
<li>更友好的对待网站，而不使用默认的下载延迟0。</li>
<li>自动调整scrapy来优化下载速度，使得用户不用调节下载延迟及并发请求数来找到优化的值。
用户只需指定允许的最大并发请求数，剩下的都交给扩展来完成。</li>
</ol>
</div>
<div class="section" id="id2">
<h4>扩展是如何实现的<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h4>
<p>在Scrapy中，下载延迟是通过计算建立TCP连接到接收到HTTP包头(header)之间的时间来测量的。</p>
<p>注意，由于Scrapy可能在忙着处理spider的回调函数或者无法下载，因此在合作的多任务环境下准确测量这些延迟是十分苦难的。
不过，这些延迟仍然是对Scrapy(甚至是服务器)繁忙程度的合理测量，而这扩展就是以此为前提进行编写的。</p>
</div>
<div class="section" id="autothrottle-algorithm">
<span id="id3"></span><h4>限速算法<a class="headerlink" href="#autothrottle-algorithm" title="永久链接至标题">¶</a></h4>
<p>算法根据以下规则调整下载延迟及并发数:</p>
<ol class="arabic simple">
<li>spider永远以1并发请求数及 <a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_START_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_START_DELAY</span></tt></a> 中指定的下载延迟启动。</li>
<li>当接收到回复时，下载延迟会调整到该回复的延迟与之前下载延迟之间的平均值。</li>
</ol>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">AutoThrottle扩展尊重标准Scrapy设置中的并发数及延迟。这意味着其永远不会设置一个比
<a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></tt></a> 更低的下载延迟或者比
<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><tt class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></tt></a> 更高的并发数
(或 <a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><tt class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></tt></a> ，取决于您使用哪一个)。</p>
</div>
</div>
<div class="section" id="id4">
<h4>设置<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h4>
<p>下面是控制AutoThrottle扩展的设置:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_ENABLED</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_START_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_START_DELAY</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_MAX_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_MAX_DELAY</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_DEBUG"><tt class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_DEBUG</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><tt class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><tt class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></tt></a></li>
</ul>
<p>更多内容请参考 <a class="reference internal" href="index.html#autothrottle-algorithm"><em>限速算法</em></a> 。</p>
<div class="section" id="autothrottle-enabled">
<span id="std:setting-AUTOTHROTTLE_ENABLED"></span><h5>AUTOTHROTTLE_ENABLED<a class="headerlink" href="#autothrottle-enabled" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>启用AutoThrottle扩展。</p>
</div>
<div class="section" id="autothrottle-start-delay">
<span id="std:setting-AUTOTHROTTLE_START_DELAY"></span><h5>AUTOTHROTTLE_START_DELAY<a class="headerlink" href="#autothrottle-start-delay" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">5.0</span></tt></p>
<p>初始下载延迟(单位:秒)。</p>
</div>
<div class="section" id="autothrottle-max-delay">
<span id="std:setting-AUTOTHROTTLE_MAX_DELAY"></span><h5>AUTOTHROTTLE_MAX_DELAY<a class="headerlink" href="#autothrottle-max-delay" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">60.0</span></tt></p>
<p>在高延迟情况下最大的下载延迟(单位秒)。</p>
</div>
<div class="section" id="autothrottle-debug">
<span id="std:setting-AUTOTHROTTLE_DEBUG"></span><h5>AUTOTHROTTLE_DEBUG<a class="headerlink" href="#autothrottle-debug" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>起用AutoThrottle调试(debug)模式，展示每个接收到的response。
您可以通过此来查看限速参数是如何实时被调整的。</p>
</div>
</div>
</div>
<span id="document-topics/benchmarking"></span><div class="section" id="benchmarking">
<span id="id1"></span><h3>Benchmarking<a class="headerlink" href="#benchmarking" title="永久链接至标题">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">0.17 新版功能.</span></p>
</div>
<p>Scrapy提供了一个简单的性能测试工具。其创建了一个本地HTTP服务器，并以最大可能的速度进行爬取。
该测试性能工具目的是测试Scrapy在您的硬件上的效率，来获得一个基本的底线用于对比。
其使用了一个简单的spider，仅跟进链接，不做任何处理。</p>
<p>运行:</p>
<div class="highlight-none"><div class="highlight"><pre>scrapy bench
</pre></div>
</div>
<p>您能看到类似的输出:</p>
<div class="highlight-none"><div class="highlight"><pre>2013-05-16 13:08:46-0300 [scrapy] INFO: Scrapy 0.17.0 started (bot: scrapybot)
2013-05-16 13:08:47-0300 [follow] INFO: Spider opened
2013-05-16 13:08:47-0300 [follow] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:48-0300 [follow] INFO: Crawled 74 pages (at 4440 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:49-0300 [follow] INFO: Crawled 143 pages (at 4140 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:50-0300 [follow] INFO: Crawled 210 pages (at 4020 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:51-0300 [follow] INFO: Crawled 274 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:52-0300 [follow] INFO: Crawled 343 pages (at 4140 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:53-0300 [follow] INFO: Crawled 410 pages (at 4020 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:54-0300 [follow] INFO: Crawled 474 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:55-0300 [follow] INFO: Crawled 538 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:56-0300 [follow] INFO: Crawled 602 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:57-0300 [follow] INFO: Closing spider (closespider_timeout)
2013-05-16 13:08:57-0300 [follow] INFO: Crawled 666 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:57-0300 [follow] INFO: Dumping Scrapy stats:
    {&#39;downloader/request_bytes&#39;: 231508,
     &#39;downloader/request_count&#39;: 682,
     &#39;downloader/request_method_count/GET&#39;: 682,
     &#39;downloader/response_bytes&#39;: 1172802,
     &#39;downloader/response_count&#39;: 682,
     &#39;downloader/response_status_count/200&#39;: 682,
     &#39;finish_reason&#39;: &#39;closespider_timeout&#39;,
     &#39;finish_time&#39;: datetime.datetime(2013, 5, 16, 16, 8, 57, 985539),
     &#39;log_count/INFO&#39;: 14,
     &#39;request_depth_max&#39;: 34,
     &#39;response_received_count&#39;: 682,
     &#39;scheduler/dequeued&#39;: 682,
     &#39;scheduler/dequeued/memory&#39;: 682,
     &#39;scheduler/enqueued&#39;: 12767,
     &#39;scheduler/enqueued/memory&#39;: 12767,
     &#39;start_time&#39;: datetime.datetime(2013, 5, 16, 16, 8, 47, 676539)}
2013-05-16 13:08:57-0300 [follow] INFO: Spider closed (closespider_timeout)
</pre></div>
</div>
<p>这说明了您的Scrapy能以3900页面/分钟的速度爬取。注意，这是一个非常简单，仅跟进链接的spider。
任何您所编写的spider会做更多处理，从而减慢爬取的速度。
减慢的程度取决于spider做的处理以及其是如何被编写的。</p>
<p>未来会有更多的用例会被加入到性能测试套装中，以覆盖更多常见的情景。</p>
</div>
<span id="document-topics/jobs"></span><div class="section" id="jobs">
<span id="topics-jobs"></span><h3>Jobs: 暂停，恢复爬虫<a class="headerlink" href="#jobs" title="永久链接至标题">¶</a></h3>
<p>有些情况下，例如爬取大的站点，我们希望能暂停爬取，之后再恢复运行。</p>
<p>Scrapy通过如下工具支持这个功能:</p>
<ul class="simple">
<li>一个把调度请求保存在磁盘的调度器</li>
<li>一个把访问请求保存在磁盘的副本过滤器[duplicates filter]</li>
<li>一个能持续保持爬虫状态(键/值对)的扩展</li>
</ul>
<div class="section" id="job">
<h4>Job 路径<a class="headerlink" href="#job" title="永久链接至标题">¶</a></h4>
<p>要启用持久化支持，你只需要通过 <tt class="docutils literal"><span class="pre">JOBDIR</span></tt> 设置 <em>job directory</em> 选项。这个路径将会存储
所有的请求数据来保持一个单独任务的状态(例如：一次spider爬取(a spider run))。必须要注意的是，这个目录不允许被不同的spider
共享，甚至是同一个spider的不同jobs/runs也不行。也就是说，这个目录就是存储一个 <em>单独</em> job的状态信息。</p>
</div>
<div class="section" id="id1">
<h4>怎么使用<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h4>
<p>要启用一个爬虫的持久化，运行以下命令:</p>
<div class="highlight-none"><div class="highlight"><pre>scrapy crawl somespider -s JOBDIR=crawls/somespider-1
</pre></div>
</div>
<p>然后，你就能在任何时候安全地停止爬虫(按Ctrl-C或者发送一个信号)。恢复这个爬虫也是同样的命令:</p>
<div class="highlight-none"><div class="highlight"><pre>scrapy crawl somespider -s JOBDIR=crawls/somespider-1
</pre></div>
</div>
</div>
<div class="section" id="id2">
<h4>保持状态<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h4>
<p>有的时候，你希望持续保持一些运行长时间的蜘蛛的状态。这时您可以使用 <tt class="docutils literal"><span class="pre">spider.state</span></tt> 属性,
该属性的类型必须是dict. scrapy提供了内置扩展负责在spider启动或结束时，从工作路径(job directory)中序列化、存储、加载属性。</p>
<p>下面这个例子展示了使用spider state的回调函数(callback)(简洁起见，省略了其他的代码):</p>
<div class="highlight-none"><div class="highlight"><pre>def parse_item(self, response):
    # parse item here
    self.state[&#39;items_count&#39;] = self.state.get(&#39;items_count&#39;, 0) + 1
</pre></div>
</div>
</div>
<div class="section" id="id3">
<h4>持久化的一些坑<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h4>
<p>如果你想要使用Scrapy的持久化支持,还有一些东西您需要了解:</p>
<div class="section" id="cookies">
<h5>Cookies的有效期<a class="headerlink" href="#cookies" title="永久链接至标题">¶</a></h5>
<p>Cookies是有有效期的(可能过期)。所以如果你没有把你的爬虫及时恢复，那么他可能在被调度回去的时候
就不能工作了。当然如果你的爬虫不依赖cookies就不会有这个问题了。</p>
</div>
<div class="section" id="id4">
<h5>请求序列化<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h5>
<p>请求是由 <cite>pickle</cite> 进行序列化的，所以你需要确保你的请求是可被pickle序列化的。
这里最常见的问题是在在request回调函数中使用 <tt class="docutils literal"><span class="pre">lambda</span></tt> 方法，导致无法序列化。</p>
<p>例如, 这样就会有问题:</p>
<div class="highlight-none"><div class="highlight"><pre>def some_callback(self, response):
    somearg = &#39;test&#39;
    return scrapy.Request(&#39;http://www.example.com&#39;, callback=lambda r: self.other_callback(r, somearg))

def other_callback(self, response, somearg):
    print &quot;the argument passed is:&quot;, somearg
</pre></div>
</div>
<p>这样才对:</p>
<div class="highlight-none"><div class="highlight"><pre>def some_callback(self, response):
    somearg = &#39;test&#39;
    return scrapy.Request(&#39;http://www.example.com&#39;, meta={&#39;somearg&#39;: somearg})

#这里的实例代码有错，应该是(译者注)
#   return scrapy.Request(&#39;http://www.example.com&#39;, meta={&#39;somearg&#39;: somearg}, callback=self.other_callback)

def other_callback(self, response):
    somearg = response.meta[&#39;somearg&#39;]
    print &quot;the argument passed is:&quot;, somearg
</pre></div>
</div>
</div>
</div>
</div>
<span id="document-topics/djangoitem"></span><div class="section" id="djangoitem">
<span id="topics-djangoitem"></span><h3>DjangoItem<a class="headerlink" href="#djangoitem" title="永久链接至标题">¶</a></h3>
<p><tt class="xref py py-class docutils literal"><span class="pre">DjangoItem</span></tt> 是一个item的类，其从Django模型中获取字段(field)定义。
您可以简单地创建一个 <tt class="xref py py-class docutils literal"><span class="pre">DjangoItem</span></tt> 并指定其关联的Django模型。</p>
<p>除了获得您item中定义的字段外， <tt class="xref py py-class docutils literal"><span class="pre">DjangoItem</span></tt>
提供了创建并获得一个具有item数据的Django模型实例(Django model instance)的方法。</p>
<div class="section" id="id1">
<h4>使用DjangoItem<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h4>
<p><tt class="xref py py-class docutils literal"><span class="pre">DjangoItem</span></tt> 使用方法与Django中的ModelForms类似。您创建一个子类，
并定义其 <tt class="docutils literal"><span class="pre">django_model</span></tt> 属性。这样，您就可以得到一个字段与Django模型字段(model field)一一对应的item了。</p>
<p>另外，您可以定义模型中没有的字段，甚至是覆盖模型中已经定义的字段。</p>
<p>让我们来看个例子:</p>
<p>创造一个Django模型:</p>
<div class="highlight-none"><div class="highlight"><pre>from django.db import models

class Person(models.Model):
    name = models.CharField(max_length=255)
    age = models.IntegerField()
</pre></div>
</div>
<p>定义一个基本的 <tt class="xref py py-class docutils literal"><span class="pre">DjangoItem</span></tt>:</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.contrib.djangoitem import DjangoItem

class PersonItem(DjangoItem):
    django_model = Person
</pre></div>
</div>
<p><tt class="xref py py-class docutils literal"><span class="pre">DjangoItem</span></tt> 的使用方法和 <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> 类似:</p>
<div class="highlight-none"><div class="highlight"><pre>&gt;&gt;&gt; p = PersonItem()
&gt;&gt;&gt; p[&#39;name&#39;] = &#39;John&#39;
&gt;&gt;&gt; p[&#39;age&#39;] = &#39;22&#39;
</pre></div>
</div>
<p>要从item中获取Django模型，调用 <tt class="xref py py-class docutils literal"><span class="pre">DjangoItem</span></tt> 中额外的方法 <tt class="xref py py-meth docutils literal"><span class="pre">save()</span></tt>:</p>
<div class="highlight-none"><div class="highlight"><pre>&gt;&gt;&gt; person = p.save()
&gt;&gt;&gt; person.name
&#39;John&#39;
&gt;&gt;&gt; person.age
&#39;22&#39;
&gt;&gt;&gt; person.id
1
</pre></div>
</div>
<p>当我们调用 <tt class="xref py py-meth docutils literal"><span class="pre">save()</span></tt> 时，模型已经保存了。我们可以在调用时带上 <tt class="docutils literal"><span class="pre">commit=False</span></tt> 来避免保存，
并获取到一个未保存的模型:</p>
<div class="highlight-none"><div class="highlight"><pre>&gt;&gt;&gt; person = p.save(commit=False)
&gt;&gt;&gt; person.name
&#39;John&#39;
&gt;&gt;&gt; person.age
&#39;22&#39;
&gt;&gt;&gt; person.id
None
</pre></div>
</div>
<p>正如之前所说的，我们可以在item中加入字段:</p>
<div class="highlight-none"><div class="highlight"><pre>import scrapy
from scrapy.contrib.djangoitem import DjangoItem

class PersonItem(DjangoItem):
    django_model = Person
    sex = scrapy.Field()
</pre></div>
</div>
<div class="highlight-none"><div class="highlight"><pre>&gt;&gt;&gt; p = PersonItem()
&gt;&gt;&gt; p[&#39;name&#39;] = &#39;John&#39;
&gt;&gt;&gt; p[&#39;age&#39;] = &#39;22&#39;
&gt;&gt;&gt; p[&#39;sex&#39;] = &#39;M&#39;
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">当执行 <tt class="xref py py-meth docutils literal"><span class="pre">save()</span></tt> 时添加到item的字段不会有作用(taken into account)。</p>
</div>
<p>并且我们可以覆盖模型中的字段:</p>
<div class="highlight-none"><div class="highlight"><pre>class PersonItem(DjangoItem):
    django_model = Person
    name = scrapy.Field(default=&#39;No Name&#39;)
</pre></div>
</div>
<p>这在提供字段属性时十分有用，例如您项目中使用的默认或者其他属性一样。</p>
</div>
<div class="section" id="id2">
<h4>DjangoItem注意事项<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h4>
<p>DjangoItem提供了在Scrapy项目中集成DjangoItem的简便方法，不过需要注意的是，
如果在Scrapy中爬取大量(百万级)的item时，Django ORM扩展得并不是很好(not scale well)。
这是因为关系型后端对于一个密集型(intensive)应用(例如web爬虫)并不是一个很好的选择，
尤其是具有大量的索引的数据库。</p>
</div>
<div class="section" id="django">
<h4>配置Django的设置<a class="headerlink" href="#django" title="永久链接至标题">¶</a></h4>
<p>在Django应用之外使用Django模型(model)，您需要设置
<tt class="docutils literal"><span class="pre">DJANGO_SETTINGS_MODULE</span></tt> 环境变量以及 &#8211;大多数情况下&#8211; 修改
<tt class="docutils literal"><span class="pre">PYTHONPATH</span></tt> 环境变量来导入设置模块。</p>
<p>完成这个配置有很多方法，具体选择取决您的情况及偏好。
下面详细给出了完成这个配置的最简单方法。</p>
<p>假设您项目的名称为 <tt class="docutils literal"><span class="pre">mysite</span></tt> ，位于
<tt class="docutils literal"><span class="pre">/home/projects/mysite</span></tt> 且用 <tt class="docutils literal"><span class="pre">Person</span></tt> 模型创建了一个应用 <tt class="docutils literal"><span class="pre">myapp</span></tt> 。
这意味着您的目录结构类似于:</p>
<div class="highlight-none"><div class="highlight"><pre>/home/projects/mysite
├── manage.py
├── myapp
│   ├── __init__.py
│   ├── models.py
│   ├── tests.py
│   └── views.py
└── mysite
    ├── __init__.py
    ├── settings.py
    ├── urls.py
    └── wsgi.py
</pre></div>
</div>
<p>接着您需要将 <tt class="docutils literal"><span class="pre">/home/projects/mysite</span></tt> 加入到 <tt class="docutils literal"><span class="pre">PYTHONPATH</span></tt>
环境变量中并将 <tt class="docutils literal"><span class="pre">mysite.settings</span></tt> 设置为 <tt class="docutils literal"><span class="pre">DJANGO_SETTINGS_MODULE</span></tt> 环境变量。
这可以在Scrapy设置文件中添加下列代码:</p>
<div class="highlight-none"><div class="highlight"><pre>import sys
sys.path.append(&#39;/home/projects/mysite&#39;)

import os
os.environ[&#39;DJANGO_SETTINGS_MODULE&#39;] = &#39;mysite.settings&#39;
</pre></div>
</div>
<p>注意，由于我们在python运行环境中，所以我们修改 <tt class="docutils literal"><span class="pre">sys.path</span></tt> 变量而不是 <tt class="docutils literal"><span class="pre">PYTHONPATH</span></tt> 环境变量。
如果所有设置正确，您应该可以运行 <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">shell</span></tt> 命令并且导入 <tt class="docutils literal"><span class="pre">Person</span></tt> 模型(例如 <tt class="docutils literal"><span class="pre">from</span> <span class="pre">myapp.models</span> <span class="pre">import</span> <span class="pre">Person</span></tt>)。</p>
</div>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-faq"><em>常见问题(FAQ)</em></a></dt>
<dd>常见问题的解决办法。</dd>
<dt><a class="reference internal" href="index.html#document-topics/debug"><em>调试(Debugging)Spiders</em></a></dt>
<dd>学习如何对scrapy spider的常见问题进行debug。</dd>
<dt><a class="reference internal" href="index.html#document-topics/contracts"><em>Spiders Contracts</em></a></dt>
<dd>学习如何使用contract来测试您的spider。</dd>
<dt><a class="reference internal" href="index.html#document-topics/practices"><em>实践经验(Common Practices)</em></a></dt>
<dd>熟悉Scrapy的一些惯例做法。</dd>
<dt><a class="reference internal" href="index.html#document-topics/broad-crawls"><em>通用爬虫(Broad Crawls)</em></a></dt>
<dd>调整Scrapy来适应并发爬取大量网站(a lot of domains)。</dd>
<dt><a class="reference internal" href="index.html#document-topics/firefox"><em>借助Firefox来爬取</em></a></dt>
<dd>了解如何使用Firefox及其他有用的插件来爬取数据。</dd>
<dt><a class="reference internal" href="index.html#document-topics/firebug"><em>使用Firebug进行爬取</em></a></dt>
<dd>了解如何使用Firebug来爬取数据。</dd>
<dt><a class="reference internal" href="index.html#document-topics/leaks"><em>调试内存溢出</em></a></dt>
<dd>了解如何查找并让您的爬虫避免内存泄露。</dd>
<dt><a class="reference internal" href="index.html#document-topics/images"><em>下载项目图片</em></a></dt>
<dd>下载爬取的item中的图片。</dd>
<dt><a class="reference internal" href="index.html#document-topics/ubuntu"><em>Ubuntu 软件包</em></a></dt>
<dd>在Ubuntu下下载最新的Scrapy。</dd>
<dt><a class="reference internal" href="index.html#document-topics/scrapyd"><em>Scrapyd</em></a></dt>
<dd>在生产环境中部署您的Scrapy项目。</dd>
<dt><a class="reference internal" href="index.html#document-topics/autothrottle"><em>自动限速(AutoThrottle)扩展</em></a></dt>
<dd>根据负载(load)动态调节爬取速度。</dd>
<dt><a class="reference internal" href="index.html#document-topics/benchmarking"><em>Benchmarking</em></a></dt>
<dd>在您的硬件平台上测试Scrapy的性能。</dd>
<dt><a class="reference internal" href="index.html#document-topics/jobs"><em>Jobs: 暂停，恢复爬虫</em></a></dt>
<dd>学习如何停止和恢复爬虫</dd>
<dt><a class="reference internal" href="index.html#document-topics/djangoitem"><em>DjangoItem</em></a></dt>
<dd>使用Django模型编写爬取的item</dd>
</dl>
</div>
<div class="section" id="scrapy">
<span id="extending-scrapy"></span><h2>扩展Scrapy<a class="headerlink" href="#scrapy" title="永久链接至标题">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-topics/architecture"></span><div class="section" id="topics-architecture">
<span id="id1"></span><h3>架构概览<a class="headerlink" href="#topics-architecture" title="永久链接至标题">¶</a></h3>
<p>本文档介绍了Scrapy架构及其组件之间的交互。</p>
<div class="section" id="id2">
<h4>概述<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h4>
<p>接下来的图表展现了Scrapy的架构，包括组件及在系统中发生的数据流的概览(绿色箭头所示)。
下面对每个组件都做了简单介绍，并给出了详细内容的链接。数据流如下所描述。</p>
<a class="reference internal image-reference" href="_images/scrapy_architecture.png"><img alt="Scrapy architecture" src="_images/scrapy_architecture.png" style="width: 700px; height: 494px;" /></a>
</div>
<div class="section" id="id3">
<h4>组件<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h4>
<div class="section" id="scrapy-engine">
<h5>Scrapy Engine<a class="headerlink" href="#scrapy-engine" title="永久链接至标题">¶</a></h5>
<p>引擎负责控制数据流在系统中所有组件中流动，并在相应动作发生时触发事件。
详细内容查看下面的数据流(Data Flow)部分。</p>
</div>
<div class="section" id="scheduler">
<h5>调度器(Scheduler)<a class="headerlink" href="#scheduler" title="永久链接至标题">¶</a></h5>
<p>调度器从引擎接受request并将他们入队，以便之后引擎请求他们时提供给引擎。</p>
</div>
<div class="section" id="downloader">
<h5>下载器(Downloader)<a class="headerlink" href="#downloader" title="永久链接至标题">¶</a></h5>
<p>下载器负责获取页面数据并提供给引擎，而后提供给spider。</p>
</div>
<div class="section" id="spiders">
<h5>Spiders<a class="headerlink" href="#spiders" title="永久链接至标题">¶</a></h5>
<p>Spider是Scrapy用户编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。
每个spider负责处理一个特定(或一些)网站。
更多内容请看 <a class="reference internal" href="index.html#topics-spiders"><em>Spiders</em></a> 。</p>
</div>
<div class="section" id="item-pipeline">
<h5>Item Pipeline<a class="headerlink" href="#item-pipeline" title="永久链接至标题">¶</a></h5>
<p>Item Pipeline负责处理被spider提取出来的item。典型的处理有清理、
验证及持久化(例如存取到数据库中)。
更多内容查看 <a class="reference internal" href="index.html#topics-item-pipeline"><em>Item Pipeline</em></a> 。</p>
</div>
<div class="section" id="downloader-middlewares">
<h5>下载器中间件(Downloader middlewares)<a class="headerlink" href="#downloader-middlewares" title="永久链接至标题">¶</a></h5>
<p>下载器中间件是在引擎及下载器之间的特定钩子(specific hook)，处理Downloader传递给引擎的response。
其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。更多内容请看 <a class="reference internal" href="index.html#topics-downloader-middleware"><em>下载器中间件(Downloader Middleware)</em></a> 。</p>
</div>
<div class="section" id="spider-spider-middlewares">
<h5>Spider中间件(Spider middlewares)<a class="headerlink" href="#spider-spider-middlewares" title="永久链接至标题">¶</a></h5>
<p>Spider中间件是在引擎及Spider之间的特定钩子(specific hook)，处理spider的输入(response)和输出(items及requests)。
其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。更多内容请看 <a class="reference internal" href="index.html#topics-spider-middleware"><em>Spider中间件(Middleware)</em></a> 。</p>
</div>
</div>
<div class="section" id="data-flow">
<h4>数据流(Data flow)<a class="headerlink" href="#data-flow" title="永久链接至标题">¶</a></h4>
<p>Scrapy中的数据流由执行引擎控制，其过程如下:</p>
<ol class="arabic simple">
<li>引擎打开一个网站(open a domain)，找到处理该网站的Spider并向该spider请求第一个要爬取的URL(s)。</li>
<li>引擎从Spider中获取到第一个要爬取的URL并在调度器(Scheduler)以Request调度。</li>
<li>引擎向调度器请求下一个要爬取的URL。</li>
<li>调度器返回下一个要爬取的URL给引擎，引擎将URL通过下载中间件(请求(request)方向)转发给下载器(Downloader)。</li>
<li>一旦页面下载完毕，下载器生成一个该页面的Response，并将其通过下载中间件(返回(response)方向)发送给引擎。</li>
<li>引擎从下载器中接收到Response并通过Spider中间件(输入方向)发送给Spider处理。</li>
<li>Spider处理Response并返回爬取到的Item及(跟进的)新的Request给引擎。</li>
<li>引擎将(Spider返回的)爬取到的Item给Item Pipeline，将(Spider返回的)Request给调度器。</li>
<li>(从第二步)重复直到调度器中没有更多地request，引擎关闭该网站。</li>
</ol>
</div>
<div class="section" id="event-driven-networking">
<h4>事件驱动网络(Event-driven networking)<a class="headerlink" href="#event-driven-networking" title="永久链接至标题">¶</a></h4>
<p>Scrapy基于事件驱动网络框架 <a class="reference external" href="http://twistedmatrix.com/trac/">Twisted</a> 编写。因此，Scrapy基于并发性考虑由非阻塞(即异步)的实现。</p>
<p>关于异步编程及Twisted更多的内容请查看下列链接:</p>
<ul class="simple">
<li><a class="reference external" href="http://twistedmatrix.com/documents/current/core/howto/defer-intro.html">Introduction to Deferreds in Twisted</a></li>
<li><a class="reference external" href="http://jessenoller.com/2009/02/11/twisted-hello-asynchronous-programming/">Twisted - hello, asynchronous programming</a></li>
</ul>
</div>
</div>
<span id="document-topics/downloader-middleware"></span><div class="section" id="downloader-middleware">
<span id="topics-downloader-middleware"></span><h3>下载器中间件(Downloader Middleware)<a class="headerlink" href="#downloader-middleware" title="永久链接至标题">¶</a></h3>
<p>下载器中间件是介于Scrapy的request/response处理的钩子框架。
是用于全局修改Scrapy request和response的一个轻量、底层的系统。</p>
<div class="section" id="topics-downloader-middleware-setting">
<span id="id1"></span><h4>激活下载器中间件<a class="headerlink" href="#topics-downloader-middleware-setting" title="永久链接至标题">¶</a></h4>
<p>要激活下载器中间件组件，将其加入到 <a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></tt></a> 设置中。
该设置是一个字典(dict)，键为中间件类的路径，值为其中间件的顺序(order)。</p>
<p>这里是一个例子:</p>
<div class="highlight-none"><div class="highlight"><pre>DOWNLOADER_MIDDLEWARES = {
    &#39;myproject.middlewares.CustomDownloaderMiddleware&#39;: 543,
}
</pre></div>
</div>
<p><a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></tt></a> 设置会与Scrapy定义的
<a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></tt></a> 设置合并(但不是覆盖)，
而后根据顺序(order)进行排序，最后得到启用中间件的有序列表:
第一个中间件是最靠近引擎的，最后一个中间件是最靠近下载器的。</p>
<p>关于如何分配中间件的顺序请查看
<a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></tt></a> 设置，而后根据您想要放置中间件的位置选择一个值。
由于每个中间件执行不同的动作，您的中间件可能会依赖于之前(或者之后)执行的中间件，因此顺序是很重要的。</p>
<p>如果您想禁止内置的(在
<a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></tt></a> 中设置并默认启用的)中间件，
您必须在项目的 <a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></tt></a> 设置中定义该中间件，并将其值赋为 <cite>None</cite> 。
例如，如果您想要关闭user-agent中间件:</p>
<div class="highlight-none"><div class="highlight"><pre>DOWNLOADER_MIDDLEWARES = {
    &#39;myproject.middlewares.CustomDownloaderMiddleware&#39;: 543,
    &#39;scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware&#39;: None,
}
</pre></div>
</div>
<p>最后，请注意，有些中间件需要通过特定的设置来启用。更多内容请查看相关中间件文档。</p>
</div>
<div class="section" id="id2">
<h4>编写您自己的下载器中间件<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h4>
<p>编写下载器中间件十分简单。每个中间件组件是一个定义了以下一个或多个方法的Python类:</p>
<span class="target" id="module-scrapy.contrib.downloadermiddleware"></span><dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.DownloaderMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.</tt><tt class="descname">DownloaderMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware" title="永久链接至目标">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request">
<tt class="descname">process_request</tt><big>(</big><em>request</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request" title="永久链接至目标">¶</a></dt>
<dd><p>当每个request通过下载中间件时，该方法被调用。</p>
<p><a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request"><tt class="xref py py-meth docutils literal"><span class="pre">process_request()</span></tt></a> 必须返回其中之一: 返回 <tt class="docutils literal"><span class="pre">None</span></tt> 、返回一个
<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> 对象、返回一个 <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a>
对象或raise <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><tt class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></tt></a> 。</p>
<p>如果其返回 <tt class="docutils literal"><span class="pre">None</span></tt> ，Scrapy将继续处理该request，执行其他的中间件的相应方法，直到合适的下载器处理函数(download handler)被调用，
该request被执行(其response被下载)。</p>
<p>如果其返回 <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> 对象，Scrapy将不会调用 <em>任何</em>
其他的 <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request"><tt class="xref py py-meth docutils literal"><span class="pre">process_request()</span></tt></a> 或 <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></tt></a> 方法，或相应地下载函数；
其将返回该response。 已安装的中间件的 <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response"><tt class="xref py py-meth docutils literal"><span class="pre">process_response()</span></tt></a> 方法则会在每个response返回时被调用。</p>
<p>如果其返回 <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象，Scrapy则停止调用
process_request方法并重新调度返回的request。当新返回的request被执行后，
相应地中间件链将会根据下载的response被调用。</p>
<p>如果其raise一个 <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><tt class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></tt></a> 异常，则安装的下载中间件的
<a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></tt></a> 方法会被调用。如果没有任何一个方法处理该异常，
则request的errback(<tt class="docutils literal"><span class="pre">Request.errback</span></tt>)方法会被调用。如果没有代码处理抛出的异常，
则该异常被忽略且不记录(不同于其他异常那样)。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象) &#8211; 处理的request</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> 对象) &#8211; 该request对应的spider</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response">
<tt class="descname">process_response</tt><big>(</big><em>request</em>, <em>response</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response" title="永久链接至目标">¶</a></dt>
<dd><p><a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request"><tt class="xref py py-meth docutils literal"><span class="pre">process_request()</span></tt></a> 必须返回以下之一: 返回一个 <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> 对象、
返回一个 <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象或raise一个 <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><tt class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></tt></a> 异常。</p>
<p>如果其返回一个 <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> (可以与传入的response相同，也可以是全新的对象)，
该response会被在链中的其他中间件的 <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response"><tt class="xref py py-meth docutils literal"><span class="pre">process_response()</span></tt></a> 方法处理。</p>
<p>如果其返回一个 <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象，则中间件链停止，
返回的request会被重新调度下载。处理类似于 <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request"><tt class="xref py py-meth docutils literal"><span class="pre">process_request()</span></tt></a> 返回request所做的那样。</p>
<p>如果其抛出一个 <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><tt class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></tt></a> 异常，则调用request的errback(<tt class="docutils literal"><span class="pre">Request.errback</span></tt>)。
如果没有代码处理抛出的异常，则该异常被忽略且不记录(不同于其他异常那样)。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象) &#8211; response所对应的request</li>
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> 对象) &#8211; 被处理的response</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> 对象) &#8211; response所对应的spider</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception">
<tt class="descname">process_exception</tt><big>(</big><em>request</em>, <em>exception</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="永久链接至目标">¶</a></dt>
<dd><p>当下载处理器(download handler)或 <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request"><tt class="xref py py-meth docutils literal"><span class="pre">process_request()</span></tt></a>
(下载中间件)抛出异常(包括 <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><tt class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></tt></a> 异常)时，
Scrapy调用 <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></tt></a> 。</p>
<p><a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></tt></a> 应该返回以下之一: 返回 <tt class="docutils literal"><span class="pre">None</span></tt> 、
一个 <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> 对象、或者一个 <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象。</p>
<p>如果其返回 <tt class="docutils literal"><span class="pre">None</span></tt> ，Scrapy将会继续处理该异常，接着调用已安装的其他中间件的
<a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></tt></a> 方法，直到所有中间件都被调用完毕，则调用默认的异常处理。</p>
<p>如果其返回一个 <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> 对象，则已安装的中间件链的
<a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response"><tt class="xref py py-meth docutils literal"><span class="pre">process_response()</span></tt></a> 方法被调用。Scrapy将不会调用任何其他中间件的
<a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></tt></a> 方法。</p>
<p>如果其返回一个 <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象，
则返回的request将会被重新调用下载。这将停止中间件的
<a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></tt></a> 方法执行，就如返回一个response的那样。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (是 <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象) &#8211; 产生异常的request</li>
<li><strong>exception</strong> (<tt class="docutils literal"><span class="pre">Exception</span></tt> 对象) &#8211; 抛出的异常</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> 对象) &#8211; request对应的spider</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="topics-downloader-middleware-ref">
<span id="id3"></span><h4>内置下载中间件参考手册<a class="headerlink" href="#topics-downloader-middleware-ref" title="永久链接至标题">¶</a></h4>
<p>本页面介绍了Scrapy自带的所有下载中间件。关于如何使用及编写您自己的中间件，请参考
<a class="reference internal" href="index.html#topics-downloader-middleware"><em>downloader middleware usage guide</em></a>.</p>
<p>关于默认启用的中间件列表(及其顺序)请参考
<a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></tt></a> 设置。</p>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.cookies">
<span id="cookiesmiddleware"></span><span id="cookies-mw"></span><h5>CookiesMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.cookies" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.cookies.</tt><tt class="descname">CookiesMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware" title="永久链接至目标">¶</a></dt>
<dd><p>该中间件使得爬取需要cookie(例如使用session)的网站成为了可能。
其追踪了web server发送的cookie，并在之后的request中发送回去，
就如浏览器所做的那样。</p>
</dd></dl>

<p>以下设置可以用来配置cookie中间件:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-COOKIES_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">COOKIES_ENABLED</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-COOKIES_DEBUG"><tt class="xref std std-setting docutils literal"><span class="pre">COOKIES_DEBUG</span></tt></a></li>
</ul>
<div class="section" id="spidercookie-session">
<span id="std:reqmeta-cookiejar"></span><h6>单spider多cookie session<a class="headerlink" href="#spidercookie-session" title="永久链接至标题">¶</a></h6>
<div class="versionadded">
<p><span class="versionmodified">0.15 新版功能.</span></p>
</div>
<p>Scrapy通过使用 <a class="reference internal" href="index.html#std:reqmeta-cookiejar"><tt class="xref std std-reqmeta docutils literal"><span class="pre">cookiejar</span></tt></a> Request meta key来支持单spider追踪多cookie session。
默认情况下其使用一个cookie jar(session)，不过您可以传递一个标示符来使用多个。</p>
<p>例如:</p>
<div class="highlight-none"><div class="highlight"><pre>for i, url in enumerate(urls):
    yield scrapy.Request(&quot;http://www.example.com&quot;, meta={&#39;cookiejar&#39;: i},
        callback=self.parse_page)
</pre></div>
</div>
<p>需要注意的是 <a class="reference internal" href="index.html#std:reqmeta-cookiejar"><tt class="xref std std-reqmeta docutils literal"><span class="pre">cookiejar</span></tt></a> meta key不是&#8221;黏性的(sticky)&#8221;。
您需要在之后的request请求中接着传递。例如:</p>
<div class="highlight-none"><div class="highlight"><pre>def parse_page(self, response):
    # do some processing
    return scrapy.Request(&quot;http://www.example.com/otherpage&quot;,
        meta={&#39;cookiejar&#39;: response.meta[&#39;cookiejar&#39;]},
        callback=self.parse_other_page)
</pre></div>
</div>
</div>
<div class="section" id="cookies-enabled">
<span id="std:setting-COOKIES_ENABLED"></span><h6>COOKIES_ENABLED<a class="headerlink" href="#cookies-enabled" title="永久链接至标题">¶</a></h6>
<p>默认: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>是否启用cookies middleware。如果关闭，cookies将不会发送给web server。</p>
</div>
<div class="section" id="cookies-debug">
<span id="std:setting-COOKIES_DEBUG"></span><h6>COOKIES_DEBUG<a class="headerlink" href="#cookies-debug" title="永久链接至标题">¶</a></h6>
<p>默认: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>如果启用，Scrapy将记录所有在request(<tt class="docutils literal"><span class="pre">Cookie</span></tt>
请求头)发送的cookies及response接收到的cookies(<tt class="docutils literal"><span class="pre">Set-Cookie</span></tt> 接收头)。</p>
<p>下边是启用 <a class="reference internal" href="index.html#std:setting-COOKIES_DEBUG"><tt class="xref std std-setting docutils literal"><span class="pre">COOKIES_DEBUG</span></tt></a> 的记录的样例:</p>
<div class="highlight-none"><div class="highlight"><pre>2011-04-06 14:35:10-0300 [diningcity] INFO: Spider opened
2011-04-06 14:35:10-0300 [diningcity] DEBUG: Sending cookies to: &lt;GET http://www.diningcity.com/netherlands/index.html&gt;
        Cookie: clientlanguage_nl=en_EN
2011-04-06 14:35:14-0300 [diningcity] DEBUG: Received cookies from: &lt;200 http://www.diningcity.com/netherlands/index.html&gt;
        Set-Cookie: JSESSIONID=B~FA4DC0C496C8762AE4F1A620EAB34F38; Path=/
        Set-Cookie: ip_isocode=US
        Set-Cookie: clientlanguage_nl=en_EN; Expires=Thu, 07-Apr-2011 21:21:34 GMT; Path=/
2011-04-06 14:49:50-0300 [diningcity] DEBUG: Crawled (200) &lt;GET http://www.diningcity.com/netherlands/index.html&gt; (referer: None)
[...]
</pre></div>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.defaultheaders">
<span id="defaultheadersmiddleware"></span><h5>DefaultHeadersMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.defaultheaders" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.defaultheaders.</tt><tt class="descname">DefaultHeadersMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware" title="永久链接至目标">¶</a></dt>
<dd><p>该中间件设置
<a class="reference internal" href="index.html#std:setting-DEFAULT_REQUEST_HEADERS"><tt class="xref std std-setting docutils literal"><span class="pre">DEFAULT_REQUEST_HEADERS</span></tt></a> 指定的默认request header。</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.downloadtimeout">
<span id="downloadtimeoutmiddleware"></span><h5>DownloadTimeoutMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.downloadtimeout" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.downloadtimeout.</tt><tt class="descname">DownloadTimeoutMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware" title="永久链接至目标">¶</a></dt>
<dd><p>该中间件设置
<a class="reference internal" href="index.html#std:setting-DOWNLOAD_TIMEOUT"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_TIMEOUT</span></tt></a> 指定的request下载超时时间.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.httpauth">
<span id="httpauthmiddleware"></span><h5>HttpAuthMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.httpauth" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.httpauth.</tt><tt class="descname">HttpAuthMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware" title="永久链接至目标">¶</a></dt>
<dd><p>该中间件完成某些使用 <a class="reference external" href="http://en.wikipedia.org/wiki/Basic_access_authentication">Basic access authentication</a> (或者叫HTTP认证)的spider生成的请求的认证过程。</p>
<p>在spider中启用HTTP认证，请设置spider的 <tt class="docutils literal"><span class="pre">http_user</span></tt> 及 <tt class="docutils literal"><span class="pre">http_pass</span></tt> 属性。</p>
<p>样例:</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.contrib.spiders import CrawlSpider

class SomeIntranetSiteSpider(CrawlSpider):

    http_user = &#39;someuser&#39;
    http_pass = &#39;somepass&#39;
    name = &#39;intranet.example.com&#39;

    # .. rest of the spider code omitted ...
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.httpcache">
<span id="httpcachemiddleware"></span><h5>HttpCacheMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.httpcache" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.httpcache.</tt><tt class="descname">HttpCacheMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware" title="永久链接至目标">¶</a></dt>
<dd><p>该中间件为所有HTTP request及response提供了底层(low-level)缓存支持。
其由cache存储后端及cache策略组成。</p>
<p>Scrapy提供了两种HTTP缓存存储后端:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#httpcache-storage-fs"><em>Filesystem storage backend (默认值)</em></a></li>
<li><a class="reference internal" href="index.html#httpcache-storage-dbm"><em>DBM storage backend</em></a></li>
</ul>
</div></blockquote>
<p>您可以使用 <a class="reference internal" href="index.html#std:setting-HTTPCACHE_STORAGE"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_STORAGE</span></tt></a> 设定来修改HTTP缓存存储后端。
您也可以实现您自己的存储后端。</p>
<p>Scrapy提供了两种了缓存策略:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#httpcache-policy-rfc2616"><em>RFC2616策略</em></a></li>
<li><a class="reference internal" href="index.html#httpcache-policy-dummy"><em>Dummy策略(默认值)</em></a></li>
</ul>
</div></blockquote>
<p>您可以使用 <a class="reference internal" href="index.html#std:setting-HTTPCACHE_POLICY"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_POLICY</span></tt></a> 设定来修改HTTP缓存存储后端。
您也可以实现您自己的存储策略。</p>
</dd></dl>

<div class="section" id="dummy">
<span id="httpcache-policy-dummy"></span><h6>Dummy策略(默认值)<a class="headerlink" href="#dummy" title="永久链接至标题">¶</a></h6>
<p>该策略不考虑任何HTTP Cache-Control指令。每个request及其对应的response都被缓存。
当相同的request发生时，其不发送任何数据，直接返回response。</p>
<p>Dummpy策略对于测试spider十分有用。其能使spider运行更快(不需要每次等待下载完成)，
同时在没有网络连接时也能测试。其目的是为了能够回放spider的运行过程， <em>使之与之前的运行过程一模一样</em> 。</p>
<p>使用这个策略请设置:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_POLICY"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_POLICY</span></tt></a> 为 <tt class="docutils literal"><span class="pre">scrapy.contrib.httpcache.DummyPolicy</span></tt></li>
</ul>
</div>
<div class="section" id="rfc2616">
<span id="httpcache-policy-rfc2616"></span><h6>RFC2616策略<a class="headerlink" href="#rfc2616" title="永久链接至标题">¶</a></h6>
<p>该策略提供了符合RFC2616的HTTP缓存，例如符合HTTP Cache-Control，
针对生产环境并且应用在持续性运行环境所设置。该策略能避免下载未修改的数据(来节省带宽，提高爬取速度)。</p>
<p>实现了:</p>
<ul class="simple">
<li>当 <cite>no-store</cite> cache-control指令设置时不存储response/request。</li>
<li>当 <cite>no-cache</cite> cache-control指定设置时不从cache中提取response，即使response为最新。</li>
<li>根据 <cite>max-age</cite> cache-control指令中计算保存时间(freshness lifetime)。</li>
<li>根据 <cite>Expires</cite> 指令来计算保存时间(freshness lifetime)。</li>
<li>根据response包头的 <cite>Last-Modified</cite> 指令来计算保存时间(freshness lifetime)(Firefox使用的启发式算法)。</li>
<li>根据response包头的 <cite>Age</cite> 计算当前年龄(current age)</li>
<li>根据 <cite>Date</cite> 计算当前年龄(current age)</li>
<li>根据response包头的 <cite>Last-Modified</cite> 验证老旧的response。</li>
<li>根据response包头的 <cite>ETag</cite> 验证老旧的response。</li>
<li>为接收到的response设置缺失的 <cite>Date</cite> 字段。</li>
</ul>
<p>目前仍然缺失:</p>
<ul class="simple">
<li><cite>Pragma: no-cache</cite> 支持 <a class="reference external" href="http://www.mnot.net/cache_docs/#PRAGMA">http://www.mnot.net/cache_docs/#PRAGMA</a></li>
<li><cite>Vary</cite> 字段支持 <a class="reference external" href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6">http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6</a></li>
<li>当update或delete之后失效相应的response <a class="reference external" href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10">http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10</a></li>
<li>... 以及其他可能缺失的特性 ..</li>
</ul>
<p>使用这个策略，设置:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_POLICY"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_POLICY</span></tt></a> 为 <tt class="docutils literal"><span class="pre">scrapy.contrib.httpcache.RFC2616Policy</span></tt></li>
</ul>
</div>
<div class="section" id="filesystem-storage-backend">
<span id="httpcache-storage-fs"></span><h6>Filesystem storage backend (默认值)<a class="headerlink" href="#filesystem-storage-backend" title="永久链接至标题">¶</a></h6>
<p>文件系统存储后端可以用于HTTP缓存中间件。</p>
<p>使用该存储端，设置:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_STORAGE"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_STORAGE</span></tt></a> 为 <tt class="docutils literal"><span class="pre">scrapy.contrib.httpcache.FilesystemCacheStorage</span></tt></li>
</ul>
<p>每个request/response组存储在不同的目录中，包含下列文件:</p>
<blockquote>
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">request_body</span></tt> - the plain request body</li>
<li><tt class="docutils literal"><span class="pre">request_headers</span></tt> - the request headers (原始HTTP格式)</li>
<li><tt class="docutils literal"><span class="pre">response_body</span></tt> - the plain response body</li>
<li><tt class="docutils literal"><span class="pre">response_headers</span></tt> - the request headers (原始HTTP格式)</li>
<li><tt class="docutils literal"><span class="pre">meta</span></tt> - 以Python <tt class="docutils literal"><span class="pre">repr()</span></tt> 格式(grep-friendly格式)存储的该缓存资源的一些元数据。</li>
<li><tt class="docutils literal"><span class="pre">pickled_meta</span></tt> - 与 <tt class="docutils literal"><span class="pre">meta</span></tt> 相同的元数据，不过使用pickle来获得更高效的反序列化性能。</li>
</ul>
</div></blockquote>
<p>目录的名称与request的指纹(参考
<tt class="docutils literal"><span class="pre">scrapy.utils.request.fingerprint</span></tt>)有关，而二级目录是为了避免在同一文件夹下有太多文件
(这在很多文件系统中是十分低效的)。目录的例子:</p>
<div class="highlight-none"><div class="highlight"><pre>/path/to/cache/dir/example.com/72/72811f648e718090f041317756c03adb0ada46c7
</pre></div>
</div>
</div>
<div class="section" id="dbm-storage-backend">
<span id="httpcache-storage-dbm"></span><h6>DBM storage backend<a class="headerlink" href="#dbm-storage-backend" title="永久链接至标题">¶</a></h6>
<div class="versionadded">
<p><span class="versionmodified">0.13 新版功能.</span></p>
</div>
<p>同时也有 <a class="reference external" href="http://en.wikipedia.org/wiki/Dbm">DBM</a> 存储后端可以用于HTTP缓存中间件。</p>
<p>默认情况下，其采用 <a class="reference external" href="http://docs.python.org/library/anydbm.html">anydbm</a> 模块，不过您也可以通过
<a class="reference internal" href="index.html#std:setting-HTTPCACHE_DBM_MODULE"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_DBM_MODULE</span></tt></a> 设置进行修改。</p>
<p>使用该存储端，设置:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_STORAGE"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_STORAGE</span></tt></a> 为 <tt class="docutils literal"><span class="pre">scrapy.contrib.httpcache.DbmCacheStorage</span></tt></li>
</ul>
</div>
<div class="section" id="leveldb-storage-backend">
<span id="httpcache-storage-leveldb"></span><h6>LevelDB storage backend<a class="headerlink" href="#leveldb-storage-backend" title="永久链接至标题">¶</a></h6>
<div class="versionadded">
<p><span class="versionmodified">0.23 新版功能.</span></p>
</div>
<p>A <a class="reference external" href="http://code.google.com/p/leveldb/">LevelDB</a> storage backend is also available for the HTTP cache middleware.</p>
<p>This backend is not recommended for development because only one process can
access LevelDB databases at the same time, so you can&#8217;t run a crawl and open
the scrapy shell in parallel for the same spider.</p>
<p>In order to use this storage backend:</p>
<ul class="simple">
<li>set <a class="reference internal" href="index.html#std:setting-HTTPCACHE_STORAGE"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_STORAGE</span></tt></a> to <tt class="docutils literal"><span class="pre">scrapy.contrib.httpcache.LeveldbCacheStorage</span></tt></li>
<li>install <a class="reference external" href="http://pypi.python.org/pypi/leveldb">LevelDB python bindings</a> like <tt class="docutils literal"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">leveldb</span></tt></li>
</ul>
</div>
<div class="section" id="httpcache">
<h6>HTTPCache中间件设置<a class="headerlink" href="#httpcache" title="永久链接至标题">¶</a></h6>
<p><a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware" title="scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">HttpCacheMiddleware</span></tt></a> 可以通过以下设置进行配置:</p>
<div class="section" id="httpcache-enabled">
<span id="std:setting-HTTPCACHE_ENABLED"></span><h7>HTTPCACHE_ENABLED<a class="headerlink" href="#httpcache-enabled" title="永久链接至标题">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">0.11 新版功能.</span></p>
</div>
<p>默认: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>HTTP缓存是否开启。</p>
<div class="versionchanged">
<p><span class="versionmodified">在 0.11 版更改: </span>在0.11版本前，是使用 <a class="reference internal" href="index.html#std:setting-HTTPCACHE_DIR"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_DIR</span></tt></a> 来开启缓存。</p>
</div>
</div>
<div class="section" id="httpcache-expiration-secs">
<span id="std:setting-HTTPCACHE_EXPIRATION_SECS"></span><h7>HTTPCACHE_EXPIRATION_SECS<a class="headerlink" href="#httpcache-expiration-secs" title="永久链接至标题">¶</a></h7>
<p>默认: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>缓存的request的超时时间，单位秒。</p>
<p>超过这个时间的缓存request将会被重新下载。如果为0，则缓存的request将永远不会超时。</p>
<div class="versionchanged">
<p><span class="versionmodified">在 0.11 版更改: </span>在0.11版本前，0的意义是缓存的request永远超时。</p>
</div>
</div>
<div class="section" id="httpcache-dir">
<span id="std:setting-HTTPCACHE_DIR"></span><h7>HTTPCACHE_DIR<a class="headerlink" href="#httpcache-dir" title="永久链接至标题">¶</a></h7>
<p>默认: <tt class="docutils literal"><span class="pre">'httpcache'</span></tt></p>
<p>存储(底层的)HTTP缓存的目录。如果为空，则HTTP缓存将会被关闭。
如果为相对目录，则相对于项目数据目录(project data dir)。更多内容请参考 <a class="reference internal" href="index.html#topics-project-structure"><em>默认的Scrapy项目结构</em></a> 。</p>
</div>
<div class="section" id="httpcache-ignore-http-codes">
<span id="std:setting-HTTPCACHE_IGNORE_HTTP_CODES"></span><h7>HTTPCACHE_IGNORE_HTTP_CODES<a class="headerlink" href="#httpcache-ignore-http-codes" title="永久链接至标题">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">0.10 新版功能.</span></p>
</div>
<p>默认: <tt class="docutils literal"><span class="pre">[]</span></tt></p>
<p>不缓存设置中的HTTP返回值(code)的request。</p>
</div>
<div class="section" id="httpcache-ignore-missing">
<span id="std:setting-HTTPCACHE_IGNORE_MISSING"></span><h7>HTTPCACHE_IGNORE_MISSING<a class="headerlink" href="#httpcache-ignore-missing" title="永久链接至标题">¶</a></h7>
<p>默认: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>如果启用，在缓存中没找到的request将会被忽略，不下载。</p>
</div>
<div class="section" id="httpcache-ignore-schemes">
<span id="std:setting-HTTPCACHE_IGNORE_SCHEMES"></span><h7>HTTPCACHE_IGNORE_SCHEMES<a class="headerlink" href="#httpcache-ignore-schemes" title="永久链接至标题">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">0.10 新版功能.</span></p>
</div>
<p>默认: <tt class="docutils literal"><span class="pre">['file']</span></tt></p>
<p>不缓存这些URI标准(scheme)的response。</p>
</div>
<div class="section" id="httpcache-storage">
<span id="std:setting-HTTPCACHE_STORAGE"></span><h7>HTTPCACHE_STORAGE<a class="headerlink" href="#httpcache-storage" title="永久链接至标题">¶</a></h7>
<p>默认: <tt class="docutils literal"><span class="pre">'scrapy.contrib.httpcache.FilesystemCacheStorage'</span></tt></p>
<p>实现缓存存储后端的类。</p>
</div>
<div class="section" id="httpcache-dbm-module">
<span id="std:setting-HTTPCACHE_DBM_MODULE"></span><h7>HTTPCACHE_DBM_MODULE<a class="headerlink" href="#httpcache-dbm-module" title="永久链接至标题">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">0.13 新版功能.</span></p>
</div>
<p>默认: <tt class="docutils literal"><span class="pre">'anydbm'</span></tt></p>
<p>在 <a class="reference internal" href="index.html#httpcache-storage-dbm"><em>DBM存储后端</em></a> 的数据库模块。
该设定针对DBM后端。</p>
</div>
<div class="section" id="httpcache-policy">
<span id="std:setting-HTTPCACHE_POLICY"></span><h7>HTTPCACHE_POLICY<a class="headerlink" href="#httpcache-policy" title="永久链接至标题">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">0.18 新版功能.</span></p>
</div>
<p>默认: <tt class="docutils literal"><span class="pre">'scrapy.contrib.httpcache.DummyPolicy'</span></tt></p>
<p>实现缓存策略的类。</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.httpcompression">
<span id="httpcompressionmiddleware"></span><h5>HttpCompressionMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.httpcompression" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.httpcompression.</tt><tt class="descname">HttpCompressionMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware" title="永久链接至目标">¶</a></dt>
<dd><p>该中间件提供了对压缩(gzip, deflate)数据的支持。</p>
</dd></dl>

<div class="section" id="httpcompressionmiddleware-settings">
<h6>HttpCompressionMiddleware Settings<a class="headerlink" href="#httpcompressionmiddleware-settings" title="永久链接至标题">¶</a></h6>
<div class="section" id="compression-enabled">
<span id="std:setting-COMPRESSION_ENABLED"></span><h7>COMPRESSION_ENABLED<a class="headerlink" href="#compression-enabled" title="永久链接至标题">¶</a></h7>
<p>默认: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>Compression Middleware(压缩中间件)是否开启。</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.chunked">
<span id="chunkedtransfermiddleware"></span><h5>ChunkedTransferMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.chunked" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.chunked.</tt><tt class="descname">ChunkedTransferMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware" title="永久链接至目标">¶</a></dt>
<dd><p>该中间件添加了对 <a class="reference external" href="http://en.wikipedia.org/wiki/Chunked_transfer_encoding">chunked transfer encoding</a> 的支持。</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.httpproxy">
<span id="httpproxymiddleware"></span><h5>HttpProxyMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.httpproxy" title="永久链接至标题">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">0.8 新版功能.</span></p>
</div>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.httpproxy.</tt><tt class="descname">HttpProxyMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware" title="永久链接至目标">¶</a></dt>
<dd><p>该中间件提供了对request设置HTTP代理的支持。您可以通过在
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象中设置 <tt class="docutils literal"><span class="pre">proxy</span></tt> 元数据来开启代理。</p>
<p>类似于Python标准库模块 <a class="reference external" href="http://docs.python.org/library/urllib.html">urllib</a> 及 <a class="reference external" href="http://docs.python.org/library/urllib2.html">urllib2</a> ，其使用了下列环境变量:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">http_proxy</span></tt></li>
<li><tt class="docutils literal"><span class="pre">https_proxy</span></tt></li>
<li><tt class="docutils literal"><span class="pre">no_proxy</span></tt></li>
</ul>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.redirect">
<span id="redirectmiddleware"></span><h5>RedirectMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.redirect" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.redirect.</tt><tt class="descname">RedirectMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware" title="永久链接至目标">¶</a></dt>
<dd><p>该中间件根据response的状态处理重定向的request。</p>
</dd></dl>

<p id="std:reqmeta-redirect_urls">通过该中间件的(被重定向的)request的url可以通过
<a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> 的 <tt class="docutils literal"><span class="pre">redirect_urls</span></tt> 键找到。</p>
<p><a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware" title="scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">RedirectMiddleware</span></tt></a> 可以通过下列设置进行配置(更多内容请参考设置文档):</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-REDIRECT_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">REDIRECT_ENABLED</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-REDIRECT_MAX_TIMES"><tt class="xref std std-setting docutils literal"><span class="pre">REDIRECT_MAX_TIMES</span></tt></a></li>
</ul>
<p id="std:reqmeta-dont_redirect">如果 <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> 包含
<tt class="docutils literal"><span class="pre">dont_redirect</span></tt> 键，则该request将会被此中间件忽略。</p>
<div class="section" id="redirectmiddleware-settings">
<h6>RedirectMiddleware settings<a class="headerlink" href="#redirectmiddleware-settings" title="永久链接至标题">¶</a></h6>
<div class="section" id="redirect-enabled">
<span id="std:setting-REDIRECT_ENABLED"></span><h7>REDIRECT_ENABLED<a class="headerlink" href="#redirect-enabled" title="永久链接至标题">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">0.13 新版功能.</span></p>
</div>
<p>默认: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>是否启用Redirect中间件。</p>
</div>
<div class="section" id="redirect-max-times">
<span id="std:setting-REDIRECT_MAX_TIMES"></span><h7>REDIRECT_MAX_TIMES<a class="headerlink" href="#redirect-max-times" title="永久链接至标题">¶</a></h7>
<p>默认: <tt class="docutils literal"><span class="pre">20</span></tt></p>
<p>单个request被重定向的最大次数。</p>
</div>
</div>
</div>
<div class="section" id="metarefreshmiddleware">
<h5>MetaRefreshMiddleware<a class="headerlink" href="#metarefreshmiddleware" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.redirect.</tt><tt class="descname">MetaRefreshMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware" title="永久链接至目标">¶</a></dt>
<dd><p>该中间件根据meta-refresh html标签处理request重定向。</p>
</dd></dl>

<p><a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware" title="scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">MetaRefreshMiddleware</span></tt></a> 可以通过以下设定进行配置
(更多内容请参考设置文档)。</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-METAREFRESH_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">METAREFRESH_ENABLED</span></tt></a></li>
<li><tt class="xref std std-setting docutils literal"><span class="pre">METAREFRESH_MAXDELAY</span></tt></li>
</ul>
<p>该中间件遵循 <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware" title="scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">RedirectMiddleware</span></tt></a> 描述的
<a class="reference internal" href="index.html#std:setting-REDIRECT_MAX_TIMES"><tt class="xref std std-setting docutils literal"><span class="pre">REDIRECT_MAX_TIMES</span></tt></a> 设定，<a class="reference internal" href="index.html#std:reqmeta-dont_redirect"><tt class="xref std std-reqmeta docutils literal"><span class="pre">dont_redirect</span></tt></a>
及 <a class="reference internal" href="index.html#std:reqmeta-redirect_urls"><tt class="xref std std-reqmeta docutils literal"><span class="pre">redirect_urls</span></tt></a> meta key。</p>
<div class="section" id="metarefreshmiddleware-settings">
<h6>MetaRefreshMiddleware settings<a class="headerlink" href="#metarefreshmiddleware-settings" title="永久链接至标题">¶</a></h6>
<div class="section" id="metarefresh-enabled">
<span id="std:setting-METAREFRESH_ENABLED"></span><h7>METAREFRESH_ENABLED<a class="headerlink" href="#metarefresh-enabled" title="永久链接至标题">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">0.17 新版功能.</span></p>
</div>
<p>默认: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>Meta Refresh中间件是否启用。</p>
</div>
<div class="section" id="redirect-max-metarefresh-delay">
<span id="std:setting-REDIRECT_MAX_METAREFRESH_DELAY"></span><h7>REDIRECT_MAX_METAREFRESH_DELAY<a class="headerlink" href="#redirect-max-metarefresh-delay" title="永久链接至标题">¶</a></h7>
<p>默认: <tt class="docutils literal"><span class="pre">100</span></tt></p>
<p>跟进重定向的最大 meta-refresh 延迟(单位:秒)。</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.retry">
<span id="retrymiddleware"></span><h5>RetryMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.retry" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.retry.RetryMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.retry.</tt><tt class="descname">RetryMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.retry.RetryMiddleware" title="永久链接至目标">¶</a></dt>
<dd><p>该中间件将重试可能由于临时的问题，例如连接超时或者HTTP 500错误导致失败的页面。</p>
</dd></dl>

<p>爬取进程会收集失败的页面并在最后，spider爬取完所有正常(不失败)的页面后重新调度。
一旦没有更多需要重试的失败页面，该中间件将会发送一个信号(retry_complete)，
其他插件可以监听该信号。</p>
<p><a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.retry.RetryMiddleware" title="scrapy.contrib.downloadermiddleware.retry.RetryMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">RetryMiddleware</span></tt></a> 可以通过下列设定进行配置
(更多内容请参考设置文档):</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-RETRY_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">RETRY_ENABLED</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-RETRY_TIMES"><tt class="xref std std-setting docutils literal"><span class="pre">RETRY_TIMES</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-RETRY_HTTP_CODES"><tt class="xref std std-setting docutils literal"><span class="pre">RETRY_HTTP_CODES</span></tt></a></li>
</ul>
<p>关于HTTP错误的考虑:</p>
<p>如果根据HTTP协议，您可能想要在设定 <a class="reference internal" href="index.html#std:setting-RETRY_HTTP_CODES"><tt class="xref std std-setting docutils literal"><span class="pre">RETRY_HTTP_CODES</span></tt></a> 中移除400错误。
该错误被默认包括是由于这个代码经常被用来指示服务器过载(overload)了。而在这种情况下，我们想进行重试。</p>
<p id="std:reqmeta-dont_retry">如果 <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> 包含 <tt class="docutils literal"><span class="pre">dont_retry</span></tt> 键，
该request将会被本中间件忽略。</p>
<div class="section" id="retrymiddleware-settings">
<h6>RetryMiddleware Settings<a class="headerlink" href="#retrymiddleware-settings" title="永久链接至标题">¶</a></h6>
<div class="section" id="retry-enabled">
<span id="std:setting-RETRY_ENABLED"></span><h7>RETRY_ENABLED<a class="headerlink" href="#retry-enabled" title="永久链接至标题">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">0.13 新版功能.</span></p>
</div>
<p>默认: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>Retry Middleware是否启用。</p>
</div>
<div class="section" id="retry-times">
<span id="std:setting-RETRY_TIMES"></span><h7>RETRY_TIMES<a class="headerlink" href="#retry-times" title="永久链接至标题">¶</a></h7>
<p>默认: <tt class="docutils literal"><span class="pre">2</span></tt></p>
<p>包括第一次下载，最多的重试次数</p>
</div>
<div class="section" id="retry-http-codes">
<span id="std:setting-RETRY_HTTP_CODES"></span><h7>RETRY_HTTP_CODES<a class="headerlink" href="#retry-http-codes" title="永久链接至标题">¶</a></h7>
<p>默认: <tt class="docutils literal"><span class="pre">[500,</span> <span class="pre">502,</span> <span class="pre">503,</span> <span class="pre">504,</span> <span class="pre">400,</span> <span class="pre">408]</span></tt></p>
<p>重试的response 返回值(code)。其他错误(DNS查找问题、连接失败及其他)则一定会进行重试。</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.robotstxt">
<span id="robotstxtmiddleware"></span><span id="topics-dlmw-robots"></span><h5>RobotsTxtMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.robotstxt" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.robotstxt.</tt><tt class="descname">RobotsTxtMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware" title="永久链接至目标">¶</a></dt>
<dd><p>该中间件过滤所有robots.txt eclusion standard中禁止的request。</p>
<p>确认该中间件及 <a class="reference internal" href="index.html#std:setting-ROBOTSTXT_OBEY"><tt class="xref std std-setting docutils literal"><span class="pre">ROBOTSTXT_OBEY</span></tt></a> 设置被启用以确保Scrapy尊重robots.txt。</p>
<div class="admonition warning">
<p class="first admonition-title">警告</p>
<p class="last">记住, 如果您在一个网站中使用了多个并发请求，
Scrapy仍然可能下载一些被禁止的页面。这是由于这些页面是在robots.txt被下载前被请求的。
这是当前robots.txt中间件已知的限制，并将在未来进行修复。</p>
</div>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.stats">
<span id="downloaderstats"></span><h5>DownloaderStats<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.stats" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.stats.DownloaderStats">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.stats.</tt><tt class="descname">DownloaderStats</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.stats.DownloaderStats" title="永久链接至目标">¶</a></dt>
<dd><p>保存所有通过的request、response及exception的中间件。</p>
<p>您必须启用 <a class="reference internal" href="index.html#std:setting-DOWNLOADER_STATS"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_STATS</span></tt></a> 来启用该中间件。</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.useragent">
<span id="useragentmiddleware"></span><h5>UserAgentMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.useragent" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.useragent.</tt><tt class="descname">UserAgentMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware" title="永久链接至目标">¶</a></dt>
<dd><p>用于覆盖spider的默认user agent的中间件。</p>
<p>要使得spider能覆盖默认的user agent，其 <cite>user_agent</cite> 属性必须被设置。</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.ajaxcrawl">
<span id="ajaxcrawlmiddleware"></span><span id="ajaxcrawl-middleware"></span><h5>AjaxCrawlMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.ajaxcrawl" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.ajaxcrawl.AjaxCrawlMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.ajaxcrawl.</tt><tt class="descname">AjaxCrawlMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.ajaxcrawl.AjaxCrawlMiddleware" title="永久链接至目标">¶</a></dt>
<dd><p>根据meta-fragment html标签查找 &#8216;AJAX可爬取&#8217; 页面的中间件。查看
<a class="reference external" href="https://developers.google.com/webmasters/ajax-crawling/docs/getting-started">https://developers.google.com/webmasters/ajax-crawling/docs/getting-started</a>
来获得更多内容。</p>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">即使没有启用该中间件，Scrapy仍能查找类似于
<tt class="docutils literal"><span class="pre">'http://example.com/!#foo=bar'</span></tt> 这样的&#8217;AJAX可爬取&#8217;页面。
AjaxCrawlMiddleware是针对不具有 <tt class="docutils literal"><span class="pre">'!#'</span></tt> 的URL，通常发生在&#8217;index&#8217;或者&#8217;main&#8217;页面中。</p>
</div>
</dd></dl>

<div class="section" id="id4">
<h6>AjaxCrawlMiddleware设置<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h6>
<div class="section" id="ajaxcrawl-enabled">
<span id="std:setting-AJAXCRAWL_ENABLED"></span><h7>AJAXCRAWL_ENABLED<a class="headerlink" href="#ajaxcrawl-enabled" title="永久链接至标题">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">0.21 新版功能.</span></p>
</div>
<p>默认: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>AjaxCrawlMiddleware是否启用。您可能需要针对 <a class="reference internal" href="index.html#topics-broad-crawls"><em>通用爬虫</em></a> 启用该中间件。</p>
</div>
</div>
</div>
</div>
</div>
<span id="document-topics/spider-middleware"></span><div class="section" id="spider-middleware">
<span id="topics-spider-middleware"></span><h3>Spider中间件(Middleware)<a class="headerlink" href="#spider-middleware" title="永久链接至标题">¶</a></h3>
<p>下载器中间件是介入到Scrapy的spider处理机制的钩子框架，您可以添加代码来处理发送给
<a class="reference internal" href="index.html#topics-spiders"><em>Spiders</em></a> 的response及spider产生的item和request。</p>
<div class="section" id="spider">
<span id="topics-spider-middleware-setting"></span><h4>激活spider中间件<a class="headerlink" href="#spider" title="永久链接至标题">¶</a></h4>
<p>要启用spider中间件，您可以将其加入到
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES"><tt class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES</span></tt></a> 设置中。
该设置是一个字典，键位中间件的路径，值为中间件的顺序(order)。</p>
<p>样例:</p>
<div class="highlight-none"><div class="highlight"><pre>SPIDER_MIDDLEWARES = {
    &#39;myproject.middlewares.CustomSpiderMiddleware&#39;: 543,
}
</pre></div>
</div>
<p><a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES"><tt class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES</span></tt></a> 设置会与Scrapy定义的
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></tt></a> 设置合并(但不是覆盖)，
而后根据顺序(order)进行排序，最后得到启用中间件的有序列表:
第一个中间件是最靠近引擎的，最后一个中间件是最靠近spider的。</p>
<p>关于如何分配中间件的顺序请查看
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></tt></a> 设置，而后根据您想要放置中间件的位置选择一个值。
由于每个中间件执行不同的动作，您的中间件可能会依赖于之前(或者之后)执行的中间件，因此顺序是很重要的。</p>
<p>如果您想禁止内置的(在
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></tt></a> 中设置并默认启用的)中间件，
您必须在项目的 <a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES"><tt class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES</span></tt></a> 设置中定义该中间件，并将其值赋为 <cite>None</cite> 。
例如，如果您想要关闭off-site中间件:</p>
<div class="highlight-none"><div class="highlight"><pre>SPIDER_MIDDLEWARES = {
    &#39;myproject.middlewares.CustomSpiderMiddleware&#39;: 543,
    &#39;scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware&#39;: None,
}
</pre></div>
</div>
<p>最后，请注意，有些中间件需要通过特定的设置来启用。更多内容请查看相关中间件文档。</p>
</div>
<div class="section" id="id1">
<h4>编写您自己的spider中间件<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h4>
<p>编写spider中间件十分简单。每个中间件组件是一个定义了以下一个或多个方法的Python类:</p>
<span class="target" id="module-scrapy.contrib.spidermiddleware"></span><dl class="class">
<dt id="scrapy.contrib.spidermiddleware.SpiderMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spidermiddleware.</tt><tt class="descname">SpiderMiddleware</tt><a class="headerlink" href="#scrapy.contrib.spidermiddleware.SpiderMiddleware" title="永久链接至目标">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_input">
<tt class="descname">process_spider_input</tt><big>(</big><em>response</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_input" title="永久链接至目标">¶</a></dt>
<dd><p>当response通过spider中间件时，该方法被调用，处理该response。</p>
<p><a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_input" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_input"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_input()</span></tt></a> 应该返回 <tt class="docutils literal"><span class="pre">None</span></tt> 或者抛出一个异常。</p>
<p>如果其返回 <tt class="docutils literal"><span class="pre">None</span></tt> ，Scrapy将会继续处理该response，调用所有其他的中间件直到spider处理该response。</p>
<p>如果其跑出一个异常(exception)，Scrapy将不会调用任何其他中间件的
<a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_input" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_input"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_input()</span></tt></a> 方法，并调用request的errback。
errback的输出将会以另一个方向被重新输入到中间件链中，使用
<a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_output" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_output"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_output()</span></tt></a> 方法来处理，当其抛出异常时则带调用
<a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_exception" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_exception()</span></tt></a> 。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> 对象) &#8211; 被处理的response</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> 对象) &#8211; 该response对应的spider</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_output">
<tt class="descname">process_spider_output</tt><big>(</big><em>response</em>, <em>result</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_output" title="永久链接至目标">¶</a></dt>
<dd><p>当Spider处理response返回result时，该方法被调用。</p>
<p><a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_output" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_output"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_output()</span></tt></a> 必须返回包含
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 或 <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> 对象的可迭代对象(iterable)。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> 对象) &#8211; 生成该输出的response</li>
<li><strong>result</strong> (包含 <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 或
<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> 对象的可迭代对象(iterable)) &#8211; spider返回的result</li>
<li><strong>spider</strong> (<tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt> 对象) &#8211; 其结果被处理的spider</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_exception">
<tt class="descname">process_spider_exception</tt><big>(</big><em>response</em>, <em>exception</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_exception" title="永久链接至目标">¶</a></dt>
<dd><p>当spider或(其他spider中间件的) <a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_input" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_input"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_input()</span></tt></a> 跑出异常时，
该方法被调用。</p>
<p><a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_exception" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_exception()</span></tt></a> 必须要么返回 <tt class="docutils literal"><span class="pre">None</span></tt> ，
要么返回一个包含 <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> 或 <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> 对象的可迭代对象(iterable)。</p>
<p>如果其返回 <tt class="docutils literal"><span class="pre">None</span></tt> ，Scrapy将继续处理该异常，调用中间件链中的其他中间件的
<a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_exception" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_exception()</span></tt></a> 方法，直到所有中间件都被调用，该异常到达引擎(异常将被记录并被忽略)。</p>
<p>如果其返回一个可迭代对象，则中间件链的 <a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_output" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_output"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_output()</span></tt></a> 方法被调用，
其他的 <a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_exception" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_exception()</span></tt></a> 将不会被调用。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> 对象) &#8211; 异常被抛出时被处理的response</li>
<li><strong>exception</strong> (<a class="reference external" href="http://docs.python.org/library/exceptions.html#exceptions.Exception">Exception</a> 对象) &#8211; 被跑出的异常</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> 对象) &#8211; 抛出该异常的spider</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_start_requests">
<tt class="descname">process_start_requests</tt><big>(</big><em>start_requests</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_start_requests" title="永久链接至目标">¶</a></dt>
<dd><div class="versionadded">
<p><span class="versionmodified">0.15 新版功能.</span></p>
</div>
<p>该方法以spider 启动的request为参数被调用，执行的过程类似于
<a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_output" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_output"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_output()</span></tt></a> ，只不过其没有相关联的response并且必须返回request(不是item)。</p>
<p>其接受一个可迭代的对象(<tt class="docutils literal"><span class="pre">start_requests</span></tt> 参数)且必须返回另一个包含
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象的可迭代对象。</p>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">当在您的spider中间件实现该方法时，
您必须返回一个可迭代对象(类似于参数start_requests)且不要遍历所有的 <tt class="docutils literal"><span class="pre">start_requests</span></tt>。
该迭代器会很大(甚至是无限)，进而导致内存溢出。
Scrapy引擎在其具有能力处理start request时将会拉起request，
因此start request迭代器会变得无限，而由其他参数来停止spider(
例如时间限制或者item/page记数)。</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>start_requests</strong> (包含 <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 的可迭代对象) &#8211; start requests</li>
<li><strong>spider</strong> (<tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt> 对象) &#8211; start requests所属的spider</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="topics-spider-middleware-ref">
<span id="id2"></span><h4>内置spider中间件参考手册<a class="headerlink" href="#topics-spider-middleware-ref" title="永久链接至标题">¶</a></h4>
<p>本页面介绍了Scrapy自带的所有spider中间件。关于如何使用及编写您自己的中间件，请参考
<a class="reference internal" href="index.html#topics-downloader-middleware"><em>spider middleware usage guide</em></a>.</p>
<p>关于默认启用的中间件列表(及其顺序)请参考
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></tt></a> 设置。</p>
<div class="section" id="module-scrapy.contrib.spidermiddleware.depth">
<span id="depthmiddleware"></span><h5>DepthMiddleware<a class="headerlink" href="#module-scrapy.contrib.spidermiddleware.depth" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.spidermiddleware.depth.DepthMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spidermiddleware.depth.</tt><tt class="descname">DepthMiddleware</tt><a class="headerlink" href="#scrapy.contrib.spidermiddleware.depth.DepthMiddleware" title="永久链接至目标">¶</a></dt>
<dd><p>DepthMiddleware是一个用于追踪每个Request在被爬取的网站的深度的中间件。
其可以用来限制爬取深度的最大深度或类似的事情。</p>
<p><a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.depth.DepthMiddleware" title="scrapy.contrib.spidermiddleware.depth.DepthMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">DepthMiddleware</span></tt></a> 可以通过下列设置进行配置(更多内容请参考设置文档):</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-DEPTH_LIMIT"><tt class="xref std std-setting docutils literal"><span class="pre">DEPTH_LIMIT</span></tt></a> - 爬取所允许的最大深度，如果为0，则没有限制。</li>
<li><a class="reference internal" href="index.html#std:setting-DEPTH_STATS"><tt class="xref std std-setting docutils literal"><span class="pre">DEPTH_STATS</span></tt></a> - 是否收集爬取状态。</li>
<li><a class="reference internal" href="index.html#std:setting-DEPTH_PRIORITY"><tt class="xref std std-setting docutils literal"><span class="pre">DEPTH_PRIORITY</span></tt></a> - 是否根据其深度对requet安排优先级</li>
</ul>
</div></blockquote>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.spidermiddleware.httperror">
<span id="httperrormiddleware"></span><h5>HttpErrorMiddleware<a class="headerlink" href="#module-scrapy.contrib.spidermiddleware.httperror" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spidermiddleware.httperror.</tt><tt class="descname">HttpErrorMiddleware</tt><a class="headerlink" href="#scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware" title="永久链接至目标">¶</a></dt>
<dd><p>过滤出所有失败(错误)的HTTP response，因此spider不需要处理这些request。
处理这些request意味着消耗更多资源，并且使得spider逻辑更为复杂。</p>
</dd></dl>

<p>根据 <a class="reference external" href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html">HTTP标准</a> ，返回值为200-300之间的值为成功的resonse。</p>
<p>如果您想处理在这个范围之外的response，您可以通过
spider的 <tt class="docutils literal"><span class="pre">handle_httpstatus_list</span></tt> 属性或
<a class="reference internal" href="index.html#std:setting-HTTPERROR_ALLOWED_CODES"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPERROR_ALLOWED_CODES</span></tt></a> 设置来指定spider能处理的response返回值。</p>
<p>例如，如果您想要处理返回值为404的response您可以这么做:</p>
<div class="highlight-none"><div class="highlight"><pre>class MySpider(CrawlSpider):
    handle_httpstatus_list = [404]
</pre></div>
</div>
<p id="std:reqmeta-handle_httpstatus_list"><a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a>
中的 <tt class="docutils literal"><span class="pre">handle_httpstatus_list</span></tt> 键也可以用来指定每个request所允许的response code。</p>
<p>不过请记住，除非您知道您在做什么，否则处理非200返回一般来说是个糟糕的决定。</p>
<p>更多内容请参考: <a class="reference external" href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html">HTTP Status Code定义</a>.</p>
<div class="section" id="httperrormiddleware-settings">
<h6>HttpErrorMiddleware settings<a class="headerlink" href="#httperrormiddleware-settings" title="永久链接至标题">¶</a></h6>
<div class="section" id="httperror-allowed-codes">
<span id="std:setting-HTTPERROR_ALLOWED_CODES"></span><h7>HTTPERROR_ALLOWED_CODES<a class="headerlink" href="#httperror-allowed-codes" title="永久链接至标题">¶</a></h7>
<p>默认: <tt class="docutils literal"><span class="pre">[]</span></tt></p>
<p>忽略该列表中所有非200状态码的response。</p>
</div>
<div class="section" id="httperror-allow-all">
<span id="std:setting-HTTPERROR_ALLOW_ALL"></span><h7>HTTPERROR_ALLOW_ALL<a class="headerlink" href="#httperror-allow-all" title="永久链接至标题">¶</a></h7>
<p>默认: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>忽略所有response，不管其状态值。</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.spidermiddleware.offsite">
<span id="offsitemiddleware"></span><h5>OffsiteMiddleware<a class="headerlink" href="#module-scrapy.contrib.spidermiddleware.offsite" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spidermiddleware.offsite.</tt><tt class="descname">OffsiteMiddleware</tt><a class="headerlink" href="#scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware" title="永久链接至目标">¶</a></dt>
<dd><p>过滤出所有URL不由该spider负责的Request。</p>
<p>该中间件过滤出所有主机名不在spider属性
<a class="reference internal" href="index.html#scrapy.spider.Spider.allowed_domains" title="scrapy.spider.Spider.allowed_domains"><tt class="xref py py-attr docutils literal"><span class="pre">allowed_domains</span></tt></a> 的request。</p>
<p>当spide返回一个主机名不属于该spider的request时，
该中间件将会做一个类似于下面的记录:</p>
<div class="highlight-none"><div class="highlight"><pre>DEBUG: Filtered offsite request to &#39;www.othersite.com&#39;: &lt;GET http://www.othersite.com/some/page.html&gt;
</pre></div>
</div>
<p>为了避免记录太多无用信息，其将对每个新发现的网站记录一次。因此，例如，
如果过滤出另一个 <tt class="docutils literal"><span class="pre">www.othersite.com</span></tt> 请求，将不会有新的记录。
但如果过滤出 <tt class="docutils literal"><span class="pre">someothersite.com</span></tt> 请求，仍然会有记录信息(仅针对第一次)。</p>
<p>如果spider没有定义
<a class="reference internal" href="index.html#scrapy.spider.Spider.allowed_domains" title="scrapy.spider.Spider.allowed_domains"><tt class="xref py py-attr docutils literal"><span class="pre">allowed_domains</span></tt></a> 属性，或该属性为空，
则offsite 中间件将会允许所有request。</p>
<p>如果request设置了 <tt class="xref py py-attr docutils literal"><span class="pre">dont_filter</span></tt> 属性，
即使该request的网站不在允许列表里，则offsite中间件将会允许该request。</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.spidermiddleware.referer">
<span id="referermiddleware"></span><h5>RefererMiddleware<a class="headerlink" href="#module-scrapy.contrib.spidermiddleware.referer" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.spidermiddleware.referer.RefererMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spidermiddleware.referer.</tt><tt class="descname">RefererMiddleware</tt><a class="headerlink" href="#scrapy.contrib.spidermiddleware.referer.RefererMiddleware" title="永久链接至目标">¶</a></dt>
<dd><p>根据生成Request的Response的URL来设置Request <tt class="docutils literal"><span class="pre">Referer</span></tt> 字段。</p>
</dd></dl>

<div class="section" id="referermiddleware-settings">
<h6>RefererMiddleware settings<a class="headerlink" href="#referermiddleware-settings" title="永久链接至标题">¶</a></h6>
<div class="section" id="referer-enabled">
<span id="std:setting-REFERER_ENABLED"></span><h7>REFERER_ENABLED<a class="headerlink" href="#referer-enabled" title="永久链接至标题">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">0.15 新版功能.</span></p>
</div>
<p>默认: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>是否启用referer中间件。</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.spidermiddleware.urllength">
<span id="urllengthmiddleware"></span><h5>UrlLengthMiddleware<a class="headerlink" href="#module-scrapy.contrib.spidermiddleware.urllength" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spidermiddleware.urllength.</tt><tt class="descname">UrlLengthMiddleware</tt><a class="headerlink" href="#scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware" title="永久链接至目标">¶</a></dt>
<dd><p>过滤出URL长度比URLLENGTH_LIMIT的request。</p>
<p><a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware" title="scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">UrlLengthMiddleware</span></tt></a> 可以通过下列设置进行配置(更多内容请参考设置文档):</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-URLLENGTH_LIMIT"><tt class="xref std std-setting docutils literal"><span class="pre">URLLENGTH_LIMIT</span></tt></a> - 允许爬取URL最长的长度.</li>
</ul>
</div></blockquote>
</dd></dl>

</div>
</div>
</div>
<span id="document-topics/extensions"></span><div class="section" id="extensions">
<span id="topics-extensions"></span><h3>扩展(Extensions)<a class="headerlink" href="#extensions" title="永久链接至标题">¶</a></h3>
<p>扩展框架提供一个机制，使得你能将自定义功能绑定到Scrapy。</p>
<p>扩展只是正常的类，它们在Scrapy启动时被实例化、初始化。</p>
<div class="section" id="extension-settings">
<h4>扩展设置(Extension settings)<a class="headerlink" href="#extension-settings" title="永久链接至标题">¶</a></h4>
<p>扩展使用 <a class="reference internal" href="index.html#topics-settings"><em>Scrapy settings</em></a> 管理它们的设置，这跟其他Scrapy代码一样。</p>
<p>通常扩展需要给它们的设置加上前缀，以避免跟已有(或将来)的扩展冲突。
比如，一个扩展处理 <a class="reference external" href="http://en.wikipedia.org/wiki/Sitemaps">Google Sitemaps</a>，
则可以使用类似 <cite>GOOGLESITEMAP_ENABLED</cite>、<cite>GOOGLESITEMAP_DEPTH</cite> 等设置。</p>
</div>
<div class="section" id="id1">
<h4>加载和激活扩展<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h4>
<p>扩展在扩展类被实例化时加载和激活。
因此，所有扩展的实例化代码必须在类的构造函数(<tt class="docutils literal"><span class="pre">__init__</span></tt>)中执行。</p>
<p>要使得扩展可用，需要把它添加到Scrapy的 <a class="reference internal" href="index.html#std:setting-EXTENSIONS"><tt class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS</span></tt></a> 配置中。
在 <a class="reference internal" href="index.html#std:setting-EXTENSIONS"><tt class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS</span></tt></a> 中，每个扩展都使用一个字符串表示，即扩展类的全Python路径。
比如:</p>
<div class="highlight-none"><div class="highlight"><pre>EXTENSIONS = {
    &#39;scrapy.contrib.corestats.CoreStats&#39;: 500,
    &#39;scrapy.webservice.WebService&#39;: 500,
    &#39;scrapy.telnet.TelnetConsole&#39;: 500,
}
</pre></div>
</div>
<p>如你所见，<a class="reference internal" href="index.html#std:setting-EXTENSIONS"><tt class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS</span></tt></a> 配置是一个dict，key是扩展类的路径，value是顺序,
它定义扩展加载的顺序。扩展顺序不像中间件的顺序那么重要，而且扩展之间一般没有关联。
扩展加载的顺序并不重要，因为它们并不相互依赖。</p>
<p>如果你需要添加扩展而且它依赖别的扩展，你就可以使用该特性了。</p>
<p>[1] 这也是为什么Scrapy的配置项 <a class="reference internal" href="index.html#std:setting-EXTENSIONS_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS_BASE</span></tt></a>
(它包括了所有内置且开启的扩展)定义所有扩展的顺序都相同 (<tt class="docutils literal"><span class="pre">500</span></tt>)。</p>
</div>
<div class="section" id="available-enabled-disabled">
<h4>可用的(Available)、开启的(enabled)和禁用的(disabled)的扩展<a class="headerlink" href="#available-enabled-disabled" title="永久链接至标题">¶</a></h4>
<p>并不是所有可用的扩展都会被开启。一些扩展经常依赖一些特别的配置。
比如，HTTP Cache扩展是可用的但默认是禁用的，除非 <a class="reference internal" href="index.html#std:setting-HTTPCACHE_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_ENABLED</span></tt></a> 配置项设置了。</p>
</div>
<div class="section" id="disabling-an-extension">
<h4>禁用扩展(Disabling an extension)<a class="headerlink" href="#disabling-an-extension" title="永久链接至标题">¶</a></h4>
<p>为了禁用一个默认开启的扩展(比如，包含在 <a class="reference internal" href="index.html#std:setting-EXTENSIONS_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS_BASE</span></tt></a> 中的扩展)，
需要将其顺序(order)设置为 <tt class="docutils literal"><span class="pre">None</span></tt> 。比如:</p>
<div class="highlight-none"><div class="highlight"><pre>EXTENSIONS = {
    &#39;scrapy.contrib.corestats.CoreStats&#39;: None,
}
</pre></div>
</div>
</div>
<div class="section" id="id2">
<h4>实现你的扩展<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h4>
<p>实现你的扩展很简单。每个扩展是一个单一的Python class，它不需要实现任何特殊的方法。</p>
<p>Scrapy扩展(包括middlewares和pipelines)的主要入口是 <tt class="docutils literal"><span class="pre">from_crawler</span></tt> 类方法，
它接收一个 <tt class="docutils literal"><span class="pre">Crawler</span></tt> 类的实例，该实例是控制Scrapy crawler的主要对象。
如果扩展需要，你可以通过这个对象访问settings，signals，stats，控制爬虫的行为。</p>
<p>通常来说，扩展关联到 <a class="reference internal" href="index.html#topics-signals"><em>signals</em></a> 并执行它们触发的任务。</p>
<p>最后，如果 <tt class="docutils literal"><span class="pre">from_crawler</span></tt> 方法抛出 <a class="reference internal" href="index.html#scrapy.exceptions.NotConfigured" title="scrapy.exceptions.NotConfigured"><tt class="xref py py-exc docutils literal"><span class="pre">NotConfigured</span></tt></a> 异常，
扩展会被禁用。否则，扩展会被开启。</p>
<div class="section" id="sample-extension">
<h5>扩展例子(Sample extension)<a class="headerlink" href="#sample-extension" title="永久链接至标题">¶</a></h5>
<p>这里我们将实现一个简单的扩展来演示上面描述到的概念。
该扩展会在以下事件时记录一条日志：</p>
<ul class="simple">
<li>spider被打开</li>
<li>spider被关闭</li>
<li>爬取了特定数量的条目(items)</li>
</ul>
<p>该扩展通过 <tt class="docutils literal"><span class="pre">MYEXT_ENABLED</span></tt> 配置项开启，
items的数量通过 <tt class="docutils literal"><span class="pre">MYEXT_ITEMCOUNT</span></tt> 配置项设置。</p>
<p>以下是扩展的代码:</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy import signals
from scrapy.exceptions import NotConfigured

class SpiderOpenCloseLogging(object):

    def __init__(self, item_count):
        self.item_count = item_count

        self.items_scraped = 0

    @classmethod
    def from_crawler(cls, crawler):
        # first check if the extension should be enabled and raise

        # NotConfigured otherwise

        if not crawler.settings.getbool(&#39;MYEXT_ENABLED&#39;):

            raise NotConfigured

        # get the number of items from settings

        item_count = crawler.settings.getint(&#39;MYEXT_ITEMCOUNT&#39;, 1000)

        # instantiate the extension object

        ext = cls(item_count)

        # connect the extension object to signals

        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)

        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)

        crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped)

        # return the extension object

        return ext

    def spider_opened(self, spider):
        spider.log(&quot;opened spider %s&quot; % spider.name)

    def spider_closed(self, spider):
        spider.log(&quot;closed spider %s&quot; % spider.name)

    def item_scraped(self, item, spider):
        self.items_scraped += 1

        if self.items_scraped == self.item_count:

            spider.log(&quot;scraped %d items, resetting counter&quot; % self.items_scraped)

            self.item_count = 0
</pre></div>
</div>
</div>
</div>
<div class="section" id="topics-extensions-ref">
<span id="id3"></span><h4>内置扩展介绍<a class="headerlink" href="#topics-extensions-ref" title="永久链接至标题">¶</a></h4>
<div class="section" id="id4">
<h5>通用扩展<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h5>
<div class="section" id="module-scrapy.contrib.logstats">
<span id="log-stats-extension"></span><h6>记录统计扩展(Log Stats extension)<a class="headerlink" href="#module-scrapy.contrib.logstats" title="永久链接至标题">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.logstats.LogStats">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.logstats.</tt><tt class="descname">LogStats</tt><a class="headerlink" href="#scrapy.contrib.logstats.LogStats" title="永久链接至目标">¶</a></dt>
<dd></dd></dl>

<p>记录基本的统计信息，比如爬取的页面和条目(items)。</p>
</div>
<div class="section" id="module-scrapy.contrib.corestats">
<span id="core-stats-extension"></span><h6>核心统计扩展(Core Stats extension)<a class="headerlink" href="#module-scrapy.contrib.corestats" title="永久链接至标题">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.corestats.CoreStats">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.corestats.</tt><tt class="descname">CoreStats</tt><a class="headerlink" href="#scrapy.contrib.corestats.CoreStats" title="永久链接至目标">¶</a></dt>
<dd></dd></dl>

<p>如果统计收集器(stats collection)启用了，该扩展开启核心统计收集(参考 <a class="reference internal" href="index.html#topics-stats"><em>数据收集(Stats Collection)</em></a>)。</p>
</div>
<div class="section" id="module-scrapy.webservice">
<span id="web-service"></span><span id="topics-extensions-ref-webservice"></span><h6>Web service 扩展<a class="headerlink" href="#module-scrapy.webservice" title="永久链接至标题">¶</a></h6>
<dl class="class">
<dt id="scrapy.webservice.scrapy.webservice.WebService">
<em class="property">class </em><tt class="descclassname">scrapy.webservice.</tt><tt class="descname">WebService</tt><a class="headerlink" href="#scrapy.webservice.scrapy.webservice.WebService" title="永久链接至目标">¶</a></dt>
<dd></dd></dl>

<p>参考 <a class="reference internal" href="index.html#topics-webservice"><em>webservice</em></a> 。</p>
</div>
<div class="section" id="module-scrapy.telnet">
<span id="telnet-console"></span><span id="topics-extensions-ref-telnetconsole"></span><h6>Telnet console 扩展<a class="headerlink" href="#module-scrapy.telnet" title="永久链接至标题">¶</a></h6>
<dl class="class">
<dt id="scrapy.telnet.scrapy.telnet.TelnetConsole">
<em class="property">class </em><tt class="descclassname">scrapy.telnet.</tt><tt class="descname">TelnetConsole</tt><a class="headerlink" href="#scrapy.telnet.scrapy.telnet.TelnetConsole" title="永久链接至目标">¶</a></dt>
<dd></dd></dl>

<p>提供一个telnet控制台，用于进入当前执行的Scrapy进程的Python解析器，
这对代码调试非常有帮助。</p>
<p>telnet控制台通过 <a class="reference internal" href="index.html#std:setting-TELNETCONSOLE_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">TELNETCONSOLE_ENABLED</span></tt></a> 配置项开启，
服务器会监听 <a class="reference internal" href="index.html#std:setting-TELNETCONSOLE_PORT"><tt class="xref std std-setting docutils literal"><span class="pre">TELNETCONSOLE_PORT</span></tt></a> 指定的端口。</p>
</div>
<div class="section" id="module-scrapy.contrib.memusage">
<span id="memory-usage-extension"></span><span id="topics-extensions-ref-memusage"></span><h6>内存使用扩展(Memory usage extension)<a class="headerlink" href="#module-scrapy.contrib.memusage" title="永久链接至标题">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.memusage.scrapy.contrib.memusage.MemoryUsage">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.memusage.</tt><tt class="descname">MemoryUsage</tt><a class="headerlink" href="#scrapy.contrib.memusage.scrapy.contrib.memusage.MemoryUsage" title="永久链接至目标">¶</a></dt>
<dd></dd></dl>

<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">This extension does not work in Windows.</p>
</div>
<p>监控Scrapy进程内存使用量，并且：</p>
<ol class="arabic simple">
<li>如果使用内存量超过某个指定值，发送提醒邮件</li>
<li>如果超过某个指定值，关闭spider</li>
</ol>
<p>当内存用量达到 <a class="reference internal" href="index.html#std:setting-MEMUSAGE_WARNING_MB"><tt class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_WARNING_MB</span></tt></a> 指定的值，发送提醒邮件。
当内存用量达到 <a class="reference internal" href="index.html#std:setting-MEMUSAGE_LIMIT_MB"><tt class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_LIMIT_MB</span></tt></a> 指定的值，发送提醒邮件，同时关闭spider，
Scrapy进程退出。</p>
<p>该扩展通过 <a class="reference internal" href="index.html#std:setting-MEMUSAGE_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_ENABLED</span></tt></a> 配置项开启，可以使用以下选项：</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_LIMIT_MB"><tt class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_LIMIT_MB</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_WARNING_MB"><tt class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_WARNING_MB</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_NOTIFY_MAIL"><tt class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_NOTIFY_MAIL</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_REPORT"><tt class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_REPORT</span></tt></a></li>
</ul>
</div>
<div class="section" id="module-scrapy.contrib.memdebug">
<span id="memory-debugger-extension"></span><h6>内存调试扩展(Memory debugger extension)<a class="headerlink" href="#module-scrapy.contrib.memdebug" title="永久链接至标题">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.memdebug.scrapy.contrib.memdebug.MemoryDebugger">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.memdebug.</tt><tt class="descname">MemoryDebugger</tt><a class="headerlink" href="#scrapy.contrib.memdebug.scrapy.contrib.memdebug.MemoryDebugger" title="永久链接至目标">¶</a></dt>
<dd></dd></dl>

<p>该扩展用于调试内存使用量，它收集以下信息：</p>
<ul class="simple">
<li>没有被Python垃圾回收器收集的对象</li>
<li>应该被销毁却仍然存活的对象。更多信息请参考 <a class="reference internal" href="index.html#topics-leaks-trackrefs"><em>使用 trackref 调试内存泄露</em></a></li>
</ul>
<p>开启该扩展，需打开 <a class="reference internal" href="index.html#std:setting-MEMDEBUG_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">MEMDEBUG_ENABLED</span></tt></a> 配置项。
信息将会存储在统计信息(stats)中。</p>
</div>
<div class="section" id="module-scrapy.contrib.closespider">
<span id="spider"></span><h6>关闭spider扩展<a class="headerlink" href="#module-scrapy.contrib.closespider" title="永久链接至标题">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.closespider.scrapy.contrib.closespider.CloseSpider">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.closespider.</tt><tt class="descname">CloseSpider</tt><a class="headerlink" href="#scrapy.contrib.closespider.scrapy.contrib.closespider.CloseSpider" title="永久链接至目标">¶</a></dt>
<dd></dd></dl>

<p>当某些状况发生，spider会自动关闭。每种情况使用指定的关闭原因。</p>
<p>关闭spider的情况可以通过下面的设置项配置：</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-CLOSESPIDER_TIMEOUT"><tt class="xref std std-setting docutils literal"><span class="pre">CLOSESPIDER_TIMEOUT</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-CLOSESPIDER_ITEMCOUNT"><tt class="xref std std-setting docutils literal"><span class="pre">CLOSESPIDER_ITEMCOUNT</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-CLOSESPIDER_PAGECOUNT"><tt class="xref std std-setting docutils literal"><span class="pre">CLOSESPIDER_PAGECOUNT</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-CLOSESPIDER_ERRORCOUNT"><tt class="xref std std-setting docutils literal"><span class="pre">CLOSESPIDER_ERRORCOUNT</span></tt></a></li>
</ul>
<div class="section" id="closespider-timeout">
<span id="std:setting-CLOSESPIDER_TIMEOUT"></span><h7>CLOSESPIDER_TIMEOUT<a class="headerlink" href="#closespider-timeout" title="永久链接至标题">¶</a></h7>
<p>默认值: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>一个整数值，单位为秒。如果一个spider在指定的秒数后仍在运行，
它将以 <tt class="docutils literal"><span class="pre">closespider_timeout</span></tt> 的原因被自动关闭。
如果值设置为0（或者没有设置），spiders不会因为超时而关闭。</p>
</div>
<div class="section" id="closespider-itemcount">
<span id="std:setting-CLOSESPIDER_ITEMCOUNT"></span><h7>CLOSESPIDER_ITEMCOUNT<a class="headerlink" href="#closespider-itemcount" title="永久链接至标题">¶</a></h7>
<p>缺省值: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>一个整数值，指定条目的个数。如果spider爬取条目数超过了指定的数，
并且这些条目通过item pipeline传递，spider将会以 <tt class="docutils literal"><span class="pre">closespider_itemcount</span></tt> 的原因被自动关闭。</p>
</div>
<div class="section" id="closespider-pagecount">
<span id="std:setting-CLOSESPIDER_PAGECOUNT"></span><h7>CLOSESPIDER_PAGECOUNT<a class="headerlink" href="#closespider-pagecount" title="永久链接至标题">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">0.11 新版功能.</span></p>
</div>
<p>缺省值: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>一个整数值，指定最大的抓取响应(reponses)数。
如果spider抓取数超过指定的值，则会以 <tt class="docutils literal"><span class="pre">closespider_pagecount</span></tt> 的原因自动关闭。
如果设置为0（或者未设置），spiders不会因为抓取的响应数而关闭。</p>
</div>
<div class="section" id="closespider-errorcount">
<span id="std:setting-CLOSESPIDER_ERRORCOUNT"></span><h7>CLOSESPIDER_ERRORCOUNT<a class="headerlink" href="#closespider-errorcount" title="永久链接至标题">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">0.11 新版功能.</span></p>
</div>
<p>缺省值: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>一个整数值，指定spider可以接受的最大错误数。
如果spider生成多于该数目的错误，它将以 <tt class="docutils literal"><span class="pre">closespider_errorcount</span></tt> 的原因关闭。
如果设置为0（或者未设置），spiders不会因为发生错误过多而关闭。</p>
</div>
</div>
<div class="section" id="module-scrapy.contrib.statsmailer">
<span id="statsmailer-extension"></span><h6>StatsMailer extension<a class="headerlink" href="#module-scrapy.contrib.statsmailer" title="永久链接至标题">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.statsmailer.scrapy.contrib.statsmailer.StatsMailer">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.statsmailer.</tt><tt class="descname">StatsMailer</tt><a class="headerlink" href="#scrapy.contrib.statsmailer.scrapy.contrib.statsmailer.StatsMailer" title="永久链接至目标">¶</a></dt>
<dd></dd></dl>

<p>这个简单的扩展可用来在一个域名爬取完毕时发送提醒邮件，
包含Scrapy收集的统计信息。
邮件会发送个通过 <a class="reference internal" href="index.html#std:setting-STATSMAILER_RCPTS"><tt class="xref std std-setting docutils literal"><span class="pre">STATSMAILER_RCPTS</span></tt></a> 指定的所有接收人。</p>
<span class="target" id="module-scrapy.contrib.debug"></span></div>
</div>
<div class="section" id="debugging-extensions">
<h5>Debugging extensions<a class="headerlink" href="#debugging-extensions" title="永久链接至标题">¶</a></h5>
<div class="section" id="stack-trace-dump-extension">
<h6>Stack trace dump extension<a class="headerlink" href="#stack-trace-dump-extension" title="永久链接至标题">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.debug.scrapy.contrib.debug.StackTraceDump">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.debug.</tt><tt class="descname">StackTraceDump</tt><a class="headerlink" href="#scrapy.contrib.debug.scrapy.contrib.debug.StackTraceDump" title="永久链接至目标">¶</a></dt>
<dd></dd></dl>

<p>当收到 <cite>SIGQUIT</cite> 或 <cite>SIGUSR2</cite> 信号，spider进程的信息将会被存储下来。
存储的信息包括：</p>
<ol class="arabic simple">
<li>engine状态(使用 <tt class="docutils literal"><span class="pre">scrapy.utils.engin.get_engine_status()</span></tt>)</li>
<li>所有存活的引用(live references)(参考 <a class="reference internal" href="index.html#topics-leaks-trackrefs"><em>使用 trackref 调试内存泄露</em></a>)</li>
<li>所有线程的堆栈信息</li>
</ol>
<p>当堆栈信息和engine状态存储后，Scrapy进程继续正常运行。</p>
<p>该扩展只在POSIX兼容的平台上可运行（比如不能在Windows运行），
因为 <cite>SIGQUIT</cite> 和 <cite>SIGUSR2</cite> 信号在Windows上不可用。</p>
<p>至少有两种方式可以向Scrapy发送 <a class="reference external" href="http://en.wikipedia.org/wiki/SIGQUIT">SIGQUIT</a> 信号:</p>
<ol class="arabic">
<li><p class="first">在Scrapy进程运行时通过按Ctrl-(仅Linux可行?)</p>
</li>
<li><p class="first">运行该命令(<tt class="docutils literal"><span class="pre">&lt;pid&gt;</span></tt> 是Scrapy运行的进程):</p>
<div class="highlight-none"><div class="highlight"><pre>kill -QUIT &lt;pid&gt;
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="debugger-extension">
<h6>调试扩展(Debugger extension)<a class="headerlink" href="#debugger-extension" title="永久链接至标题">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.debug.scrapy.contrib.debug.Debugger">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.debug.</tt><tt class="descname">Debugger</tt><a class="headerlink" href="#scrapy.contrib.debug.scrapy.contrib.debug.Debugger" title="永久链接至目标">¶</a></dt>
<dd></dd></dl>

<p>当收到 <cite>SIGUSR2</cite> 信号，将会在Scrapy进程中调用 <a class="reference external" href="http://docs.python.org/library/pdb.html">Python debugger</a> 。
debugger退出后，Scrapy进程继续正常运行。</p>
<p>更多信息参考 <cite>Debugging in Python</cite> 。</p>
<p>该扩展只在POSIX兼容平台上工作(比如不能再Windows上运行)。</p>
</div>
</div>
</div>
</div>
<span id="document-topics/api"></span><div class="section" id="api">
<span id="topics-api"></span><h3>核心API<a class="headerlink" href="#api" title="永久链接至标题">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">0.15 新版功能.</span></p>
</div>
<p>该节文档讲述Scrapy核心API，目标用户是开发Scrapy扩展(extensions)和中间件(middlewares)的开发人员。</p>
<div class="section" id="crawler-api">
<span id="topics-api-crawler"></span><h4>Crawler API<a class="headerlink" href="#crawler-api" title="永久链接至标题">¶</a></h4>
<p>Scrapy API的主要入口是 <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><tt class="xref py py-class docutils literal"><span class="pre">Crawler</span></tt></a> 的实例对象，
通过类方法 <tt class="docutils literal"><span class="pre">from_crawler</span></tt> 将它传递给扩展(extensions)。
该对象提供对所有Scrapy核心组件的访问，
也是扩展访问Scrapy核心组件和挂载功能到Scrapy的唯一途径。</p>
<span class="target" id="module-scrapy.crawler"></span><p>Extension Manager负责加载和跟踪已经安装的扩展，
它通过 <a class="reference internal" href="index.html#std:setting-EXTENSIONS"><tt class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS</span></tt></a> 配置，包含一个所有可用扩展的字典，
字典的顺序跟你在 <a class="reference internal" href="index.html#topics-downloader-middleware-setting"><em>configure the downloader middlewares</em></a> 配置的顺序一致。</p>
<dl class="class">
<dt id="scrapy.crawler.Crawler">
<em class="property">class </em><tt class="descclassname">scrapy.crawler.</tt><tt class="descname">Crawler</tt><big>(</big><em>settings</em><big>)</big><a class="headerlink" href="#scrapy.crawler.Crawler" title="永久链接至目标">¶</a></dt>
<dd><p>Crawler必须使用 <a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><tt class="xref py py-class docutils literal"><span class="pre">scrapy.settings.Settings</span></tt></a> 的对象进行实例化</p>
<dl class="attribute">
<dt id="scrapy.crawler.Crawler.settings">
<tt class="descname">settings</tt><a class="headerlink" href="#scrapy.crawler.Crawler.settings" title="永久链接至目标">¶</a></dt>
<dd><p>crawler的配置管理器。</p>
<p>扩展(extensions)和中间件(middlewares)使用它用来访问Scrapy的配置。</p>
<p>关于Scrapy配置的介绍参考这里 <a class="reference internal" href="index.html#topics-settings"><em>Settings</em></a>。</p>
<p>API参考 <a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><tt class="xref py py-class docutils literal"><span class="pre">Settings</span></tt></a>。</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.signals">
<tt class="descname">signals</tt><a class="headerlink" href="#scrapy.crawler.Crawler.signals" title="永久链接至目标">¶</a></dt>
<dd><p>crawler的信号管理器。</p>
<p>扩展和中间件使用它将自己的功能挂载到Scrapy。</p>
<p>关于信号的介绍参考 <a class="reference internal" href="index.html#topics-signals"><em>信号(Signals)</em></a>。</p>
<p>API参考 <a class="reference internal" href="index.html#scrapy.signalmanager.SignalManager" title="scrapy.signalmanager.SignalManager"><tt class="xref py py-class docutils literal"><span class="pre">SignalManager</span></tt></a>。</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.stats">
<tt class="descname">stats</tt><a class="headerlink" href="#scrapy.crawler.Crawler.stats" title="永久链接至目标">¶</a></dt>
<dd><p>crawler的统计信息收集器。</p>
<p>扩展和中间件使用它记录操作的统计信息，或者访问由其他扩展收集的统计信息。</p>
<p>关于统计信息收集器的介绍参考 <a class="reference internal" href="index.html#topics-stats"><em>数据收集(Stats Collection)</em></a>。</p>
<p>API参考类 <a class="reference internal" href="index.html#scrapy.statscol.StatsCollector" title="scrapy.statscol.StatsCollector"><tt class="xref py py-class docutils literal"><span class="pre">StatsCollector</span></tt></a> class。</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.extensions">
<tt class="descname">extensions</tt><a class="headerlink" href="#scrapy.crawler.Crawler.extensions" title="永久链接至目标">¶</a></dt>
<dd><p>扩展管理器，跟踪所有开启的扩展。</p>
<p>大多数扩展不需要访问该属性。</p>
<p>关于扩展和可用扩展列表器的介绍参考 <a class="reference internal" href="index.html#topics-extensions"><em>扩展(Extensions)</em></a>。</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.spiders">
<tt class="descname">spiders</tt><a class="headerlink" href="#scrapy.crawler.Crawler.spiders" title="永久链接至目标">¶</a></dt>
<dd><p>spider管理器，加载和实例化spiders。</p>
<p>大多数扩展不需要访问该属性。</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.engine">
<tt class="descname">engine</tt><a class="headerlink" href="#scrapy.crawler.Crawler.engine" title="永久链接至目标">¶</a></dt>
<dd><p>执行引擎，协调crawler的核心逻辑，包括调度，下载和spider。</p>
<p>某些扩展可能需要访问Scrapy的引擎属性，以修改检查(modify inspect)或修改下载器和调度器的行为，
这是该API的高级使用，但还不稳定。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.Crawler.configure">
<tt class="descname">configure</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.crawler.Crawler.configure" title="永久链接至目标">¶</a></dt>
<dd><p>配置crawler。</p>
<p>该方法加载扩展、中间件和spiders，使crawler处于ready状态。
同时，它还配置好了执行引擎。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.Crawler.start">
<tt class="descname">start</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.crawler.Crawler.start" title="永久链接至目标">¶</a></dt>
<dd><p>启动crawler。如果 :meth: <cite>configure</cite> 方法还未被调用过，该方法会调用它。
返回一个延迟deferred对象，当爬取结束是触发它。</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-scrapy.settings">
<span id="settings-api"></span><span id="topics-api-settings"></span><h4>设置(Settings) API<a class="headerlink" href="#module-scrapy.settings" title="永久链接至标题">¶</a></h4>
<dl class="attribute">
<dt id="scrapy.settings.SETTINGS_PRIORITIES">
<tt class="descclassname">scrapy.settings.</tt><tt class="descname">SETTINGS_PRIORITIES</tt><a class="headerlink" href="#scrapy.settings.SETTINGS_PRIORITIES" title="永久链接至目标">¶</a></dt>
<dd><p>Dictionary that sets the key name and priority level of the default
settings priorities used in Scrapy.</p>
<p>Each item defines a settings entry point, giving it a code name for
identification and an integer priority. Greater priorities take more
precedence over lesser ones when setting and retrieving values in the
<a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><tt class="xref py py-class docutils literal"><span class="pre">Settings</span></tt></a> class.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">SETTINGS_PRIORITIES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">&#39;default&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">&#39;command&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="s">&#39;project&#39;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="s">&#39;cmdline&#39;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>For a detailed explanation on each settings sources, see:
<a class="reference internal" href="index.html#topics-settings"><em>Settings</em></a>.</p>
</dd></dl>

<dl class="class">
<dt id="scrapy.settings.Settings">
<em class="property">class </em><tt class="descclassname">scrapy.settings.</tt><tt class="descname">Settings</tt><big>(</big><em>values={}</em>, <em>priority='project'</em><big>)</big><a class="headerlink" href="#scrapy.settings.Settings" title="永久链接至目标">¶</a></dt>
<dd><p>This object stores Scrapy settings for the configuration of internal
components, and can be used for any further customization.</p>
<p>After instantiation of this class, the new object will have the global
default settings described on <a class="reference internal" href="index.html#topics-settings-ref"><em>内置设定参考手册</em></a> already
populated.</p>
<p>Additional values can be passed on initialization with the <tt class="docutils literal"><span class="pre">values</span></tt>
argument, and they would take the <tt class="docutils literal"><span class="pre">priority</span></tt> level.  If the latter
argument is a string, the priority name will be looked up in
<a class="reference internal" href="index.html#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><tt class="xref py py-attr docutils literal"><span class="pre">SETTINGS_PRIORITIES</span></tt></a>. Otherwise, a expecific
integer should be provided.</p>
<p>Once the object is created, new settings can be loaded or updated with the
<a class="reference internal" href="index.html#scrapy.settings.Settings.set" title="scrapy.settings.Settings.set"><tt class="xref py py-meth docutils literal"><span class="pre">set()</span></tt></a> method, and can be accessed with the
square bracket notation of dictionaries, or with the
<a class="reference internal" href="index.html#scrapy.settings.Settings.get" title="scrapy.settings.Settings.get"><tt class="xref py py-meth docutils literal"><span class="pre">get()</span></tt></a> method of the instance and its value
conversion variants.  When requesting a stored key, the value with the
highest priority will be retrieved.</p>
<dl class="method">
<dt id="scrapy.settings.Settings.set">
<tt class="descname">set</tt><big>(</big><em>name</em>, <em>value</em>, <em>priority='project'</em><big>)</big><a class="headerlink" href="#scrapy.settings.Settings.set" title="永久链接至目标">¶</a></dt>
<dd><p>Store a key/value attribute with a given priority.
Settings should be populated <em>before</em> configuring the Crawler object
(through the <a class="reference internal" href="index.html#scrapy.crawler.Crawler.configure" title="scrapy.crawler.Crawler.configure"><tt class="xref py py-meth docutils literal"><span class="pre">configure()</span></tt></a> method),
otherwise they won&#8217;t have any effect.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) &#8211; the setting name</li>
<li><strong>value</strong> (<em>any</em>) &#8211; the value to associate with the setting</li>
<li><strong>priority</strong> (<em>string or int</em>) &#8211; the priority of the setting. Should be a key of
<a class="reference internal" href="index.html#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><tt class="xref py py-attr docutils literal"><span class="pre">SETTINGS_PRIORITIES</span></tt></a> or an integer</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.Settings.setdict">
<tt class="descname">setdict</tt><big>(</big><em>values</em>, <em>priority='project'</em><big>)</big><a class="headerlink" href="#scrapy.settings.Settings.setdict" title="永久链接至目标">¶</a></dt>
<dd><p>Store key/value pairs with a given priority.</p>
<p>This is a helper function that calls
<a class="reference internal" href="index.html#scrapy.settings.Settings.set" title="scrapy.settings.Settings.set"><tt class="xref py py-meth docutils literal"><span class="pre">set()</span></tt></a> for every item of <tt class="docutils literal"><span class="pre">values</span></tt>
with the provided <tt class="docutils literal"><span class="pre">priority</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>values</strong> (<em>dict</em>) &#8211; the settings names and values</li>
<li><strong>priority</strong> (<em>string or int</em>) &#8211; the priority of the settings. Should be a key of
<a class="reference internal" href="index.html#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><tt class="xref py py-attr docutils literal"><span class="pre">SETTINGS_PRIORITIES</span></tt></a> or an integer</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.Settings.setmodule">
<tt class="descname">setmodule</tt><big>(</big><em>module</em>, <em>priority='project'</em><big>)</big><a class="headerlink" href="#scrapy.settings.Settings.setmodule" title="永久链接至目标">¶</a></dt>
<dd><p>Store settings from a module with a given priority.</p>
<p>This is a helper function that calls
<a class="reference internal" href="index.html#scrapy.settings.Settings.set" title="scrapy.settings.Settings.set"><tt class="xref py py-meth docutils literal"><span class="pre">set()</span></tt></a> for every globally declared
uppercase variable of <tt class="docutils literal"><span class="pre">module</span></tt> with the provided <tt class="docutils literal"><span class="pre">priority</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>module</strong> (<em>module object or string</em>) &#8211; the module or the path of the module</li>
<li><strong>priority</strong> (<em>string or int</em>) &#8211; the priority of the settings. Should be a key of
<a class="reference internal" href="index.html#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><tt class="xref py py-attr docutils literal"><span class="pre">SETTINGS_PRIORITIES</span></tt></a> or an integer</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.Settings.get">
<tt class="descname">get</tt><big>(</big><em>name</em>, <em>default=None</em><big>)</big><a class="headerlink" href="#scrapy.settings.Settings.get" title="永久链接至目标">¶</a></dt>
<dd><p>获取某项配置的值，且不修改其原有的值。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>字符串</em>) &#8211; 配置名</li>
<li><strong>default</strong> (<em>任何</em>) &#8211; 如果没有该项配置时返回的缺省值</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.Settings.getbool">
<tt class="descname">getbool</tt><big>(</big><em>name</em>, <em>default=False</em><big>)</big><a class="headerlink" href="#scrapy.settings.Settings.getbool" title="永久链接至目标">¶</a></dt>
<dd><p>return <tt class="docutils literal"><span class="pre">False</span></tt>
将某项配置的值以布尔值形式返回。比如，<tt class="docutils literal"><span class="pre">1</span></tt> 和 <tt class="docutils literal"><span class="pre">'1'</span></tt>，<tt class="docutils literal"><span class="pre">True</span></tt> 都返回``True``，
而 <tt class="docutils literal"><span class="pre">0</span></tt>，<tt class="docutils literal"><span class="pre">'0'</span></tt>，<tt class="docutils literal"><span class="pre">False</span></tt> 和 <tt class="docutils literal"><span class="pre">None</span></tt> 返回 <tt class="docutils literal"><span class="pre">False</span></tt>。</p>
<p>比如，通过环境变量计算将某项配置设置为 <tt class="docutils literal"><span class="pre">'0'</span></tt>，通过该方法获取得到 <tt class="docutils literal"><span class="pre">False</span></tt>。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>字符串</em>) &#8211; 配置名</li>
<li><strong>default</strong> (<em>任何</em>) &#8211; 如果该配置项未设置，返回的缺省值</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.Settings.getint">
<tt class="descname">getint</tt><big>(</big><em>name</em>, <em>default=0</em><big>)</big><a class="headerlink" href="#scrapy.settings.Settings.getint" title="永久链接至目标">¶</a></dt>
<dd><p>将某项配置的值以整数形式返回</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>字符串</em>) &#8211; 配置名</li>
<li><strong>default</strong> (<em>任何</em>) &#8211; 如果该配置项未设置，返回的缺省值</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.Settings.getfloat">
<tt class="descname">getfloat</tt><big>(</big><em>name</em>, <em>default=0.0</em><big>)</big><a class="headerlink" href="#scrapy.settings.Settings.getfloat" title="永久链接至目标">¶</a></dt>
<dd><p>将某项配置的值以浮点数形式返回</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>字符串</em>) &#8211; 配置名</li>
<li><strong>default</strong> (<em>任何</em>) &#8211; 如果该配置项未设置，返回的缺省值</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.Settings.getlist">
<tt class="descname">getlist</tt><big>(</big><em>name</em>, <em>default=None</em><big>)</big><a class="headerlink" href="#scrapy.settings.Settings.getlist" title="永久链接至目标">¶</a></dt>
<dd><p>将某项配置的值以列表形式返回。如果配置值本来就是list则原样返回。
如果是字符串，则返回被 &#8221;,&#8221; 分割后的列表。</p>
<p>比如，某项值通过环境变量的计算被设置为 <tt class="docutils literal"><span class="pre">'one,two'</span></tt> ，该方法返回[&#8216;one&#8217;, &#8216;two&#8217;]。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>字符串</em>) &#8211; 配置名</li>
<li><strong>default</strong> (<em>任何</em>) &#8211; 如果该配置项未设置，返回的缺省值</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-scrapy.signalmanager">
<span id="signals-api"></span><span id="topics-api-signals"></span><h4>信号(Signals) API<a class="headerlink" href="#module-scrapy.signalmanager" title="永久链接至标题">¶</a></h4>
<dl class="class">
<dt id="scrapy.signalmanager.SignalManager">
<em class="property">class </em><tt class="descclassname">scrapy.signalmanager.</tt><tt class="descname">SignalManager</tt><a class="headerlink" href="#scrapy.signalmanager.SignalManager" title="永久链接至目标">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.signalmanager.SignalManager.connect">
<tt class="descname">connect</tt><big>(</big><em>receiver</em>, <em>signal</em><big>)</big><a class="headerlink" href="#scrapy.signalmanager.SignalManager.connect" title="永久链接至目标">¶</a></dt>
<dd><p>链接一个接收器函数(receiver function) 到一个信号(signal)。</p>
<p>signal可以是任何对象，虽然Scrapy提供了一些预先定义好的信号，
参考文档 <a class="reference internal" href="index.html#topics-signals"><em>信号(Signals)</em></a>。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>receiver</strong> (<em>可调用对象</em>) &#8211; 被链接到的函数</li>
<li><strong>signal</strong> (<em>对象</em>) &#8211; 链接的信号</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.signalmanager.SignalManager.send_catch_log">
<tt class="descname">send_catch_log</tt><big>(</big><em>signal</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.signalmanager.SignalManager.send_catch_log" title="永久链接至目标">¶</a></dt>
<dd><p>发送一个信号，捕获异常并记录日志。</p>
<p>关键字参数会传递给信号处理者(signal handlers)(通过方法 <a class="reference internal" href="index.html#scrapy.signalmanager.SignalManager.connect" title="scrapy.signalmanager.SignalManager.connect"><tt class="xref py py-meth docutils literal"><span class="pre">connect()</span></tt></a> 关联)。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.signalmanager.SignalManager.send_catch_log_deferred">
<tt class="descname">send_catch_log_deferred</tt><big>(</big><em>signal</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.signalmanager.SignalManager.send_catch_log_deferred" title="永久链接至目标">¶</a></dt>
<dd><p>跟 <a class="reference internal" href="index.html#scrapy.signalmanager.SignalManager.send_catch_log" title="scrapy.signalmanager.SignalManager.send_catch_log"><tt class="xref py py-meth docutils literal"><span class="pre">send_catch_log()</span></tt></a> 相似但支持返回 <a class="reference external" href="http://twistedmatrix.com/documents/current/core/howto/defer.html">deferreds</a> 形式的信号处理器。</p>
<p>返回一个 <a class="reference external" href="http://twistedmatrix.com/documents/current/core/howto/defer.html">deferred</a> ，当所有的信号处理器的延迟被触发时调用。
发送一个信号，处理异常并记录日志。</p>
<p>关键字参数会传递给信号处理者(signal handlers)(通过方法 <a class="reference internal" href="index.html#scrapy.signalmanager.SignalManager.connect" title="scrapy.signalmanager.SignalManager.connect"><tt class="xref py py-meth docutils literal"><span class="pre">connect()</span></tt></a> 关联)。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.signalmanager.SignalManager.disconnect">
<tt class="descname">disconnect</tt><big>(</big><em>receiver</em>, <em>signal</em><big>)</big><a class="headerlink" href="#scrapy.signalmanager.SignalManager.disconnect" title="永久链接至目标">¶</a></dt>
<dd><p>解除一个接收器函数和一个信号的关联。这跟方法 <a class="reference internal" href="index.html#scrapy.signalmanager.SignalManager.connect" title="scrapy.signalmanager.SignalManager.connect"><tt class="xref py py-meth docutils literal"><span class="pre">connect()</span></tt></a> 有相反的作用，
参数也相同。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.signalmanager.SignalManager.disconnect_all">
<tt class="descname">disconnect_all</tt><big>(</big><em>signal</em><big>)</big><a class="headerlink" href="#scrapy.signalmanager.SignalManager.disconnect_all" title="永久链接至目标">¶</a></dt>
<dd><p>取消给定信号绑定的所有接收器。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><strong>signal</strong> (<em>object</em>) &#8211; 要取消绑定的信号</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="stats-collector-api">
<span id="topics-api-stats"></span><h4>状态收集器(Stats Collector) API<a class="headerlink" href="#stats-collector-api" title="永久链接至标题">¶</a></h4>
<p>模块 <cite>scrapy.statscol</cite> 下有好几种状态收集器，
它们都实现了状态收集器API对应的类 <tt class="xref py py-class docutils literal"><span class="pre">Statscollector</span></tt> (即它们都继承至该类)。</p>
<span class="target" id="module-scrapy.statscol"></span><dl class="class">
<dt id="scrapy.statscol.StatsCollector">
<em class="property">class </em><tt class="descclassname">scrapy.statscol.</tt><tt class="descname">StatsCollector</tt><a class="headerlink" href="#scrapy.statscol.StatsCollector" title="永久链接至目标">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.statscol.StatsCollector.get_value">
<tt class="descname">get_value</tt><big>(</big><em>key</em>, <em>default=None</em><big>)</big><a class="headerlink" href="#scrapy.statscol.StatsCollector.get_value" title="永久链接至目标">¶</a></dt>
<dd><p>返回指定key的统计值，如果key不存在则返回缺省值。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscol.StatsCollector.get_stats">
<tt class="descname">get_stats</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.statscol.StatsCollector.get_stats" title="永久链接至目标">¶</a></dt>
<dd><p>以dict形式返回当前spider的所有统计值。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscol.StatsCollector.set_value">
<tt class="descname">set_value</tt><big>(</big><em>key</em>, <em>value</em><big>)</big><a class="headerlink" href="#scrapy.statscol.StatsCollector.set_value" title="永久链接至目标">¶</a></dt>
<dd><p>设置key所指定的统计值为value。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscol.StatsCollector.set_stats">
<tt class="descname">set_stats</tt><big>(</big><em>stats</em><big>)</big><a class="headerlink" href="#scrapy.statscol.StatsCollector.set_stats" title="永久链接至目标">¶</a></dt>
<dd><p>使用dict形式的 <tt class="docutils literal"><span class="pre">stats</span></tt> 参数覆盖当前的统计值。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscol.StatsCollector.inc_value">
<tt class="descname">inc_value</tt><big>(</big><em>key</em>, <em>count=1</em>, <em>start=0</em><big>)</big><a class="headerlink" href="#scrapy.statscol.StatsCollector.inc_value" title="永久链接至目标">¶</a></dt>
<dd><p>增加key所对应的统计值，增长值由count指定。
如果key未设置，则使用start的值设置为初始值。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscol.StatsCollector.max_value">
<tt class="descname">max_value</tt><big>(</big><em>key</em>, <em>value</em><big>)</big><a class="headerlink" href="#scrapy.statscol.StatsCollector.max_value" title="永久链接至目标">¶</a></dt>
<dd><p>如果key所对应的当前value小于参数所指定的value，则设置value。
如果没有key所对应的value，设置value。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscol.StatsCollector.min_value">
<tt class="descname">min_value</tt><big>(</big><em>key</em>, <em>value</em><big>)</big><a class="headerlink" href="#scrapy.statscol.StatsCollector.min_value" title="永久链接至目标">¶</a></dt>
<dd><p>如果key所对应的当前value大于参数所指定的value，则设置value。
如果没有key所对应的value，设置value。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscol.StatsCollector.clear_stats">
<tt class="descname">clear_stats</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.statscol.StatsCollector.clear_stats" title="永久链接至目标">¶</a></dt>
<dd><p>清除所有统计信息。</p>
</dd></dl>

<p>以下方法不是统计收集api的一部分，但实现自定义的统计收集器时会使用到：</p>
<dl class="method">
<dt id="scrapy.statscol.StatsCollector.open_spider">
<tt class="descname">open_spider</tt><big>(</big><em>spider</em><big>)</big><a class="headerlink" href="#scrapy.statscol.StatsCollector.open_spider" title="永久链接至目标">¶</a></dt>
<dd><p>打开指定spider进行统计信息收集。</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscol.StatsCollector.close_spider">
<tt class="descname">close_spider</tt><big>(</big><em>spider</em><big>)</big><a class="headerlink" href="#scrapy.statscol.StatsCollector.close_spider" title="永久链接至目标">¶</a></dt>
<dd><p>关闭指定spider。调用后，不能访问和收集统计信息。</p>
</dd></dl>

</dd></dl>

</div>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-topics/architecture"><em>架构概览</em></a></dt>
<dd>了解Scrapy架构。</dd>
<dt><a class="reference internal" href="index.html#document-topics/downloader-middleware"><em>下载器中间件(Downloader Middleware)</em></a></dt>
<dd>自定义页面被请求及下载操作。</dd>
<dt><a class="reference internal" href="index.html#document-topics/spider-middleware"><em>Spider中间件(Middleware)</em></a></dt>
<dd>自定义spider的输入与输出。</dd>
<dt><a class="reference internal" href="index.html#document-topics/extensions"><em>扩展(Extensions)</em></a></dt>
<dd>提供您自定义的功能来扩展Scrapy</dd>
<dt><a class="reference internal" href="index.html#document-topics/api"><em>核心API</em></a></dt>
<dd>在extension(扩展)和middleware(中间件)使用api来扩展Scrapy的功能</dd>
</dl>
</div>
<div class="section" id="id7">
<h2>参考<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-topics/request-response"></span><div class="section" id="module-scrapy.http">
<span id="requests-and-responses"></span><span id="topics-request-response"></span><h3>Requests and Responses<a class="headerlink" href="#module-scrapy.http" title="永久链接至标题">¶</a></h3>
<p>Scrapy uses <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> and <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> objects for crawling web
sites.</p>
<p>Typically, <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> objects are generated in the spiders and pass
across the system until they reach the Downloader, which executes the request
and returns a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object which travels back to the spider that
issued the request.</p>
<p>Both <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> and <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> classes have subclasses which add
functionality not required in the base classes. These are described
below in <a class="reference internal" href="index.html#topics-request-response-ref-request-subclasses"><em>Request subclasses</em></a> and
<a class="reference internal" href="index.html#topics-request-response-ref-response-subclasses"><em>Response subclasses</em></a>.</p>
<div class="section" id="request-objects">
<h4>Request objects<a class="headerlink" href="#request-objects" title="永久链接至标题">¶</a></h4>
<dl class="class">
<dt id="scrapy.http.Request">
<em class="property">class </em><tt class="descclassname">scrapy.http.</tt><tt class="descname">Request</tt><big>(</big><em>url</em><span class="optional">[</span>, <em>callback</em>, <em>method='GET'</em>, <em>headers</em>, <em>body</em>, <em>cookies</em>, <em>meta</em>, <em>encoding='utf-8'</em>, <em>priority=0</em>, <em>dont_filter=False</em>, <em>errback</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.Request" title="永久链接至目标">¶</a></dt>
<dd><p>A <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object represents an HTTP request, which is usually
generated in the Spider and executed by the Downloader, and thus generating
a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>url</strong> (<em>string</em>) &#8211; the URL of this request</li>
<li><strong>callback</strong> (<em>callable</em>) &#8211; the function that will be called with the response of this
request (once its downloaded) as its first parameter. For more information
see <a class="reference internal" href="index.html#topics-request-response-ref-request-callback-arguments"><em>Passing additional data to callback functions</em></a> below.
If a Request doesn&#8217;t specify a callback, the spider&#8217;s
<a class="reference internal" href="index.html#scrapy.spider.Spider.parse" title="scrapy.spider.Spider.parse"><tt class="xref py py-meth docutils literal"><span class="pre">parse()</span></tt></a> method will be used.
Note that if exceptions are raised during processing, errback is called instead.</li>
<li><strong>method</strong> (<em>string</em>) &#8211; the HTTP method of this request. Defaults to <tt class="docutils literal"><span class="pre">'GET'</span></tt>.</li>
<li><strong>meta</strong> (<em>dict</em>) &#8211; the initial values for the <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> attribute. If
given, the dict passed in this parameter will be shallow copied.</li>
<li><strong>body</strong> (<em>str or unicode</em>) &#8211; the request body. If a <tt class="docutils literal"><span class="pre">unicode</span></tt> is passed, then it&#8217;s encoded to
<tt class="docutils literal"><span class="pre">str</span></tt> using the <cite>encoding</cite> passed (which defaults to <tt class="docutils literal"><span class="pre">utf-8</span></tt>). If
<tt class="docutils literal"><span class="pre">body</span></tt> is not given,, an empty string is stored. Regardless of the
type of this argument, the final value stored will be a <tt class="docutils literal"><span class="pre">str</span></tt> (never
<tt class="docutils literal"><span class="pre">unicode</span></tt> or <tt class="docutils literal"><span class="pre">None</span></tt>).</li>
<li><strong>headers</strong> (<em>dict</em>) &#8211; the headers of this request. The dict values can be strings
(for single valued headers) or lists (for multi-valued headers). If
<tt class="docutils literal"><span class="pre">None</span></tt> is passed as value, the HTTP header will not be sent at all.</li>
<li><strong>cookies</strong> (<em>dict or list</em>) &#8211; <p>the request cookies. These can be sent in two forms.</p>
<ol class="arabic">
<li>Using a dict:<div class="highlight-python"><div class="highlight"><pre><span class="n">request_with_cookies</span> <span class="o">=</span> <span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s">&quot;http://www.example.com&quot;</span><span class="p">,</span>
                               <span class="n">cookies</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;currency&#39;</span><span class="p">:</span> <span class="s">&#39;USD&#39;</span><span class="p">,</span> <span class="s">&#39;country&#39;</span><span class="p">:</span> <span class="s">&#39;UY&#39;</span><span class="p">})</span>
</pre></div>
</div>
</li>
<li>Using a list of dicts:<div class="highlight-python"><div class="highlight"><pre><span class="n">request_with_cookies</span> <span class="o">=</span> <span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s">&quot;http://www.example.com&quot;</span><span class="p">,</span>
                               <span class="n">cookies</span><span class="o">=</span><span class="p">[{</span><span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">&#39;currency&#39;</span><span class="p">,</span>
                                        <span class="s">&#39;value&#39;</span><span class="p">:</span> <span class="s">&#39;USD&#39;</span><span class="p">,</span>
                                        <span class="s">&#39;domain&#39;</span><span class="p">:</span> <span class="s">&#39;example.com&#39;</span><span class="p">,</span>
                                        <span class="s">&#39;path&#39;</span><span class="p">:</span> <span class="s">&#39;/currency&#39;</span><span class="p">}])</span>
</pre></div>
</div>
</li>
</ol>
<p>The latter form allows for customizing the <tt class="docutils literal"><span class="pre">domain</span></tt> and <tt class="docutils literal"><span class="pre">path</span></tt>
attributes of the cookie. This is only useful if the cookies are saved
for later requests.</p>
<p>When some site returns cookies (in a response) those are stored in the
cookies for that domain and will be sent again in future requests. That&#8217;s
the typical behaviour of any regular web browser. However, if, for some
reason, you want to avoid merging with existing cookies you can instruct
Scrapy to do so by setting the <tt class="docutils literal"><span class="pre">dont_merge_cookies</span></tt> key in the
<a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a>.</p>
<p>Example of request without merging cookies:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">request_with_cookies</span> <span class="o">=</span> <span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s">&quot;http://www.example.com&quot;</span><span class="p">,</span>
                               <span class="n">cookies</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;currency&#39;</span><span class="p">:</span> <span class="s">&#39;USD&#39;</span><span class="p">,</span> <span class="s">&#39;country&#39;</span><span class="p">:</span> <span class="s">&#39;UY&#39;</span><span class="p">},</span>
                               <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;dont_merge_cookies&#39;</span><span class="p">:</span> <span class="bp">True</span><span class="p">})</span>
</pre></div>
</div>
<p>For more info see <a class="reference internal" href="index.html#cookies-mw"><em>CookiesMiddleware</em></a>.</p>
</li>
<li><strong>encoding</strong> (<em>string</em>) &#8211; the encoding of this request (defaults to <tt class="docutils literal"><span class="pre">'utf-8'</span></tt>).
This encoding will be used to percent-encode the URL and to convert the
body to <tt class="docutils literal"><span class="pre">str</span></tt> (if given as <tt class="docutils literal"><span class="pre">unicode</span></tt>).</li>
<li><strong>priority</strong> (<em>int</em>) &#8211; the priority of this request (defaults to <tt class="docutils literal"><span class="pre">0</span></tt>).
The priority is used by the scheduler to define the order used to process
requests.  Requests with a higher priority value will execute earlier.
Negative values are allowed in order to indicate relatively low-priority.</li>
<li><strong>dont_filter</strong> (<em>boolean</em>) &#8211; indicates that this request should not be filtered by
the scheduler. This is used when you want to perform an identical
request multiple times, to ignore the duplicates filter. Use it with
care, or you will get into crawling loops. Default to <tt class="docutils literal"><span class="pre">False</span></tt>.</li>
<li><strong>errback</strong> (<em>callable</em>) &#8211; a function that will be called if any exception was
raised while processing the request. This includes pages that failed
with 404 HTTP errors and such. It receives a <a class="reference external" href="http://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Twisted Failure</a> instance
as first parameter.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="scrapy.http.Request.url">
<tt class="descname">url</tt><a class="headerlink" href="#scrapy.http.Request.url" title="永久链接至目标">¶</a></dt>
<dd><p>A string containing the URL of this request. Keep in mind that this
attribute contains the escaped URL, so it can differ from the URL passed in
the constructor.</p>
<p>This attribute is read-only. To change the URL of a Request use
<a class="reference internal" href="index.html#scrapy.http.Request.replace" title="scrapy.http.Request.replace"><tt class="xref py py-meth docutils literal"><span class="pre">replace()</span></tt></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Request.method">
<tt class="descname">method</tt><a class="headerlink" href="#scrapy.http.Request.method" title="永久链接至目标">¶</a></dt>
<dd><p>A string representing the HTTP method in the request. This is guaranteed to
be uppercase. Example: <tt class="docutils literal"><span class="pre">&quot;GET&quot;</span></tt>, <tt class="docutils literal"><span class="pre">&quot;POST&quot;</span></tt>, <tt class="docutils literal"><span class="pre">&quot;PUT&quot;</span></tt>, etc</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Request.headers">
<tt class="descname">headers</tt><a class="headerlink" href="#scrapy.http.Request.headers" title="永久链接至目标">¶</a></dt>
<dd><p>A dictionary-like object which contains the request headers.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Request.body">
<tt class="descname">body</tt><a class="headerlink" href="#scrapy.http.Request.body" title="永久链接至目标">¶</a></dt>
<dd><p>A str that contains the request body.</p>
<p>This attribute is read-only. To change the body of a Request use
<a class="reference internal" href="index.html#scrapy.http.Request.replace" title="scrapy.http.Request.replace"><tt class="xref py py-meth docutils literal"><span class="pre">replace()</span></tt></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Request.meta">
<tt class="descname">meta</tt><a class="headerlink" href="#scrapy.http.Request.meta" title="永久链接至目标">¶</a></dt>
<dd><p>A dict that contains arbitrary metadata for this request. This dict is
empty for new Requests, and is usually  populated by different Scrapy
components (extensions, middlewares, etc). So the data contained in this
dict depends on the extensions you have enabled.</p>
<p>See <a class="reference internal" href="index.html#topics-request-meta"><em>Request.meta special keys</em></a> for a list of special meta keys
recognized by Scrapy.</p>
<p>This dict is <a class="reference external" href="http://docs.python.org/library/copy.html">shallow copied</a> when the request is cloned using the
<tt class="docutils literal"><span class="pre">copy()</span></tt> or <tt class="docutils literal"><span class="pre">replace()</span></tt> methods, and can also be accessed, in your
spider, from the <tt class="docutils literal"><span class="pre">response.meta</span></tt> attribute.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Request.copy">
<tt class="descname">copy</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.http.Request.copy" title="永久链接至目标">¶</a></dt>
<dd><p>Return a new Request which is a copy of this Request. See also:
<a class="reference internal" href="index.html#topics-request-response-ref-request-callback-arguments"><em>Passing additional data to callback functions</em></a>.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Request.replace">
<tt class="descname">replace</tt><big>(</big><span class="optional">[</span><em>url</em>, <em>method</em>, <em>headers</em>, <em>body</em>, <em>cookies</em>, <em>meta</em>, <em>encoding</em>, <em>dont_filter</em>, <em>callback</em>, <em>errback</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.Request.replace" title="永久链接至目标">¶</a></dt>
<dd><p>Return a Request object with the same members, except for those members
given new values by whichever keyword arguments are specified. The
attribute <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> is copied by default (unless a new value
is given in the <tt class="docutils literal"><span class="pre">meta</span></tt> argument). See also
<a class="reference internal" href="index.html#topics-request-response-ref-request-callback-arguments"><em>Passing additional data to callback functions</em></a>.</p>
</dd></dl>

</dd></dl>

<div class="section" id="passing-additional-data-to-callback-functions">
<span id="topics-request-response-ref-request-callback-arguments"></span><h5>Passing additional data to callback functions<a class="headerlink" href="#passing-additional-data-to-callback-functions" title="永久链接至标题">¶</a></h5>
<p>The callback of a request is a function that will be called when the response
of that request is downloaded. The callback function will be called with the
downloaded <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object as its first argument.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">parse_page1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s">&quot;http://www.example.com/some_page.html&quot;</span><span class="p">,</span>
                          <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_page2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">parse_page2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="c"># this would log http://www.example.com/some_page.html</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s">&quot;Visited </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
<p>In some cases you may be interested in passing arguments to those callback
functions so you can receive the arguments later, in the second callback. You
can use the <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> attribute for that.</p>
<p>Here&#8217;s an example of how to pass an item using this mechanism, to populate
different fields from different pages:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">parse_page1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="n">item</span> <span class="o">=</span> <span class="n">MyItem</span><span class="p">()</span>
    <span class="n">item</span><span class="p">[</span><span class="s">&#39;main_url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span>
    <span class="n">request</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s">&quot;http://www.example.com/some_page.html&quot;</span><span class="p">,</span>
                             <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_page2</span><span class="p">)</span>
    <span class="n">request</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s">&#39;item&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span>
    <span class="k">return</span> <span class="n">request</span>

<span class="k">def</span> <span class="nf">parse_page2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="n">item</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s">&#39;item&#39;</span><span class="p">]</span>
    <span class="n">item</span><span class="p">[</span><span class="s">&#39;other_url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span>
    <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="request-meta-special-keys">
<span id="topics-request-meta"></span><h4>Request.meta special keys<a class="headerlink" href="#request-meta-special-keys" title="永久链接至标题">¶</a></h4>
<p>The <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> attribute can contain any arbitrary data, but there
are some special keys recognized by Scrapy and its built-in extensions.</p>
<p>Those are:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:reqmeta-dont_redirect"><tt class="xref std std-reqmeta docutils literal"><span class="pre">dont_redirect</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-dont_retry"><tt class="xref std std-reqmeta docutils literal"><span class="pre">dont_retry</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-handle_httpstatus_list"><tt class="xref std std-reqmeta docutils literal"><span class="pre">handle_httpstatus_list</span></tt></a></li>
<li><tt class="docutils literal"><span class="pre">dont_merge_cookies</span></tt> (see <tt class="docutils literal"><span class="pre">cookies</span></tt> parameter of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> constructor)</li>
<li><a class="reference internal" href="index.html#std:reqmeta-cookiejar"><tt class="xref std std-reqmeta docutils literal"><span class="pre">cookiejar</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-redirect_urls"><tt class="xref std std-reqmeta docutils literal"><span class="pre">redirect_urls</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-bindaddress"><tt class="xref std std-reqmeta docutils literal"><span class="pre">bindaddress</span></tt></a></li>
</ul>
<div class="section" id="bindaddress">
<span id="std:reqmeta-bindaddress"></span><h5>bindaddress<a class="headerlink" href="#bindaddress" title="永久链接至标题">¶</a></h5>
<p>The IP of the outgoing IP address to use for the performing the request.</p>
</div>
</div>
<div class="section" id="request-subclasses">
<span id="topics-request-response-ref-request-subclasses"></span><h4>Request subclasses<a class="headerlink" href="#request-subclasses" title="永久链接至标题">¶</a></h4>
<p>Here is the list of built-in <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> subclasses. You can also subclass
it to implement your own custom functionality.</p>
<div class="section" id="formrequest-objects">
<h5>FormRequest objects<a class="headerlink" href="#formrequest-objects" title="永久链接至标题">¶</a></h5>
<p>The FormRequest class extends the base <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> with functionality for
dealing with HTML forms. It uses <a class="reference external" href="http://lxml.de/lxmlhtml.html#forms">lxml.html forms</a>  to pre-populate form
fields with form data from <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> objects.</p>
<dl class="class">
<dt id="scrapy.http.FormRequest">
<em class="property">class </em><tt class="descclassname">scrapy.http.</tt><tt class="descname">FormRequest</tt><big>(</big><em>url</em><span class="optional">[</span>, <em>formdata</em>, <em>...</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.FormRequest" title="永久链接至目标">¶</a></dt>
<dd><p>The <a class="reference internal" href="index.html#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><tt class="xref py py-class docutils literal"><span class="pre">FormRequest</span></tt></a> class adds a new argument to the constructor. The
remaining arguments are the same as for the <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> class and are
not documented here.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><strong>formdata</strong> (<em>dict or iterable of tuples</em>) &#8211; is a dictionary (or iterable of (key, value) tuples)
containing HTML Form data which will be url-encoded and assigned to the
body of the request.</td>
</tr>
</tbody>
</table>
<p>The <a class="reference internal" href="index.html#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><tt class="xref py py-class docutils literal"><span class="pre">FormRequest</span></tt></a> objects support the following class method in
addition to the standard <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> methods:</p>
<dl class="classmethod">
<dt id="scrapy.http.FormRequest.from_response">
<em class="property">classmethod </em><tt class="descname">from_response</tt><big>(</big><em>response</em><span class="optional">[</span>, <em>formname=None</em>, <em>formnumber=0</em>, <em>formdata=None</em>, <em>formxpath=None</em>, <em>clickdata=None</em>, <em>dont_click=False</em>, <em>...</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.FormRequest.from_response" title="永久链接至目标">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="index.html#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><tt class="xref py py-class docutils literal"><span class="pre">FormRequest</span></tt></a> object with its form field values
pre-populated with those found in the HTML <tt class="docutils literal"><span class="pre">&lt;form&gt;</span></tt> element contained
in the given response. For an example see
<a class="reference internal" href="index.html#topics-request-response-ref-request-userlogin"><em>使用FormRequest.from_response()方法模拟用户登录</em></a>.</p>
<p>The policy is to automatically simulate a click, by default, on any form
control that looks clickable, like a <tt class="docutils literal"><span class="pre">&lt;input</span> <span class="pre">type=&quot;submit&quot;&gt;</span></tt>.  Even
though this is quite convenient, and often the desired behaviour,
sometimes it can cause problems which could be hard to debug. For
example, when working with forms that are filled and/or submitted using
javascript, the default <a class="reference internal" href="index.html#scrapy.http.FormRequest.from_response" title="scrapy.http.FormRequest.from_response"><tt class="xref py py-meth docutils literal"><span class="pre">from_response()</span></tt></a> behaviour may not be the
most appropriate. To disable this behaviour you can set the
<tt class="docutils literal"><span class="pre">dont_click</span></tt> argument to <tt class="docutils literal"><span class="pre">True</span></tt>. Also, if you want to change the
control clicked (instead of disabling it) you can also use the
<tt class="docutils literal"><span class="pre">clickdata</span></tt> argument.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object) &#8211; the response containing a HTML form which will be used
to pre-populate the form fields</li>
<li><strong>formname</strong> (<em>string</em>) &#8211; if given, the form with name attribute set to this value will be used.</li>
<li><strong>formxpath</strong> (<em>string</em>) &#8211; if given, the first form that matches the xpath will be used.</li>
<li><strong>formnumber</strong> (<em>integer</em>) &#8211; the number of form to use, when the response contains
multiple forms. The first one (and also the default) is <tt class="docutils literal"><span class="pre">0</span></tt>.</li>
<li><strong>formdata</strong> (<em>dict</em>) &#8211; fields to override in the form data. If a field was
already present in the response <tt class="docutils literal"><span class="pre">&lt;form&gt;</span></tt> element, its value is
overridden by the one passed in this parameter.</li>
<li><strong>clickdata</strong> (<em>dict</em>) &#8211; attributes to lookup the control clicked. If it&#8217;s not
given, the form data will be submitted simulating a click on the
first clickable element. In addition to html attributes, the control
can be identified by its zero-based index relative to other
submittable inputs inside the form, via the <tt class="docutils literal"><span class="pre">nr</span></tt> attribute.</li>
<li><strong>dont_click</strong> (<em>boolean</em>) &#8211; If True, the form data will be submitted without
clicking in any element.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>The other parameters of this class method are passed directly to the
<a class="reference internal" href="index.html#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><tt class="xref py py-class docutils literal"><span class="pre">FormRequest</span></tt></a> constructor.</p>
<div class="versionadded">
<p><span class="versionmodified">0.10.3 新版功能: </span>The <tt class="docutils literal"><span class="pre">formname</span></tt> parameter.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">0.17 新版功能: </span>The <tt class="docutils literal"><span class="pre">formxpath</span></tt> parameter.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="request-usage-examples">
<h5>Request usage examples<a class="headerlink" href="#request-usage-examples" title="永久链接至标题">¶</a></h5>
<div class="section" id="using-formrequest-to-send-data-via-http-post">
<h6>Using FormRequest to send data via HTTP POST<a class="headerlink" href="#using-formrequest-to-send-data-via-http-post" title="永久链接至标题">¶</a></h6>
<p>If you want to simulate a HTML Form POST in your spider and send a couple of
key-value fields, you can return a <a class="reference internal" href="index.html#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><tt class="xref py py-class docutils literal"><span class="pre">FormRequest</span></tt></a> object (from your
spider) like this:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">return</span> <span class="p">[</span><span class="n">FormRequest</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s">&quot;http://www.example.com/post/action&quot;</span><span class="p">,</span>
                    <span class="n">formdata</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">&#39;John Doe&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">:</span> <span class="s">&#39;27&#39;</span><span class="p">},</span>
                    <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">after_post</span><span class="p">)]</span>
</pre></div>
</div>
</div>
<div class="section" id="formrequest-from-response">
<span id="topics-request-response-ref-request-userlogin"></span><h6>使用FormRequest.from_response()方法模拟用户登录<a class="headerlink" href="#formrequest-from-response" title="永久链接至标题">¶</a></h6>
<p>通常网站通过 <tt class="docutils literal"><span class="pre">&lt;input</span> <span class="pre">type=&quot;hidden&quot;&gt;</span></tt> 实现对某些表单字段（如数据或是登录界面中的认证令牌等）的预填充。
使用Scrapy抓取网页时，如果想要预填充或重写像用户名、用户密码这些表单字段，
可以使用 <a class="reference internal" href="index.html#scrapy.http.FormRequest.from_response" title="scrapy.http.FormRequest.from_response"><tt class="xref py py-meth docutils literal"><span class="pre">FormRequest.from_response()</span></tt></a> 方法实现。下面是使用这种方法的爬虫例子:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">LoginSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;example.com&#39;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.example.com/users/login.php&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">FormRequest</span><span class="o">.</span><span class="n">from_response</span><span class="p">(</span>
            <span class="n">response</span><span class="p">,</span>
            <span class="n">formdata</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;username&#39;</span><span class="p">:</span> <span class="s">&#39;john&#39;</span><span class="p">,</span> <span class="s">&#39;password&#39;</span><span class="p">:</span> <span class="s">&#39;secret&#39;</span><span class="p">},</span>
            <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">after_login</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">after_login</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c"># check login succeed before going on</span>
        <span class="k">if</span> <span class="s">&quot;authentication failed&quot;</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s">&quot;Login failed&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">log</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="c"># continue scraping with authenticated session...</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="response-objects">
<h4>Response objects<a class="headerlink" href="#response-objects" title="永久链接至标题">¶</a></h4>
<dl class="class">
<dt id="scrapy.http.Response">
<em class="property">class </em><tt class="descclassname">scrapy.http.</tt><tt class="descname">Response</tt><big>(</big><em>url</em><span class="optional">[</span>, <em>status=200</em>, <em>headers</em>, <em>body</em>, <em>flags</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.Response" title="永久链接至目标">¶</a></dt>
<dd><p>A <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object represents an HTTP response, which is usually
downloaded (by the Downloader) and fed to the Spiders for processing.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>url</strong> (<em>string</em>) &#8211; the URL of this response</li>
<li><strong>headers</strong> (<em>dict</em>) &#8211; the headers of this response. The dict values can be strings
(for single valued headers) or lists (for multi-valued headers).</li>
<li><strong>status</strong> (<em>integer</em>) &#8211; the HTTP status of the response. Defaults to <tt class="docutils literal"><span class="pre">200</span></tt>.</li>
<li><strong>body</strong> (<em>str</em>) &#8211; the response body. It must be str, not unicode, unless you&#8217;re
using a encoding-aware <a class="reference internal" href="index.html#topics-request-response-ref-response-subclasses"><em>Response subclass</em></a>, such as
<a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a>.</li>
<li><strong>meta</strong> (<em>dict</em>) &#8211; the initial values for the <a class="reference internal" href="index.html#scrapy.http.Response.meta" title="scrapy.http.Response.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Response.meta</span></tt></a> attribute. If
given, the dict will be shallow copied.</li>
<li><strong>flags</strong> (<em>list</em>) &#8211; is a list containing the initial values for the
<a class="reference internal" href="index.html#scrapy.http.Response.flags" title="scrapy.http.Response.flags"><tt class="xref py py-attr docutils literal"><span class="pre">Response.flags</span></tt></a> attribute. If given, the list will be shallow
copied.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="scrapy.http.Response.url">
<tt class="descname">url</tt><a class="headerlink" href="#scrapy.http.Response.url" title="永久链接至目标">¶</a></dt>
<dd><p>A string containing the URL of the response.</p>
<p>This attribute is read-only. To change the URL of a Response use
<a class="reference internal" href="index.html#scrapy.http.Response.replace" title="scrapy.http.Response.replace"><tt class="xref py py-meth docutils literal"><span class="pre">replace()</span></tt></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.status">
<tt class="descname">status</tt><a class="headerlink" href="#scrapy.http.Response.status" title="永久链接至目标">¶</a></dt>
<dd><p>An integer representing the HTTP status of the response. Example: <tt class="docutils literal"><span class="pre">200</span></tt>,
<tt class="docutils literal"><span class="pre">404</span></tt>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.headers">
<tt class="descname">headers</tt><a class="headerlink" href="#scrapy.http.Response.headers" title="永久链接至目标">¶</a></dt>
<dd><p>A dictionary-like object which contains the response headers.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.body">
<tt class="descname">body</tt><a class="headerlink" href="#scrapy.http.Response.body" title="永久链接至目标">¶</a></dt>
<dd><p>A str containing the body of this Response. Keep in mind that Response.body
is always a str. If you want the unicode version use
<a class="reference internal" href="index.html#scrapy.http.TextResponse.body_as_unicode" title="scrapy.http.TextResponse.body_as_unicode"><tt class="xref py py-meth docutils literal"><span class="pre">TextResponse.body_as_unicode()</span></tt></a> (only available in
<a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> and subclasses).</p>
<p>This attribute is read-only. To change the body of a Response use
<a class="reference internal" href="index.html#scrapy.http.Response.replace" title="scrapy.http.Response.replace"><tt class="xref py py-meth docutils literal"><span class="pre">replace()</span></tt></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.request">
<tt class="descname">request</tt><a class="headerlink" href="#scrapy.http.Response.request" title="永久链接至目标">¶</a></dt>
<dd><p>The <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object that generated this response. This attribute is
assigned in the Scrapy engine, after the response and the request have passed
through all <a class="reference internal" href="index.html#topics-downloader-middleware"><em>Downloader Middlewares</em></a>.
In particular, this means that:</p>
<ul class="simple">
<li>HTTP redirections will cause the original request (to the URL before
redirection) to be assigned to the redirected response (with the final
URL after redirection).</li>
<li>Response.request.url doesn&#8217;t always equal Response.url</li>
<li>This attribute is only available in the spider code, and in the
<a class="reference internal" href="index.html#topics-spider-middleware"><em>Spider Middlewares</em></a>, but not in
Downloader Middlewares (although you have the Request available there by
other means) and handlers of the <a class="reference internal" href="index.html#std:signal-response_downloaded"><tt class="xref std std-signal docutils literal"><span class="pre">response_downloaded</span></tt></a> signal.</li>
</ul>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.meta">
<tt class="descname">meta</tt><a class="headerlink" href="#scrapy.http.Response.meta" title="永久链接至目标">¶</a></dt>
<dd><p>A shortcut to the <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> attribute of the
<a class="reference internal" href="index.html#scrapy.http.Response.request" title="scrapy.http.Response.request"><tt class="xref py py-attr docutils literal"><span class="pre">Response.request</span></tt></a> object (ie. <tt class="docutils literal"><span class="pre">self.request.meta</span></tt>).</p>
<p>Unlike the <a class="reference internal" href="index.html#scrapy.http.Response.request" title="scrapy.http.Response.request"><tt class="xref py py-attr docutils literal"><span class="pre">Response.request</span></tt></a> attribute, the <a class="reference internal" href="index.html#scrapy.http.Response.meta" title="scrapy.http.Response.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Response.meta</span></tt></a>
attribute is propagated along redirects and retries, so you will get
the original <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> sent from your spider.</p>
<div class="admonition seealso">
<p class="first admonition-title">参见</p>
<p class="last"><a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> attribute</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.flags">
<tt class="descname">flags</tt><a class="headerlink" href="#scrapy.http.Response.flags" title="永久链接至目标">¶</a></dt>
<dd><p>A list that contains flags for this response. Flags are labels used for
tagging Responses. For example: <cite>&#8216;cached&#8217;</cite>, <cite>&#8216;redirected</cite>&#8216;, etc. And
they&#8217;re shown on the string representation of the Response (<cite>__str__</cite>
method) which is used by the engine for logging.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Response.copy">
<tt class="descname">copy</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.http.Response.copy" title="永久链接至目标">¶</a></dt>
<dd><p>Returns a new Response which is a copy of this Response.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Response.replace">
<tt class="descname">replace</tt><big>(</big><span class="optional">[</span><em>url</em>, <em>status</em>, <em>headers</em>, <em>body</em>, <em>request</em>, <em>flags</em>, <em>cls</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.Response.replace" title="永久链接至目标">¶</a></dt>
<dd><p>Returns a Response object with the same members, except for those members
given new values by whichever keyword arguments are specified. The
attribute <a class="reference internal" href="index.html#scrapy.http.Response.meta" title="scrapy.http.Response.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Response.meta</span></tt></a> is copied by default.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="response-subclasses">
<span id="topics-request-response-ref-response-subclasses"></span><h4>Response subclasses<a class="headerlink" href="#response-subclasses" title="永久链接至标题">¶</a></h4>
<p>Here is the list of available built-in Response subclasses. You can also
subclass the Response class to implement your own functionality.</p>
<div class="section" id="textresponse-objects">
<h5>TextResponse objects<a class="headerlink" href="#textresponse-objects" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.http.TextResponse">
<em class="property">class </em><tt class="descclassname">scrapy.http.</tt><tt class="descname">TextResponse</tt><big>(</big><em>url</em><span class="optional">[</span>, <em>encoding</em><span class="optional">[</span>, <em>...</em><span class="optional">]</span><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.TextResponse" title="永久链接至目标">¶</a></dt>
<dd><p><a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> objects adds encoding capabilities to the base
<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> class, which is meant to be used only for binary data,
such as images, sounds or any media file.</p>
<p><a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> objects support a new constructor argument, in
addition to the base <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> objects. The remaining functionality
is the same as for the <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> class and is not documented here.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><strong>encoding</strong> (<em>string</em>) &#8211; is a string which contains the encoding to use for this
response. If you create a <a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> object with a unicode
body, it will be encoded using this encoding (remember the body attribute
is always a string). If <tt class="docutils literal"><span class="pre">encoding</span></tt> is <tt class="docutils literal"><span class="pre">None</span></tt> (default value), the
encoding will be looked up in the response headers and body instead.</td>
</tr>
</tbody>
</table>
<p><a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> objects support the following attributes in addition
to the standard <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> ones:</p>
<dl class="attribute">
<dt id="scrapy.http.TextResponse.encoding">
<tt class="descname">encoding</tt><a class="headerlink" href="#scrapy.http.TextResponse.encoding" title="永久链接至目标">¶</a></dt>
<dd><p>A string with the encoding of this response. The encoding is resolved by
trying the following mechanisms, in order:</p>
<ol class="arabic simple">
<li>the encoding passed in the constructor <cite>encoding</cite> argument</li>
<li>the encoding declared in the Content-Type HTTP header. If this
encoding is not valid (ie. unknown), it is ignored and the next
resolution mechanism is tried.</li>
<li>the encoding declared in the response body. The TextResponse class
doesn&#8217;t provide any special functionality for this. However, the
<a class="reference internal" href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">HtmlResponse</span></tt></a> and <a class="reference internal" href="index.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">XmlResponse</span></tt></a> classes do.</li>
<li>the encoding inferred by looking at the response body. This is the more
fragile method but also the last one tried.</li>
</ol>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.TextResponse.selector">
<tt class="descname">selector</tt><a class="headerlink" href="#scrapy.http.TextResponse.selector" title="永久链接至目标">¶</a></dt>
<dd><p>A <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> instance using the response as
target. The selector is lazily instantiated on first access.</p>
</dd></dl>

<p><a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> objects support the following methods in addition to
the standard <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> ones:</p>
<dl class="method">
<dt id="scrapy.http.TextResponse.body_as_unicode">
<tt class="descname">body_as_unicode</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.http.TextResponse.body_as_unicode" title="永久链接至目标">¶</a></dt>
<dd><p>Returns the body of the response as unicode. This is equivalent to:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">encoding</span><span class="p">)</span>
</pre></div>
</div>
<p>But <strong>not</strong> equivalent to:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="nb">unicode</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">)</span>
</pre></div>
</div>
<p>Since, in the latter case, you would be using you system default encoding
(typically <cite>ascii</cite>) to convert the body to unicode, instead of the response
encoding.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.TextResponse.xpath">
<tt class="descname">xpath</tt><big>(</big><em>query</em><big>)</big><a class="headerlink" href="#scrapy.http.TextResponse.xpath" title="永久链接至目标">¶</a></dt>
<dd><p>A shortcut to <tt class="docutils literal"><span class="pre">TextResponse.selector.xpath(query)</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//p&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.TextResponse.css">
<tt class="descname">css</tt><big>(</big><em>query</em><big>)</big><a class="headerlink" href="#scrapy.http.TextResponse.css" title="永久链接至目标">¶</a></dt>
<dd><p>A shortcut to <tt class="docutils literal"><span class="pre">TextResponse.selector.css(query)</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s">&#39;p&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="htmlresponse-objects">
<h5>HtmlResponse objects<a class="headerlink" href="#htmlresponse-objects" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.http.HtmlResponse">
<em class="property">class </em><tt class="descclassname">scrapy.http.</tt><tt class="descname">HtmlResponse</tt><big>(</big><em>url</em><span class="optional">[</span>, <em>...</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.HtmlResponse" title="永久链接至目标">¶</a></dt>
<dd><p>The <a class="reference internal" href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">HtmlResponse</span></tt></a> class is a subclass of <a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a>
which adds encoding auto-discovering support by looking into the HTML <a class="reference external" href="http://www.w3schools.com/TAGS/att_meta_http_equiv.asp">meta
http-equiv</a> attribute.  See <a class="reference internal" href="index.html#scrapy.http.TextResponse.encoding" title="scrapy.http.TextResponse.encoding"><tt class="xref py py-attr docutils literal"><span class="pre">TextResponse.encoding</span></tt></a>.</p>
</dd></dl>

</div>
<div class="section" id="xmlresponse-objects">
<h5>XmlResponse objects<a class="headerlink" href="#xmlresponse-objects" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.http.XmlResponse">
<em class="property">class </em><tt class="descclassname">scrapy.http.</tt><tt class="descname">XmlResponse</tt><big>(</big><em>url</em><span class="optional">[</span>, <em>...</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.XmlResponse" title="永久链接至目标">¶</a></dt>
<dd><p>The <a class="reference internal" href="index.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">XmlResponse</span></tt></a> class is a subclass of <a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> which
adds encoding auto-discovering support by looking into the XML declaration
line.  See <a class="reference internal" href="index.html#scrapy.http.TextResponse.encoding" title="scrapy.http.TextResponse.encoding"><tt class="xref py py-attr docutils literal"><span class="pre">TextResponse.encoding</span></tt></a>.</p>
</dd></dl>

</div>
</div>
</div>
<span id="document-topics/settings"></span><div class="section" id="settings">
<span id="topics-settings"></span><h3>Settings<a class="headerlink" href="#settings" title="永久链接至标题">¶</a></h3>
<p>Scrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。</p>
<p>设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。
设定可以通过下面介绍的多种机制进行设置。</p>
<p>设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。</p>
<p>内置设定列表请参考 <a class="reference internal" href="index.html#topics-settings-ref"><em>内置设定参考手册</em></a> 。</p>
<div class="section" id="designating-the-settings">
<h4>指定设定(Designating the settings)<a class="headerlink" href="#designating-the-settings" title="永久链接至标题">¶</a></h4>
<p>当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:
<tt class="docutils literal"><span class="pre">SCRAPY_SETTINGS_MODULE</span></tt> 来完成。</p>
<p><tt class="docutils literal"><span class="pre">SCRAPY_SETTINGS_MODULE</span></tt> 必须以Python路径语法编写, 如 <tt class="docutils literal"><span class="pre">myproject.settings</span></tt> 。
注意，设定模块应该在 Python <a class="reference external" href="http://docs.python.org/2/tutorial/modules.html#the-module-search-path">import search path</a> 中。</p>
</div>
<div class="section" id="populating-the-settings">
<h4>获取设定值(Populating the settings)<a class="headerlink" href="#populating-the-settings" title="永久链接至标题">¶</a></h4>
<p>设定可以通过多种方式设置，每个方式具有不同的优先级。
下面以优先级降序的方式给出方式列表:</p>
<blockquote>
<div><ol class="arabic simple">
<li>命令行选项(Command line Options)(最高优先级)</li>
<li>项目设定模块(Project settings module)</li>
<li>命令默认设定模块(Default settings per-command)</li>
<li>全局默认设定(Default global settings) (最低优先级)</li>
</ol>
</div></blockquote>
<p>这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。
详情请参考 <a class="reference internal" href="index.html#topics-api-settings"><em>设置(Settings) API</em></a>.</p>
<p>这些机制将在下面详细介绍。</p>
<div class="section" id="command-line-options">
<h5>1. 命令行选项(Command line options)<a class="headerlink" href="#command-line-options" title="永久链接至标题">¶</a></h5>
<p>命令行传入的参数具有最高的优先级。
您可以使用command line 选项 <tt class="docutils literal"><span class="pre">-s</span></tt> (或 <tt class="docutils literal"><span class="pre">--set</span></tt>) 来覆盖一个(或更多)选项。</p>
<p>样例:</p>
<div class="highlight-sh"><div class="highlight"><pre>scrapy crawl myspider -s <span class="nv">LOG_FILE</span><span class="o">=</span>scrapy.log
</pre></div>
</div>
</div>
<div class="section" id="project-settings-module">
<h5>2. 项目设定模块(Project settings module)<a class="headerlink" href="#project-settings-module" title="永久链接至标题">¶</a></h5>
<p>项目设定模块是您Scrapy项目的标准配置文件。
其是获取大多数设定的方法。例如:: <tt class="docutils literal"><span class="pre">myproject.settings</span></tt> 。</p>
</div>
<div class="section" id="default-settings-per-command">
<h5>3. 命令默认设定(Default settings per-command)<a class="headerlink" href="#default-settings-per-command" title="永久链接至标题">¶</a></h5>
<p>每个 <a class="reference internal" href="index.html#document-topics/commands"><em>Scrapy tool</em></a> 命令拥有其默认设定，并覆盖了全局默认的设定。
这些设定在命令的类的 <tt class="docutils literal"><span class="pre">default_settings</span></tt> 属性中指定。</p>
</div>
<div class="section" id="default-global-settings">
<h5>4. 默认全局设定(Default global settings)<a class="headerlink" href="#default-global-settings" title="永久链接至标题">¶</a></h5>
<p>全局默认设定存储在 <tt class="docutils literal"><span class="pre">scrapy.settings.default_settings</span></tt> 模块，
并在 <a class="reference internal" href="index.html#topics-settings-ref"><em>内置设定参考手册</em></a> 部分有所记录。</p>
</div>
</div>
<div class="section" id="how-to-access-settings">
<h4>如何访问设定(How to access settings)<a class="headerlink" href="#how-to-access-settings" title="永久链接至标题">¶</a></h4>
<p>设定可以通过Crawler的 <a class="reference internal" href="index.html#scrapy.crawler.Crawler.settings" title="scrapy.crawler.Crawler.settings"><tt class="xref py py-attr docutils literal"><span class="pre">scrapy.crawler.Crawler.settings</span></tt></a>
属性进行访问。其由插件及中间件的 <tt class="docutils literal"><span class="pre">from_crawler</span></tt> 方法所传入:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">MyExtension</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="n">settings</span> <span class="o">=</span> <span class="n">crawler</span><span class="o">.</span><span class="n">settings</span>
        <span class="k">if</span> <span class="n">settings</span><span class="p">[</span><span class="s">&#39;LOG_ENABLED&#39;</span><span class="p">]:</span>
            <span class="k">print</span> <span class="s">&quot;log is enabled!&quot;</span>
</pre></div>
</div>
<p>另外，设定可以以字典方式进行访问。不过为了避免类型错误，
通常更希望返回需要的格式。
这可以通过 <a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><tt class="xref py py-class docutils literal"><span class="pre">Settings</span></tt></a> API
提供的方法来实现。</p>
</div>
<div class="section" id="id1">
<h4>设定名字的命名规则<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h4>
<p>设定的名字以要配置的组件作为前缀。
例如，一个robots.txt插件的合适设定应该为
<tt class="docutils literal"><span class="pre">ROBOTSTXT_ENABLED</span></tt>, <tt class="docutils literal"><span class="pre">ROBOTSTXT_OBEY</span></tt>, <tt class="docutils literal"><span class="pre">ROBOTSTXT_CACHEDIR</span></tt> 等等。</p>
</div>
<div class="section" id="topics-settings-ref">
<span id="id2"></span><h4>内置设定参考手册<a class="headerlink" href="#topics-settings-ref" title="永久链接至标题">¶</a></h4>
<p>这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。</p>
<p>如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。
这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。
同时也意味着为了使设定生效，该组件必须被启用。</p>
<div class="section" id="aws-access-key-id">
<span id="std:setting-AWS_ACCESS_KEY_ID"></span><h5>AWS_ACCESS_KEY_ID<a class="headerlink" href="#aws-access-key-id" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">None</span></tt></p>
<p>连接 <a class="reference external" href="http://aws.amazon.com/">Amazon Web services</a> 的AWS access key。
<a class="reference internal" href="index.html#topics-feed-storage-s3"><em>S3 feed storage backend</em></a> 中使用.</p>
</div>
<div class="section" id="aws-secret-access-key">
<span id="std:setting-AWS_SECRET_ACCESS_KEY"></span><h5>AWS_SECRET_ACCESS_KEY<a class="headerlink" href="#aws-secret-access-key" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">None</span></tt></p>
<p>连接 <a class="reference external" href="http://aws.amazon.com/">Amazon Web services</a>  的AWS secret key。
<a class="reference internal" href="index.html#topics-feed-storage-s3"><em>S3 feed storage backend</em></a> 中使用。</p>
</div>
<div class="section" id="bot-name">
<span id="std:setting-BOT_NAME"></span><h5>BOT_NAME<a class="headerlink" href="#bot-name" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">'scrapybot'</span></tt></p>
<p>Scrapy项目实现的bot的名字(也未项目名称)。
这将用来构造默认 User-Agent，同时也用来log。</p>
<p>当您使用 <a class="reference internal" href="index.html#std:command-startproject"><tt class="xref std std-command docutils literal"><span class="pre">startproject</span></tt></a> 命令创建项目时其也被自动赋值。</p>
</div>
<div class="section" id="concurrent-items">
<span id="std:setting-CONCURRENT_ITEMS"></span><h5>CONCURRENT_ITEMS<a class="headerlink" href="#concurrent-items" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">100</span></tt></p>
<p>Item Processor(即 <a class="reference internal" href="index.html#topics-item-pipeline"><em>Item Pipeline</em></a>)
同时处理(每个response的)item的最大值。</p>
</div>
<div class="section" id="concurrent-requests">
<span id="std:setting-CONCURRENT_REQUESTS"></span><h5>CONCURRENT_REQUESTS<a class="headerlink" href="#concurrent-requests" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">16</span></tt></p>
<p>Scrapy downloader 并发请求(concurrent requests)的最大值。</p>
</div>
<div class="section" id="concurrent-requests-per-domain">
<span id="std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"></span><h5>CONCURRENT_REQUESTS_PER_DOMAIN<a class="headerlink" href="#concurrent-requests-per-domain" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">8</span></tt></p>
<p>对单个网站进行并发请求的最大值。</p>
</div>
<div class="section" id="concurrent-requests-per-ip">
<span id="std:setting-CONCURRENT_REQUESTS_PER_IP"></span><h5>CONCURRENT_REQUESTS_PER_IP<a class="headerlink" href="#concurrent-requests-per-ip" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>对单个IP进行并发请求的最大值。如果非0，则忽略
<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><tt class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></tt></a>  设定， 使用该设定。
也就是说，并发限制将针对IP，而不是网站。</p>
<p>该设定也影响 <a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></tt></a>:
如果 <a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><tt class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></tt></a> 非0，下载延迟应用在IP而不是网站上。</p>
</div>
<div class="section" id="default-item-class">
<span id="std:setting-DEFAULT_ITEM_CLASS"></span><h5>DEFAULT_ITEM_CLASS<a class="headerlink" href="#default-item-class" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">'scrapy.item.Item'</span></tt></p>
<p><a class="reference internal" href="index.html#topics-shell"><em>the Scrapy shell</em></a> 中实例化item使用的默认类。</p>
</div>
<div class="section" id="default-request-headers">
<span id="std:setting-DEFAULT_REQUEST_HEADERS"></span><h5>DEFAULT_REQUEST_HEADERS<a class="headerlink" href="#default-request-headers" title="永久链接至标题">¶</a></h5>
<p>默认:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">{</span>
    <span class="s">&#39;Accept&#39;</span><span class="p">:</span> <span class="s">&#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;</span><span class="p">,</span>
    <span class="s">&#39;Accept-Language&#39;</span><span class="p">:</span> <span class="s">&#39;en&#39;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Scrapy HTTP Request使用的默认header。由
<a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware" title="scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">DefaultHeadersMiddleware</span></tt></a>
产生。</p>
</div>
<div class="section" id="depth-limit">
<span id="std:setting-DEPTH_LIMIT"></span><h5>DEPTH_LIMIT<a class="headerlink" href="#depth-limit" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>爬取网站最大允许的深度(depth)值。如果为0，则没有限制。</p>
</div>
<div class="section" id="depth-priority">
<span id="std:setting-DEPTH_PRIORITY"></span><h5>DEPTH_PRIORITY<a class="headerlink" href="#depth-priority" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>整数值。用于根据深度调整request优先级。</p>
<p>如果为0，则不根据深度进行优先级调整。</p>
</div>
<div class="section" id="depth-stats">
<span id="std:setting-DEPTH_STATS"></span><h5>DEPTH_STATS<a class="headerlink" href="#depth-stats" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>是否收集最大深度数据。</p>
</div>
<div class="section" id="depth-stats-verbose">
<span id="std:setting-DEPTH_STATS_VERBOSE"></span><h5>DEPTH_STATS_VERBOSE<a class="headerlink" href="#depth-stats-verbose" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。</p>
</div>
<div class="section" id="dnscache-enabled">
<span id="std:setting-DNSCACHE_ENABLED"></span><h5>DNSCACHE_ENABLED<a class="headerlink" href="#dnscache-enabled" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>是否启用DNS内存缓存(DNS in-memory cache)。</p>
</div>
<div class="section" id="downloader">
<span id="std:setting-DOWNLOADER"></span><h5>DOWNLOADER<a class="headerlink" href="#downloader" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">'scrapy.core.downloader.Downloader'</span></tt></p>
<p>用于crawl的downloader.</p>
</div>
<div class="section" id="downloader-middlewares">
<span id="std:setting-DOWNLOADER_MIDDLEWARES"></span><h5>DOWNLOADER_MIDDLEWARES<a class="headerlink" href="#downloader-middlewares" title="永久链接至标题">¶</a></h5>
<p>默认:: <tt class="docutils literal"><span class="pre">{}</span></tt></p>
<p>保存项目中启用的下载中间件及其顺序的字典。
更多内容请查看 <a class="reference internal" href="index.html#topics-downloader-middleware-setting"><em>激活下载器中间件</em></a> 。</p>
</div>
<div class="section" id="downloader-middlewares-base">
<span id="std:setting-DOWNLOADER_MIDDLEWARES_BASE"></span><h5>DOWNLOADER_MIDDLEWARES_BASE<a class="headerlink" href="#downloader-middlewares-base" title="永久链接至标题">¶</a></h5>
<p>默认:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">{</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware&#39;</span><span class="p">:</span> <span class="mi">350</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware&#39;</span><span class="p">:</span> <span class="mi">400</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.retry.RetryMiddleware&#39;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware&#39;</span><span class="p">:</span> <span class="mi">550</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware&#39;</span><span class="p">:</span> <span class="mi">580</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware&#39;</span><span class="p">:</span> <span class="mi">590</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware&#39;</span><span class="p">:</span> <span class="mi">600</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware&#39;</span><span class="p">:</span> <span class="mi">700</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware&#39;</span><span class="p">:</span> <span class="mi">750</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware&#39;</span><span class="p">:</span> <span class="mi">830</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.stats.DownloaderStats&#39;</span><span class="p">:</span> <span class="mi">850</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware&#39;</span><span class="p">:</span> <span class="mi">900</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>包含Scrapy默认启用的下载中间件的字典。
永远不要在项目中修改该设定，而是修改
<a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></tt></a> 。更多内容请参考
<a class="reference internal" href="index.html#topics-downloader-middleware-setting"><em>激活下载器中间件</em></a>.</p>
</div>
<div class="section" id="downloader-stats">
<span id="std:setting-DOWNLOADER_STATS"></span><h5>DOWNLOADER_STATS<a class="headerlink" href="#downloader-stats" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>是否收集下载器数据。</p>
</div>
<div class="section" id="download-delay">
<span id="std:setting-DOWNLOAD_DELAY"></span><h5>DOWNLOAD_DELAY<a class="headerlink" href="#download-delay" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，
减轻服务器压力。同时也支持小数:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">DOWNLOAD_DELAY</span> <span class="o">=</span> <span class="mf">0.25</span>    <span class="c"># 250 ms of delay</span>
</pre></div>
</div>
<p>该设定影响(默认启用的) <a class="reference internal" href="index.html#std:setting-RANDOMIZE_DOWNLOAD_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">RANDOMIZE_DOWNLOAD_DELAY</span></tt></a> 设定。
默认情况下，Scrapy在两个请求间不等待一个固定的值，
而是使用0.5到1.5之间的一个随机值 * <a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></tt></a> 的结果作为等待间隔。</p>
<p>当 <a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><tt class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></tt></a> 非0时，延迟针对的是每个ip而不是网站。</p>
<p>另外您可以通过spider的 <tt class="docutils literal"><span class="pre">download_delay</span></tt> 属性为每个spider设置该设定。</p>
</div>
<div class="section" id="download-handlers">
<span id="std:setting-DOWNLOAD_HANDLERS"></span><h5>DOWNLOAD_HANDLERS<a class="headerlink" href="#download-handlers" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">{}</span></tt></p>
<p>保存项目中启用的下载处理器(request downloader handler)的字典。
例子请查看 <cite>DOWNLOAD_HANDLERS_BASE</cite> 。</p>
</div>
<div class="section" id="download-handlers-base">
<span id="std:setting-DOWNLOAD_HANDLERS_BASE"></span><h5>DOWNLOAD_HANDLERS_BASE<a class="headerlink" href="#download-handlers-base" title="永久链接至标题">¶</a></h5>
<p>默认:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">{</span>
    <span class="s">&#39;file&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.core.downloader.handlers.file.FileDownloadHandler&#39;</span><span class="p">,</span>
    <span class="s">&#39;http&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.core.downloader.handlers.http.HttpDownloadHandler&#39;</span><span class="p">,</span>
    <span class="s">&#39;https&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.core.downloader.handlers.http.HttpDownloadHandler&#39;</span><span class="p">,</span>
    <span class="s">&#39;s3&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.core.downloader.handlers.s3.S3DownloadHandler&#39;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>保存项目中默认启用的下载处理器(request downloader handler)的字典。
永远不要在项目中修改该设定，而是修改
<tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_HANDLERS</span></tt> 。</p>
<p>如果需要关闭上面的下载处理器，您必须在项目中的
<a class="reference internal" href="index.html#std:setting-DOWNLOAD_HANDLERS"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_HANDLERS</span></tt></a> 设定中设置该处理器，并为其赋值为 <cite>None</cite> 。
例如，关闭文件下载处理器:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">DOWNLOAD_HANDLERS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">&#39;file&#39;</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="download-timeout">
<span id="std:setting-DOWNLOAD_TIMEOUT"></span><h5>DOWNLOAD_TIMEOUT<a class="headerlink" href="#download-timeout" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">180</span></tt></p>
<p>下载器超时时间(单位: 秒)。</p>
</div>
<div class="section" id="dupefilter-class">
<span id="std:setting-DUPEFILTER_CLASS"></span><h5>DUPEFILTER_CLASS<a class="headerlink" href="#dupefilter-class" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">'scrapy.dupefilter.RFPDupeFilter'</span></tt></p>
<p>用于检测过滤重复请求的类。</p>
<p>默认的 (<tt class="docutils literal"><span class="pre">RFPDupeFilter</span></tt>) 过滤器基于
<tt class="docutils literal"><span class="pre">scrapy.utils.request.request_fingerprint</span></tt> 函数生成的请求fingerprint(指纹)。
如果您需要修改检测的方式，您可以继承 <tt class="docutils literal"><span class="pre">RFPDupeFilter</span></tt>
并覆盖其 <tt class="docutils literal"><span class="pre">request_fingerprint</span></tt> 方法。
该方法接收 <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象并返回其fingerprint(一个字符串)。</p>
</div>
<div class="section" id="dupefilter-debug">
<span id="std:setting-DUPEFILTER_DEBUG"></span><h5>DUPEFILTER_DEBUG<a class="headerlink" href="#dupefilter-debug" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>默认情况下， <tt class="docutils literal"><span class="pre">RFPDupeFilter</span></tt> 只记录第一次重复的请求。
设置 <a class="reference internal" href="index.html#std:setting-DUPEFILTER_DEBUG"><tt class="xref std std-setting docutils literal"><span class="pre">DUPEFILTER_DEBUG</span></tt></a> 为 <tt class="docutils literal"><span class="pre">True</span></tt> 将会使其记录所有重复的requests。</p>
</div>
<div class="section" id="editor">
<span id="std:setting-EDITOR"></span><h5>EDITOR<a class="headerlink" href="#editor" title="永久链接至标题">¶</a></h5>
<p>默认: <cite>depends on the environment</cite></p>
<p>执行 <a class="reference internal" href="index.html#std:command-edit"><tt class="xref std std-command docutils literal"><span class="pre">edit</span></tt></a> 命令编辑spider时使用的编辑器。
其默认为 <tt class="docutils literal"><span class="pre">EDITOR</span></tt> 环境变量。如果该变量未设置，其默认为 <tt class="docutils literal"><span class="pre">vi</span></tt> (Unix系统) 或者 IDLE编辑器(Windows)。</p>
</div>
<div class="section" id="extensions">
<span id="std:setting-EXTENSIONS"></span><h5>EXTENSIONS<a class="headerlink" href="#extensions" title="永久链接至标题">¶</a></h5>
<p>默认:: <tt class="docutils literal"><span class="pre">{}</span></tt></p>
<p>保存项目中启用的插件及其顺序的字典。</p>
</div>
<div class="section" id="extensions-base">
<span id="std:setting-EXTENSIONS_BASE"></span><h5>EXTENSIONS_BASE<a class="headerlink" href="#extensions-base" title="永久链接至标题">¶</a></h5>
<p>默认:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">{</span>
    <span class="s">&#39;scrapy.contrib.corestats.CoreStats&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">&#39;scrapy.webservice.WebService&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">&#39;scrapy.telnet.TelnetConsole&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.memusage.MemoryUsage&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.memdebug.MemoryDebugger&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.closespider.CloseSpider&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.feedexport.FeedExporter&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.logstats.LogStats&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.spiderstate.SpiderState&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.throttle.AutoThrottle&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，
该设定包含所有稳定(stable)的内置插件。</p>
<p>更多内容请参考 <a class="reference internal" href="index.html#topics-extensions"><em>extensions用户手册</em></a> 及
<a class="reference internal" href="index.html#topics-extensions-ref"><em>所有可用的插件</em></a> 。</p>
</div>
<div class="section" id="item-pipelines">
<span id="std:setting-ITEM_PIPELINES"></span><h5>ITEM_PIPELINES<a class="headerlink" href="#item-pipelines" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">{}</span></tt></p>
<p>保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。
不过值(value)习惯设定在0-1000范围内。</p>
<p>为了兼容性，<a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES"><tt class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></tt></a> 支持列表，不过已经被废弃了。</p>
<p>样例:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">&#39;mybot.pipelines.validate.ValidateMyItem&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="s">&#39;mybot.pipelines.validate.StoreMyItem&#39;</span><span class="p">:</span> <span class="mi">800</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="item-pipelines-base">
<span id="std:setting-ITEM_PIPELINES_BASE"></span><h5>ITEM_PIPELINES_BASE<a class="headerlink" href="#item-pipelines-base" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">{}</span></tt></p>
<p>保存项目中默认启用的pipeline的字典。
永远不要在项目中修改该设定，而是修改
<a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES"><tt class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></tt></a> 。</p>
</div>
<div class="section" id="log-enabled">
<span id="std:setting-LOG_ENABLED"></span><h5>LOG_ENABLED<a class="headerlink" href="#log-enabled" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>是否启用logging。</p>
</div>
<div class="section" id="log-encoding">
<span id="std:setting-LOG_ENCODING"></span><h5>LOG_ENCODING<a class="headerlink" href="#log-encoding" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">'utf-8'</span></tt></p>
<p>logging使用的编码。</p>
</div>
<div class="section" id="log-file">
<span id="std:setting-LOG_FILE"></span><h5>LOG_FILE<a class="headerlink" href="#log-file" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">None</span></tt></p>
<p>logging输出的文件名。如果为None，则使用标准错误输出(standard error)。</p>
</div>
<div class="section" id="log-level">
<span id="std:setting-LOG_LEVEL"></span><h5>LOG_LEVEL<a class="headerlink" href="#log-level" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">'DEBUG'</span></tt></p>
<p>log的最低级别。可选的级别有: CRITICAL、
ERROR、WARNING、INFO、DEBUG。更多内容请查看 <a class="reference internal" href="index.html#topics-logging"><em>Logging</em></a> 。</p>
</div>
<div class="section" id="log-stdout">
<span id="std:setting-LOG_STDOUT"></span><h5>LOG_STDOUT<a class="headerlink" href="#log-stdout" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>如果为 <tt class="docutils literal"><span class="pre">True</span></tt> ，进程所有的标准输出(及错误)将会被重定向到log中。例如，
执行 <tt class="docutils literal"><span class="pre">print</span> <span class="pre">'hello'</span></tt> ，其将会在Scrapy log中显示。</p>
</div>
<div class="section" id="memdebug-enabled">
<span id="std:setting-MEMDEBUG_ENABLED"></span><h5>MEMDEBUG_ENABLED<a class="headerlink" href="#memdebug-enabled" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>是否启用内存调试(memory debugging)。</p>
</div>
<div class="section" id="memdebug-notify">
<span id="std:setting-MEMDEBUG_NOTIFY"></span><h5>MEMDEBUG_NOTIFY<a class="headerlink" href="#memdebug-notify" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">[]</span></tt></p>
<p>如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。</p>
<p>样例:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">MEMDEBUG_NOTIFY</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;user@example.com&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="memusage-enabled">
<span id="std:setting-MEMUSAGE_ENABLED"></span><h5>MEMUSAGE_ENABLED<a class="headerlink" href="#memusage-enabled" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>Scope: <tt class="docutils literal"><span class="pre">scrapy.contrib.memusage</span></tt></p>
<p>是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，
同时发送email进行通知。</p>
<p>See <a class="reference internal" href="index.html#topics-extensions-ref-memusage"><em>内存使用扩展(Memory usage extension)</em></a>.</p>
</div>
<div class="section" id="memusage-limit-mb">
<span id="std:setting-MEMUSAGE_LIMIT_MB"></span><h5>MEMUSAGE_LIMIT_MB<a class="headerlink" href="#memusage-limit-mb" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>Scope: <tt class="docutils literal"><span class="pre">scrapy.contrib.memusage</span></tt></p>
<p>在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。
如果为0，将不做限制。</p>
<p>See <a class="reference internal" href="index.html#topics-extensions-ref-memusage"><em>内存使用扩展(Memory usage extension)</em></a>.</p>
</div>
<div class="section" id="memusage-notify-mail">
<span id="std:setting-MEMUSAGE_NOTIFY_MAIL"></span><h5>MEMUSAGE_NOTIFY_MAIL<a class="headerlink" href="#memusage-notify-mail" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>Scope: <tt class="docutils literal"><span class="pre">scrapy.contrib.memusage</span></tt></p>
<p>达到内存限制时通知的email列表。</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">MEMUSAGE_NOTIFY_MAIL</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;user@example.com&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="index.html#topics-extensions-ref-memusage"><em>内存使用扩展(Memory usage extension)</em></a>.</p>
</div>
<div class="section" id="memusage-report">
<span id="std:setting-MEMUSAGE_REPORT"></span><h5>MEMUSAGE_REPORT<a class="headerlink" href="#memusage-report" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>Scope: <tt class="docutils literal"><span class="pre">scrapy.contrib.memusage</span></tt></p>
<p>每个spider被关闭时是否发送内存使用报告。</p>
<p>查看 <a class="reference internal" href="index.html#topics-extensions-ref-memusage"><em>内存使用扩展(Memory usage extension)</em></a>.</p>
</div>
<div class="section" id="memusage-warning-mb">
<span id="std:setting-MEMUSAGE_WARNING_MB"></span><h5>MEMUSAGE_WARNING_MB<a class="headerlink" href="#memusage-warning-mb" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>Scope: <tt class="docutils literal"><span class="pre">scrapy.contrib.memusage</span></tt></p>
<p>在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。
如果为0，将不发送警告。</p>
</div>
<div class="section" id="newspider-module">
<span id="std:setting-NEWSPIDER_MODULE"></span><h5>NEWSPIDER_MODULE<a class="headerlink" href="#newspider-module" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">''</span></tt></p>
<p>使用 <a class="reference internal" href="index.html#std:command-genspider"><tt class="xref std std-command docutils literal"><span class="pre">genspider</span></tt></a> 命令创建新spider的模块。</p>
<p>样例:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">NEWSPIDER_MODULE</span> <span class="o">=</span> <span class="s">&#39;mybot.spiders_dev&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="randomize-download-delay">
<span id="std:setting-RANDOMIZE_DOWNLOAD_DELAY"></span><h5>RANDOMIZE_DOWNLOAD_DELAY<a class="headerlink" href="#randomize-download-delay" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值
(0.5到1.5之间的一个随机值 * <a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></tt></a>)。</p>
<p>该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，
查找请求之间时间的相似性。</p>
<p>随机的策略与 <a class="reference external" href="http://www.gnu.org/software/wget/manual/wget.html">wget</a> <tt class="docutils literal"><span class="pre">--random-wait</span></tt> 选项的策略相同。</p>
<p>若 <a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></tt></a> 为0(默认值)，该选项将不起作用。</p>
</div>
<div class="section" id="redirect-max-times">
<span id="std:setting-REDIRECT_MAX_TIMES"></span><h5>REDIRECT_MAX_TIMES<a class="headerlink" href="#redirect-max-times" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">20</span></tt></p>
<p>定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。
对某些任务我们使用Firefox默认值。</p>
</div>
<div class="section" id="redirect-max-metarefresh-delay">
<span id="std:setting-REDIRECT_MAX_METAREFRESH_DELAY"></span><h5>REDIRECT_MAX_METAREFRESH_DELAY<a class="headerlink" href="#redirect-max-metarefresh-delay" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">100</span></tt></p>
<p>有些网站使用 meta-refresh 重定向到session超时页面，
因此我们限制自动重定向到最大延迟(秒)。
=&gt;有点不肯定:</p>
</div>
<div class="section" id="redirect-priority-adjust">
<span id="std:setting-REDIRECT_PRIORITY_ADJUST"></span><h5>REDIRECT_PRIORITY_ADJUST<a class="headerlink" href="#redirect-priority-adjust" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">+2</span></tt></p>
<p>修改重定向请求相对于原始请求的优先级。
负数意味着更多优先级。</p>
</div>
<div class="section" id="robotstxt-obey">
<span id="std:setting-ROBOTSTXT_OBEY"></span><h5>ROBOTSTXT_OBEY<a class="headerlink" href="#robotstxt-obey" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>Scope: <tt class="docutils literal"><span class="pre">scrapy.contrib.downloadermiddleware.robotstxt</span></tt></p>
<p>如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看
<a class="reference internal" href="index.html#topics-dlmw-robots"><em>RobotsTxtMiddleware</em></a> 。</p>
</div>
<div class="section" id="scheduler">
<span id="std:setting-SCHEDULER"></span><h5>SCHEDULER<a class="headerlink" href="#scheduler" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">'scrapy.core.scheduler.Scheduler'</span></tt></p>
<p>用于爬取的调度器。</p>
</div>
<div class="section" id="spider-contracts">
<span id="std:setting-SPIDER_CONTRACTS"></span><h5>SPIDER_CONTRACTS<a class="headerlink" href="#spider-contracts" title="永久链接至标题">¶</a></h5>
<p>默认:: <tt class="docutils literal"><span class="pre">{}</span></tt></p>
<p>保存项目中启用用于测试spider的scrapy contract及其顺序的字典。
更多内容请参考 <a class="reference internal" href="index.html#topics-contracts"><em>Spiders Contracts</em></a> 。</p>
</div>
<div class="section" id="spider-contracts-base">
<span id="std:setting-SPIDER_CONTRACTS_BASE"></span><h5>SPIDER_CONTRACTS_BASE<a class="headerlink" href="#spider-contracts-base" title="永久链接至标题">¶</a></h5>
<p>默认:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">{</span>
    <span class="s">&#39;scrapy.contracts.default.UrlContract&#39;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contracts.default.ReturnsContract&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contracts.default.ScrapesContract&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>保存项目中默认启用的scrapy contract的字典。
永远不要在项目中修改该设定，而是修改
<a class="reference internal" href="index.html#std:setting-SPIDER_CONTRACTS"><tt class="xref std std-setting docutils literal"><span class="pre">SPIDER_CONTRACTS</span></tt></a> 。更多内容请参考
<a class="reference internal" href="index.html#topics-contracts"><em>Spiders Contracts</em></a> 。</p>
</div>
<div class="section" id="spider-middlewares">
<span id="std:setting-SPIDER_MIDDLEWARES"></span><h5>SPIDER_MIDDLEWARES<a class="headerlink" href="#spider-middlewares" title="永久链接至标题">¶</a></h5>
<p>默认:: <tt class="docutils literal"><span class="pre">{}</span></tt></p>
<p>保存项目中启用的下载中间件及其顺序的字典。
更多内容请参考 <a class="reference internal" href="index.html#topics-spider-middleware-setting"><em>激活spider中间件</em></a> 。</p>
</div>
<div class="section" id="spider-middlewares-base">
<span id="std:setting-SPIDER_MIDDLEWARES_BASE"></span><h5>SPIDER_MIDDLEWARES_BASE<a class="headerlink" href="#spider-middlewares-base" title="永久链接至标题">¶</a></h5>
<p>默认:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">{</span>
    <span class="s">&#39;scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware&#39;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.spidermiddleware.referer.RefererMiddleware&#39;</span><span class="p">:</span> <span class="mi">700</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware&#39;</span><span class="p">:</span> <span class="mi">800</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.spidermiddleware.depth.DepthMiddleware&#39;</span><span class="p">:</span> <span class="mi">900</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>保存项目中默认启用的spider中间件的字典。
永远不要在项目中修改该设定，而是修改
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES"><tt class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES</span></tt></a> 。更多内容请参考
<a class="reference internal" href="index.html#topics-spider-middleware-setting"><em>激活spider中间件</em></a>.</p>
</div>
<div class="section" id="spider-modules">
<span id="std:setting-SPIDER_MODULES"></span><h5>SPIDER_MODULES<a class="headerlink" href="#spider-modules" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">[]</span></tt></p>
<p>Scrapy搜索spider的模块列表。</p>
<p>样例:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">SPIDER_MODULES</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;mybot.spiders_prod&#39;</span><span class="p">,</span> <span class="s">&#39;mybot.spiders_dev&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="stats-class">
<span id="std:setting-STATS_CLASS"></span><h5>STATS_CLASS<a class="headerlink" href="#stats-class" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">'scrapy.statscol.MemoryStatsCollector'</span></tt></p>
<p>收集数据的类。该类必须实现
<a class="reference internal" href="index.html#topics-api-stats"><em>状态收集器(Stats Collector) API</em></a>.</p>
</div>
<div class="section" id="stats-dump">
<span id="std:setting-STATS_DUMP"></span><h5>STATS_DUMP<a class="headerlink" href="#stats-dump" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>当spider结束时dump <a class="reference internal" href="index.html#topics-stats"><em>Scrapy状态数据</em></a> (到Scrapy log中)。</p>
<p>更多内容请查看 <a class="reference internal" href="index.html#topics-stats"><em>数据收集(Stats Collection)</em></a> 。</p>
</div>
<div class="section" id="statsmailer-rcpts">
<span id="std:setting-STATSMAILER_RCPTS"></span><h5>STATSMAILER_RCPTS<a class="headerlink" href="#statsmailer-rcpts" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">[]</span></tt> (空list)</p>
<p>spider完成爬取后发送Scrapy数据。更多内容请查看
<tt class="xref py py-class docutils literal"><span class="pre">StatsMailer</span></tt> 。</p>
</div>
<div class="section" id="telnetconsole-enabled">
<span id="std:setting-TELNETCONSOLE_ENABLED"></span><h5>TELNETCONSOLE_ENABLED<a class="headerlink" href="#telnetconsole-enabled" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>表明 <a class="reference internal" href="index.html#topics-telnetconsole"><em>telnet 终端</em></a> (及其插件)是否启用的布尔值。</p>
</div>
<div class="section" id="telnetconsole-port">
<span id="std:setting-TELNETCONSOLE_PORT"></span><h5>TELNETCONSOLE_PORT<a class="headerlink" href="#telnetconsole-port" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">[6023,</span> <span class="pre">6073]</span></tt></p>
<p>telnet终端使用的端口范围。如果设置为 <tt class="docutils literal"><span class="pre">None</span></tt> 或 <tt class="docutils literal"><span class="pre">0</span></tt> ，
则使用动态分配的端口。更多内容请查看
<a class="reference internal" href="index.html#topics-telnetconsole"><em>Telnet终端(Telnet Console)</em></a> 。</p>
</div>
<div class="section" id="templates-dir">
<span id="std:setting-TEMPLATES_DIR"></span><h5>TEMPLATES_DIR<a class="headerlink" href="#templates-dir" title="永久链接至标题">¶</a></h5>
<p>默认:  scrapy模块内部的 <tt class="docutils literal"><span class="pre">templates</span></tt></p>
<p>使用 <a class="reference internal" href="index.html#std:command-startproject"><tt class="xref std std-command docutils literal"><span class="pre">startproject</span></tt></a> 命令创建项目时查找模板的目录。</p>
</div>
<div class="section" id="urllength-limit">
<span id="std:setting-URLLENGTH_LIMIT"></span><h5>URLLENGTH_LIMIT<a class="headerlink" href="#urllength-limit" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">2083</span></tt></p>
<p>Scope: <tt class="docutils literal"><span class="pre">contrib.spidermiddleware.urllength</span></tt></p>
<p>爬取URL的最大长度。更多关于该设定的默认值信息请查看:
<a class="reference external" href="http://www.boutell.com/newfaq/misc/urllength.html">http://www.boutell.com/newfaq/misc/urllength.html</a></p>
</div>
<div class="section" id="user-agent">
<span id="std:setting-USER_AGENT"></span><h5>USER_AGENT<a class="headerlink" href="#user-agent" title="永久链接至标题">¶</a></h5>
<p>默认: <tt class="docutils literal"><span class="pre">&quot;Scrapy/VERSION</span> <span class="pre">(+http://scrapy.org)&quot;</span></tt></p>
<p>爬取的默认User-Agent，除非被覆盖。</p>
</div>
</div>
</div>
<span id="document-topics/signals"></span><div class="section" id="signals">
<span id="topics-signals"></span><h3>信号(Signals)<a class="headerlink" href="#signals" title="永久链接至标题">¶</a></h3>
<p>Scrapy使用信号来通知事情发生。您可以在您的Scrapy项目中捕捉一些信号(使用
<a class="reference internal" href="index.html#topics-extensions"><em>extension</em></a>)来完成额外的工作或添加额外的功能，扩展Scrapy。</p>
<p>虽然信号提供了一些参数，不过处理函数不用接收所有的参数 -
信号分发机制(singal dispatching mechanism)仅仅提供处理器(handler)接受的参数。</p>
<p>您可以通过
<a class="reference internal" href="index.html#topics-api-signals"><em>信号(Signals) API</em></a> 来连接(或发送您自己的)信号。</p>
<div class="section" id="deferred-signal-handlers">
<h4>延迟的信号处理器(Deferred signal handlers)<a class="headerlink" href="#deferred-signal-handlers" title="永久链接至标题">¶</a></h4>
<p>有些信号支持从处理器返回 <a class="reference external" href="http://twistedmatrix.com/documents/current/core/howto/defer.html">Twisted deferreds</a> ，参考下边的
<a class="reference internal" href="index.html#topics-signals-ref"><em>内置信号参考手册(Built-in signals reference)</em></a> 来了解哪些支持。</p>
</div>
<div class="section" id="module-scrapy.signals">
<span id="built-in-signals-reference"></span><span id="topics-signals-ref"></span><h4>内置信号参考手册(Built-in signals reference)<a class="headerlink" href="#module-scrapy.signals" title="永久链接至标题">¶</a></h4>
<p>以下给出Scrapy内置信号的列表及其意义。</p>
<div class="section" id="engine-started">
<h5>engine_started<a class="headerlink" href="#engine-started" title="永久链接至标题">¶</a></h5>
<span class="target" id="std:signal-engine_started"></span><dl class="function">
<dt id="scrapy.signals.engine_started">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">engine_started</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.signals.engine_started" title="永久链接至目标">¶</a></dt>
<dd><p>当Scrapy引擎启动爬取时发送该信号。</p>
<p>该信号支持返回deferreds。</p>
</dd></dl>

<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">该信号可能会在信号 <a class="reference internal" href="index.html#std:signal-spider_opened"><tt class="xref std std-signal docutils literal"><span class="pre">spider_opened</span></tt></a> 之后被发送，取决于spider的启动方式。
所以不要 <strong>依赖</strong> 该信号会比 <tt class="xref std std-signal docutils literal"><span class="pre">spider-opened</span></tt> 更早被发送。</p>
</div>
</div>
<div class="section" id="engine-stopped">
<h5>engine_stopped<a class="headerlink" href="#engine-stopped" title="永久链接至标题">¶</a></h5>
<span class="target" id="std:signal-engine_stopped"></span><dl class="function">
<dt id="scrapy.signals.engine_stopped">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">engine_stopped</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.signals.engine_stopped" title="永久链接至目标">¶</a></dt>
<dd><p>当Scrapy引擎停止时发送该信号(例如，爬取结束)。</p>
<p>该信号支持返回deferreds。</p>
</dd></dl>

</div>
<div class="section" id="item-scraped">
<h5>item_scraped<a class="headerlink" href="#item-scraped" title="永久链接至标题">¶</a></h5>
<span class="target" id="std:signal-item_scraped"></span><dl class="function">
<dt id="scrapy.signals.item_scraped">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">item_scraped</tt><big>(</big><em>item</em>, <em>response</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.signals.item_scraped" title="永久链接至目标">¶</a></dt>
<dd><p>当item被爬取，并通过所有
<a class="reference internal" href="index.html#topics-item-pipeline"><em>Item Pipeline</em></a> 后(没有被丢弃(dropped)，发送该信号。</p>
<p>该信号支持返回deferreds。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>item</strong> (<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> 对象) &#8211; 爬取到的item</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> 对象) &#8211; 爬取item的spider</li>
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> 对象) &#8211; 提取item的response</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="item-dropped">
<h5>item_dropped<a class="headerlink" href="#item-dropped" title="永久链接至标题">¶</a></h5>
<span class="target" id="std:signal-item_dropped"></span><dl class="function">
<dt id="scrapy.signals.item_dropped">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">item_dropped</tt><big>(</big><em>item</em>, <em>exception</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.signals.item_dropped" title="永久链接至目标">¶</a></dt>
<dd><p>当item通过 <a class="reference internal" href="index.html#topics-item-pipeline"><em>Item Pipeline</em></a> ，有些pipeline抛出
<a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><tt class="xref py py-exc docutils literal"><span class="pre">DropItem</span></tt></a> 异常，丢弃item时，该信号被发送。</p>
<p>该信号支持返回deferreds。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>item</strong> (<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> 对象) &#8211; <a class="reference internal" href="index.html#topics-item-pipeline"><em>Item Pipeline</em></a> 丢弃的item</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> 对象) &#8211; 爬取item的spider</li>
<li><strong>exception</strong> (<a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><tt class="xref py py-exc docutils literal"><span class="pre">DropItem</span></tt></a> 异常) &#8211; 导致item被丢弃的异常(必须是
<a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><tt class="xref py py-exc docutils literal"><span class="pre">DropItem</span></tt></a> 的子类)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="spider-closed">
<h5>spider_closed<a class="headerlink" href="#spider-closed" title="永久链接至标题">¶</a></h5>
<span class="target" id="std:signal-spider_closed"></span><dl class="function">
<dt id="scrapy.signals.spider_closed">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">spider_closed</tt><big>(</big><em>spider</em>, <em>reason</em><big>)</big><a class="headerlink" href="#scrapy.signals.spider_closed" title="永久链接至目标">¶</a></dt>
<dd><p>当某个spider被关闭时，该信号被发送。该信号可以用来释放每个spider在
<a class="reference internal" href="index.html#std:signal-spider_opened"><tt class="xref std std-signal docutils literal"><span class="pre">spider_opened</span></tt></a> 时占用的资源。</p>
<p>该信号支持返回deferreds。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> 对象) &#8211; 关闭的spider</li>
<li><strong>reason</strong> (<em>str</em>) &#8211; 描述spider被关闭的原因的字符串。如果spider是由于完成爬取而被关闭，则其为
<tt class="docutils literal"><span class="pre">'finished'</span></tt> 。否则，如果spider是被引擎的 <tt class="docutils literal"><span class="pre">close_spider</span></tt> 方法所关闭，则其为调用该方法时传入的
<tt class="docutils literal"><span class="pre">reason</span></tt> 参数(默认为 <tt class="docutils literal"><span class="pre">'cancelled'</span></tt>)。如果引擎被关闭(例如，
输入Ctrl-C)，则其为 <tt class="docutils literal"><span class="pre">'shutdown'</span></tt> 。</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="spider-opened">
<h5>spider_opened<a class="headerlink" href="#spider-opened" title="永久链接至标题">¶</a></h5>
<span class="target" id="std:signal-spider_opened"></span><dl class="function">
<dt id="scrapy.signals.spider_opened">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">spider_opened</tt><big>(</big><em>spider</em><big>)</big><a class="headerlink" href="#scrapy.signals.spider_opened" title="永久链接至目标">¶</a></dt>
<dd><p>当spider开始爬取时发送该信号。该信号一般用来分配spider的资源，不过其也能做任何事。</p>
<p>该信号支持返回deferreds。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> 对象) &#8211; 开启的spider</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="spider-idle">
<h5>spider_idle<a class="headerlink" href="#spider-idle" title="永久链接至标题">¶</a></h5>
<span class="target" id="std:signal-spider_idle"></span><dl class="function">
<dt id="scrapy.signals.spider_idle">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">spider_idle</tt><big>(</big><em>spider</em><big>)</big><a class="headerlink" href="#scrapy.signals.spider_idle" title="永久链接至目标">¶</a></dt>
<dd><p>当spider进入空闲(idle)状态时该信号被发送。空闲意味着:</p>
<blockquote>
<div><ul class="simple">
<li>requests正在等待被下载</li>
<li>requests被调度</li>
<li>items正在item pipeline中被处理</li>
</ul>
</div></blockquote>
<p>当该信号的所有处理器(handler)被调用后，如果spider仍然保持空闲状态，
引擎将会关闭该spider。当spider被关闭后， <a class="reference internal" href="index.html#std:signal-spider_closed"><tt class="xref std std-signal docutils literal"><span class="pre">spider_closed</span></tt></a> 信号将被发送。</p>
<p>您可以，比如，在 <a class="reference internal" href="index.html#std:signal-spider_idle"><tt class="xref std std-signal docutils literal"><span class="pre">spider_idle</span></tt></a> 处理器中调度某些请求来避免spider被关闭。</p>
<p>该信号 <strong>不支持</strong> 返回deferreds。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> 对象) &#8211; 空闲的spider</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="spider-error">
<h5>spider_error<a class="headerlink" href="#spider-error" title="永久链接至标题">¶</a></h5>
<span class="target" id="std:signal-spider_error"></span><dl class="function">
<dt id="scrapy.signals.spider_error">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">spider_error</tt><big>(</big><em>failure</em>, <em>response</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.signals.spider_error" title="永久链接至目标">¶</a></dt>
<dd><p>当spider的回调函数产生错误时(例如，抛出异常)，该信号被发送。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>failure</strong> (<a class="reference external" href="http://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Failure</a> 对象) &#8211; 以Twisted <a class="reference external" href="http://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Failure</a> 对象抛出的异常</li>
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> 对象) &#8211; 当异常被抛出时被处理的response</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> 对象) &#8211; 抛出异常的spider</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="request-scheduled">
<h5>request_scheduled<a class="headerlink" href="#request-scheduled" title="永久链接至标题">¶</a></h5>
<span class="target" id="std:signal-request_scheduled"></span><dl class="function">
<dt id="scrapy.signals.request_scheduled">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">request_scheduled</tt><big>(</big><em>request</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.signals.request_scheduled" title="永久链接至目标">¶</a></dt>
<dd><p>当引擎调度一个 <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象用于下载时，该信号被发送。</p>
<p>该信号 <strong>不支持</strong> 返回deferreds。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象) &#8211; 到达调度器的request</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> 对象) &#8211; 产生该request的spider</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="response-received">
<h5>response_received<a class="headerlink" href="#response-received" title="永久链接至标题">¶</a></h5>
<span class="target" id="std:signal-response_received"></span><dl class="function">
<dt id="scrapy.signals.response_received">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">response_received</tt><big>(</big><em>response</em>, <em>request</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.signals.response_received" title="永久链接至目标">¶</a></dt>
<dd><p>当引擎从downloader获取到一个新的 <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> 时发送该信号。</p>
<p>该信号 <strong>不支持</strong> 返回deferreds。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> 对象) &#8211; 接收到的response</li>
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象) &#8211; 生成response的request</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> 对象) &#8211; response所对应的spider</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="response-downloaded">
<h5>response_downloaded<a class="headerlink" href="#response-downloaded" title="永久链接至标题">¶</a></h5>
<span class="target" id="std:signal-response_downloaded"></span><dl class="function">
<dt id="scrapy.signals.response_downloaded">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">response_downloaded</tt><big>(</big><em>response</em>, <em>request</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.signals.response_downloaded" title="永久链接至目标">¶</a></dt>
<dd><p>当一个 <tt class="docutils literal"><span class="pre">HTTPResponse</span></tt> 被下载时，由downloader发送该信号。</p>
<p>该信号 <strong>不支持</strong> 返回deferreds。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> 对象) &#8211; 下载的response</li>
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> 对象) &#8211; 生成response的request</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> 对象) &#8211; response所对应的spider</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
</div>
<span id="document-topics/exceptions"></span><div class="section" id="module-scrapy.exceptions">
<span id="exceptions"></span><span id="topics-exceptions"></span><h3>异常(Exceptions)<a class="headerlink" href="#module-scrapy.exceptions" title="永久链接至标题">¶</a></h3>
<div class="section" id="built-in-exceptions-reference">
<span id="topics-exceptions-ref"></span><h4>内置异常参考手册(Built-in Exceptions reference)<a class="headerlink" href="#built-in-exceptions-reference" title="永久链接至标题">¶</a></h4>
<p>下面是Scrapy提供的异常及其用法。</p>
<div class="section" id="dropitem">
<h5>DropItem<a class="headerlink" href="#dropitem" title="永久链接至标题">¶</a></h5>
<dl class="exception">
<dt id="scrapy.exceptions.DropItem">
<em class="property">exception </em><tt class="descclassname">scrapy.exceptions.</tt><tt class="descname">DropItem</tt><a class="headerlink" href="#scrapy.exceptions.DropItem" title="永久链接至目标">¶</a></dt>
<dd></dd></dl>

<p>该异常由item pipeline抛出，用于停止处理item。详细内容请参考
<a class="reference internal" href="index.html#topics-item-pipeline"><em>Item Pipeline</em></a> 。</p>
</div>
<div class="section" id="closespider">
<h5>CloseSpider<a class="headerlink" href="#closespider" title="永久链接至标题">¶</a></h5>
<dl class="exception">
<dt id="scrapy.exceptions.CloseSpider">
<em class="property">exception </em><tt class="descclassname">scrapy.exceptions.</tt><tt class="descname">CloseSpider</tt><big>(</big><em>reason='cancelled'</em><big>)</big><a class="headerlink" href="#scrapy.exceptions.CloseSpider" title="永久链接至目标">¶</a></dt>
<dd><p>该异常由spider的回调函数(callback)抛出，来暂停/停止spider。支持的参数:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><strong>reason</strong> (<em>str</em>) &#8211; 关闭的原因</td>
</tr>
</tbody>
</table>
</dd></dl>

<p>样例:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">parse_page</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">if</span> <span class="s">&#39;Bandwidth exceeded&#39;</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">CloseSpider</span><span class="p">(</span><span class="s">&#39;bandwidth_exceeded&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="ignorerequest">
<h5>IgnoreRequest<a class="headerlink" href="#ignorerequest" title="永久链接至标题">¶</a></h5>
<dl class="exception">
<dt id="scrapy.exceptions.IgnoreRequest">
<em class="property">exception </em><tt class="descclassname">scrapy.exceptions.</tt><tt class="descname">IgnoreRequest</tt><a class="headerlink" href="#scrapy.exceptions.IgnoreRequest" title="永久链接至目标">¶</a></dt>
<dd></dd></dl>

<p>该异常由调度器(Scheduler)或其他下载中间件抛出，声明忽略该request。</p>
</div>
<div class="section" id="notconfigured">
<h5>NotConfigured<a class="headerlink" href="#notconfigured" title="永久链接至标题">¶</a></h5>
<dl class="exception">
<dt id="scrapy.exceptions.NotConfigured">
<em class="property">exception </em><tt class="descclassname">scrapy.exceptions.</tt><tt class="descname">NotConfigured</tt><a class="headerlink" href="#scrapy.exceptions.NotConfigured" title="永久链接至目标">¶</a></dt>
<dd></dd></dl>

<p>该异常由某些组件抛出，声明其仍然保持关闭。这些组件包括:</p>
<blockquote>
<div><ul class="simple">
<li>Extensions</li>
<li>Item pipelines</li>
<li>Downloader middlwares</li>
<li>Spider middlewares</li>
</ul>
</div></blockquote>
<p>该异常必须由组件的构造器(constructor)抛出。</p>
</div>
<div class="section" id="notsupported">
<h5>NotSupported<a class="headerlink" href="#notsupported" title="永久链接至标题">¶</a></h5>
<dl class="exception">
<dt id="scrapy.exceptions.NotSupported">
<em class="property">exception </em><tt class="descclassname">scrapy.exceptions.</tt><tt class="descname">NotSupported</tt><a class="headerlink" href="#scrapy.exceptions.NotSupported" title="永久链接至目标">¶</a></dt>
<dd></dd></dl>

<p>该异常声明一个不支持的特性。</p>
</div>
</div>
</div>
<span id="document-topics/exporters"></span><div class="section" id="module-scrapy.contrib.exporter">
<span id="item-exporters"></span><span id="topics-exporters"></span><h3>Item Exporters<a class="headerlink" href="#module-scrapy.contrib.exporter" title="永久链接至标题">¶</a></h3>
<p>当你抓取了你要的数据(Items)，你就会想要将他们持久化或导出它们，并应用在其他的程序。这是整个抓取过程的目的。</p>
<p>为此，Scrapy提供了Item Exporters 来创建不同的输出格式，如XML，CSV或JSON。</p>
<div class="section" id="item-exporter">
<h4>使用 Item Exporter<a class="headerlink" href="#item-exporter" title="永久链接至标题">¶</a></h4>
<p>如果你很忙，只想使用 Item Exporter 输出数据，请查看 <a class="reference internal" href="index.html#topics-feed-exports"><em>Feed exports</em></a>. 相反，如果你想知道Item Exporter 是如何工作的，或需要更多的自定义功能（不包括默认的 exports），请继续阅读下文。</p>
<p>为了使用 Item Exporter，你必须对 Item Exporter 及其参数 (args) 实例化。每个 Item Exporter 需要不同的参数，详细请查看 <a class="reference internal" href="index.html#topics-exporters-reference"><em>Item Exporters 参考资料</em></a> 。在实例化了 exporter 之后，你必须：</p>
<ol class="arabic simple">
<li>调用方法 <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter.start_exporting" title="scrapy.contrib.exporter.BaseItemExporter.start_exporting"><tt class="xref py py-meth docutils literal"><span class="pre">start_exporting()</span></tt></a> 以标识 exporting 过程的开始。</li>
<li>对要导出的每个项目调用 <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter.export_item" title="scrapy.contrib.exporter.BaseItemExporter.export_item"><tt class="xref py py-meth docutils literal"><span class="pre">export_item()</span></tt></a> 方法。</li>
<li>最后调用 <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter.finish_exporting" title="scrapy.contrib.exporter.BaseItemExporter.finish_exporting"><tt class="xref py py-meth docutils literal"><span class="pre">finish_exporting()</span></tt></a> 表示 exporting 过程的结束</li>
</ol>
<p>这里，你可以看到一个 <a class="reference internal" href="index.html#document-topics/item-pipeline"><em>Item Pipeline</em></a> ，它使用 Item Exporter 导出 items 到不同的文件，每个 spider 一个:</p>
<div class="highlight-python"><div class="highlight"><pre>from scrapy import signals
from scrapy.contrib.exporter import XmlItemExporter

class XmlExportPipeline(object):

    def __init__(self):
        self.files = {}

     @classmethod
     def from_crawler(cls, crawler):
         pipeline = cls()
         crawler.signals.connect(pipeline.spider_opened, signals.spider_opened)
         crawler.signals.connect(pipeline.spider_closed, signals.spider_closed)
         return pipeline

    def spider_opened(self, spider):
        file = open(&#39;%s_products.xml&#39; % spider.name, &#39;w+b&#39;)
        self.files[spider] = file
        self.exporter = XmlItemExporter(file)
        self.exporter.start_exporting()

    def spider_closed(self, spider):
        self.exporter.finish_exporting()
        file = self.files.pop(spider)
        file.close()

    def process_item(self, item, spider):
        self.exporter.export_item(item)
        return item
</pre></div>
</div>
<p id="topics-exporters-field-serialization">序列化 item fields</p>
<hr class="docutils" />
<p>B默认情况下，该字段值将不变的传递到序列化库，如何对其进行序列化的决定被委托给每一个特定的序列化库。</p>
<p>但是，你可以自定义每个字段值如何序列化在它被传递到序列化库中之前。</p>
<p>有两种方法可以自定义一个字段如何被序列化，请看下文。</p>
<div class="section" id="field-serializer">
<span id="topics-exporters-serializers"></span><h5>1. 在 field 类中声明一个 serializer<a class="headerlink" href="#field-serializer" title="永久链接至标题">¶</a></h5>
<p>您可以在 <a class="reference internal" href="index.html#topics-items-fields"><em>field metadata</em></a> 声明一个 serializer。该 serializer 必须可调用，并返回它的序列化形式。</p>
<p>实例:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">def</span> <span class="nf">serialize_price</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
    <span class="k">return</span> <span class="s">&#39;$ </span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Product</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">price</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">serializer</span><span class="o">=</span><span class="n">serialize_price</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="overriding-serialize-field">
<h5>2. 覆盖(overriding) serialize_field() 方法<a class="headerlink" href="#overriding-serialize-field" title="永久链接至标题">¶</a></h5>
<p>你可以覆盖 <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter.serialize_field" title="scrapy.contrib.exporter.BaseItemExporter.serialize_field"><tt class="xref py py-meth docutils literal"><span class="pre">serialize_field()</span></tt></a> 方法来自定义如何输出你的数据。</p>
<p>在你的自定义代码后确保你调用父类的 <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter.serialize_field" title="scrapy.contrib.exporter.BaseItemExporter.serialize_field"><tt class="xref py py-meth docutils literal"><span class="pre">serialize_field()</span></tt></a> 方法。</p>
<p>实例:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.exporter</span> <span class="kn">import</span> <span class="n">XmlItemExporter</span>

<span class="k">class</span> <span class="nc">ProductXmlExporter</span><span class="p">(</span><span class="n">XmlItemExporter</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">serialize_field</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">field</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">field</span> <span class="o">==</span> <span class="s">&#39;price&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="s">&#39;$ </span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">Product</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">serialize_field</span><span class="p">(</span><span class="n">field</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="topics-exporters-reference">
<span id="id1"></span><h4>Item Exporters 参考资料<a class="headerlink" href="#topics-exporters-reference" title="永久链接至标题">¶</a></h4>
<p>下面是一些Scrapy内置的 Item Exporters类. 其中一些包括了实例, 假设你要输出以下2个Items:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">Item</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">&#39;Color TV&#39;</span><span class="p">,</span> <span class="n">price</span><span class="o">=</span><span class="s">&#39;1200&#39;</span><span class="p">)</span>
<span class="n">Item</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">&#39;DVD player&#39;</span><span class="p">,</span> <span class="n">price</span><span class="o">=</span><span class="s">&#39;200&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="baseitemexporter">
<h5>BaseItemExporter<a class="headerlink" href="#baseitemexporter" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.exporter.BaseItemExporter">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.exporter.</tt><tt class="descname">BaseItemExporter</tt><big>(</big><em>fields_to_export=None</em>, <em>export_empty_fields=False</em>, <em>encoding='utf-8'</em><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.BaseItemExporter" title="永久链接至目标">¶</a></dt>
<dd><p>这是一个对所有 Item Exporters 的(抽象)父类。它对所有(具体) Item Exporters 提供基本属性，如定义export什么fields, 是否export空fields, 或是否进行编码。</p>
<p>你可以在构造器中设置它们不同的属性值: <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter.fields_to_export" title="scrapy.contrib.exporter.BaseItemExporter.fields_to_export"><tt class="xref py py-attr docutils literal"><span class="pre">fields_to_export</span></tt></a> ,
<a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter.export_empty_fields" title="scrapy.contrib.exporter.BaseItemExporter.export_empty_fields"><tt class="xref py py-attr docutils literal"><span class="pre">export_empty_fields</span></tt></a>, <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter.encoding" title="scrapy.contrib.exporter.BaseItemExporter.encoding"><tt class="xref py py-attr docutils literal"><span class="pre">encoding</span></tt></a>.</p>
<dl class="method">
<dt id="scrapy.contrib.exporter.BaseItemExporter.export_item">
<tt class="descname">export_item</tt><big>(</big><em>item</em><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.BaseItemExporter.export_item" title="永久链接至目标">¶</a></dt>
<dd><p>输出给定item. 此方法必须在子类中实现.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.exporter.BaseItemExporter.serialize_field">
<tt class="descname">serialize_field</tt><big>(</big><em>field</em>, <em>name</em>, <em>value</em><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.BaseItemExporter.serialize_field" title="永久链接至目标">¶</a></dt>
<dd><p>返回给定field的序列化值. 你可以覆盖此方法来控制序列化或输出指定的field.</p>
<p>默认情况下, 此方法寻找一个 serializer <a class="reference internal" href="index.html#topics-exporters-serializers"><em>在 item
field 中声明</em></a> 并返回它的值. 如果没有发现   serializer, 则值不会改变，除非你使用 <tt class="docutils literal"><span class="pre">unicode</span></tt> 值并编码到
<tt class="docutils literal"><span class="pre">str</span></tt>， 编码可以在 <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter.encoding" title="scrapy.contrib.exporter.BaseItemExporter.encoding"><tt class="xref py py-attr docutils literal"><span class="pre">encoding</span></tt></a> 属性中声明.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>field</strong> (<a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a> object) &#8211; the field being serialized</li>
<li><strong>name</strong> (<em>str</em>) &#8211; the name of the field being serialized</li>
<li><strong>value</strong> &#8211; the value being serialized</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.exporter.BaseItemExporter.start_exporting">
<tt class="descname">start_exporting</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.BaseItemExporter.start_exporting" title="永久链接至目标">¶</a></dt>
<dd><p>表示exporting过程的开始. 一些exporters用于产生需要的头元素(例如
<a class="reference internal" href="index.html#scrapy.contrib.exporter.XmlItemExporter" title="scrapy.contrib.exporter.XmlItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">XmlItemExporter</span></tt></a>). 在实现exporting item前必须调用此方法.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.exporter.BaseItemExporter.finish_exporting">
<tt class="descname">finish_exporting</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.BaseItemExporter.finish_exporting" title="永久链接至目标">¶</a></dt>
<dd><p>表示exporting过程的结束. 一些exporters用于产生需要的尾元素 (例如
<a class="reference internal" href="index.html#scrapy.contrib.exporter.XmlItemExporter" title="scrapy.contrib.exporter.XmlItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">XmlItemExporter</span></tt></a>). 在完成exporting item后必须调用此方法.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.exporter.BaseItemExporter.fields_to_export">
<tt class="descname">fields_to_export</tt><a class="headerlink" href="#scrapy.contrib.exporter.BaseItemExporter.fields_to_export" title="永久链接至目标">¶</a></dt>
<dd><p>列出export什么fields值, None表示export所有fields. 默认值为None.</p>
<p>一些 exporters (例如 <a class="reference internal" href="index.html#scrapy.contrib.exporter.CsvItemExporter" title="scrapy.contrib.exporter.CsvItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">CsvItemExporter</span></tt></a>) 按照定义在属性中fields的次序依次输出.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.exporter.BaseItemExporter.export_empty_fields">
<tt class="descname">export_empty_fields</tt><a class="headerlink" href="#scrapy.contrib.exporter.BaseItemExporter.export_empty_fields" title="永久链接至目标">¶</a></dt>
<dd><p>是否在输出数据中包含为空的item fields.
默认值是 <tt class="docutils literal"><span class="pre">False</span></tt>. 一些 exporters (例如 <a class="reference internal" href="index.html#scrapy.contrib.exporter.CsvItemExporter" title="scrapy.contrib.exporter.CsvItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">CsvItemExporter</span></tt></a>)
会忽略此属性并输出所有fields.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.exporter.BaseItemExporter.encoding">
<tt class="descname">encoding</tt><a class="headerlink" href="#scrapy.contrib.exporter.BaseItemExporter.encoding" title="永久链接至目标">¶</a></dt>
<dd><p>Encoding 属性将用于编码 unicode 值. (仅用于序列化字符串).其他值类型将不变的传递到指定的序列化库.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="xmlitemexporter">
<h5>XmlItemExporter<a class="headerlink" href="#xmlitemexporter" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.exporter.XmlItemExporter">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.exporter.</tt><tt class="descname">XmlItemExporter</tt><big>(</big><em>file</em>, <em>item_element='item'</em>, <em>root_element='items'</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.XmlItemExporter" title="永久链接至目标">¶</a></dt>
<dd><p>以XML格式 exports Items 到指定的文件类.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>file</strong> &#8211; 文件类.</li>
<li><strong>root_element</strong> (<em>str</em>) &#8211; XML 根元素名.</li>
<li><strong>item_element</strong> (<em>str</em>) &#8211; XML item 的元素名.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>构造器额外的关键字参数将传给 <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter" title="scrapy.contrib.exporter.BaseItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></tt></a> 构造器.</p>
<p>一个典型的 exporter 实例:</p>
<div class="highlight-none"><div class="highlight"><pre>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;items&gt;
  &lt;item&gt;
    &lt;name&gt;Color TV&lt;/name&gt;
    &lt;price&gt;1200&lt;/price&gt;
 &lt;/item&gt;
  &lt;item&gt;
    &lt;name&gt;DVD player&lt;/name&gt;
    &lt;price&gt;200&lt;/price&gt;
 &lt;/item&gt;
&lt;/items&gt;
</pre></div>
</div>
<p>除了覆盖 <tt class="xref py py-meth docutils literal"><span class="pre">serialize_field()</span></tt> 方法, 多个值的 fields 会转化每个值到 <tt class="docutils literal"><span class="pre">&lt;value&gt;</span></tt> 元素.</p>
<p>例如, item:</p>
<div class="highlight-none"><div class="highlight"><pre>Item(name=[&#39;John&#39;, &#39;Doe&#39;], age=&#39;23&#39;)
</pre></div>
</div>
<p>将被转化为:</p>
<div class="highlight-none"><div class="highlight"><pre>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;items&gt;
  &lt;item&gt;
    &lt;name&gt;
      &lt;value&gt;John&lt;/value&gt;
      &lt;value&gt;Doe&lt;/value&gt;
    &lt;/name&gt;
    &lt;age&gt;23&lt;/age&gt;
  &lt;/item&gt;
&lt;/items&gt;
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="csvitemexporter">
<h5>CsvItemExporter<a class="headerlink" href="#csvitemexporter" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.exporter.CsvItemExporter">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.exporter.</tt><tt class="descname">CsvItemExporter</tt><big>(</big><em>file</em>, <em>include_headers_line=True</em>, <em>join_multivalued='</em>, <em>'</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.CsvItemExporter" title="永久链接至目标">¶</a></dt>
<dd><p>输出 csv 文件格式. 如果添加 <tt class="xref py py-attr docutils literal"><span class="pre">fields_to_export</span></tt> 属性, 它会按顺序定义CSV的列名. <tt class="xref py py-attr docutils literal"><span class="pre">export_empty_fields</span></tt> 属性在此没有作用.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>file</strong> &#8211; 文件类.</li>
<li><strong>include_headers_line</strong> (<em>str</em>) &#8211; 启用后 exporter 会输出第一行为列名, 列名从 <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter.fields_to_export" title="scrapy.contrib.exporter.BaseItemExporter.fields_to_export"><tt class="xref py py-attr docutils literal"><span class="pre">BaseItemExporter.fields_to_export</span></tt></a> 或第一个 item fields 获取.</li>
<li><strong>join_multivalued</strong> &#8211; char 将用于连接多个值的fields.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>此构造器额外的关键字参数将传给 <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter" title="scrapy.contrib.exporter.BaseItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></tt></a> 构造器 , 其余的将传给 <a class="reference external" href="http://docs.python.org/library/csv.html#csv.writer">csv.writer</a> 构造器, 以此来定制 exporter.</p>
<p>一个典型的 exporter 实例:</p>
<div class="highlight-none"><div class="highlight"><pre>product,price
Color TV,1200
DVD player,200
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="pickleitemexporter">
<h5>PickleItemExporter<a class="headerlink" href="#pickleitemexporter" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.exporter.PickleItemExporter">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.exporter.</tt><tt class="descname">PickleItemExporter</tt><big>(</big><em>file</em>, <em>protocol=0</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.PickleItemExporter" title="永久链接至目标">¶</a></dt>
<dd><p>输出 pickle 文件格式.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><ul class="first last simple">
<li><strong>file</strong> &#8211; 文件类.</li>
<li><strong>protocol</strong> (<em>int</em>) &#8211; pickle 协议.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>更多信息请看 <a class="reference external" href="http://docs.python.org/library/pickle.html">pickle module documentation</a>.</p>
<p>此构造器额外的关键字参数将传给 <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter" title="scrapy.contrib.exporter.BaseItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></tt></a> 构造器.</p>
<p>Pickle 不是可读的格式，这里不提供实例.</p>
</dd></dl>

</div>
<div class="section" id="pprintitemexporter">
<h5>PprintItemExporter<a class="headerlink" href="#pprintitemexporter" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.exporter.PprintItemExporter">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.exporter.</tt><tt class="descname">PprintItemExporter</tt><big>(</big><em>file</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.PprintItemExporter" title="永久链接至目标">¶</a></dt>
<dd><p>输出整齐打印的文件格式.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><strong>file</strong> &#8211; 文件类.</td>
</tr>
</tbody>
</table>
<p>此构造器额外的关键字参数将传给 <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter" title="scrapy.contrib.exporter.BaseItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></tt></a> 构造器.</p>
<p>一个典型的 exporter 实例:</p>
<div class="highlight-none"><div class="highlight"><pre>{&#39;name&#39;: &#39;Color TV&#39;, &#39;price&#39;: &#39;1200&#39;}
{&#39;name&#39;: &#39;DVD player&#39;, &#39;price&#39;: &#39;200&#39;}
</pre></div>
</div>
<p>此格式会根据行的长短进行调整.</p>
</dd></dl>

</div>
<div class="section" id="jsonitemexporter">
<h5>JsonItemExporter<a class="headerlink" href="#jsonitemexporter" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.exporter.JsonItemExporter">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.exporter.</tt><tt class="descname">JsonItemExporter</tt><big>(</big><em>file</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.JsonItemExporter" title="永久链接至目标">¶</a></dt>
<dd><p>输出 JSON 文件格式, 所有对象将写进一个对象的列表. 此构造器额外的关键字参数将传给 <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter" title="scrapy.contrib.exporter.BaseItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></tt></a> 构造器, 其余的将传给 <a class="reference external" href="http://docs.python.org/library/json.html#json.JSONEncoder">JSONEncoder</a> 构造器, 以此来定制 exporter.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><strong>file</strong> &#8211; 文件类.</td>
</tr>
</tbody>
</table>
<p>一个典型的 exporter 实例:</p>
<div class="highlight-none"><div class="highlight"><pre>[{&quot;name&quot;: &quot;Color TV&quot;, &quot;price&quot;: &quot;1200&quot;},
{&quot;name&quot;: &quot;DVD player&quot;, &quot;price&quot;: &quot;200&quot;}]
</pre></div>
</div>
<div class="admonition warning" id="json-with-large-data">
<p class="first admonition-title">警告</p>
<p class="last">JSON 是一个简单而有弹性的格式, 但对大量数据的扩展性不是很好，因为这里会将整个对象放入内存. 如果你要JSON既强大又简单,可以考虑 <a class="reference internal" href="index.html#scrapy.contrib.exporter.JsonLinesItemExporter" title="scrapy.contrib.exporter.JsonLinesItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">JsonLinesItemExporter</span></tt></a> , 或把输出对象分为多个块.</p>
</div>
</dd></dl>

</div>
<div class="section" id="jsonlinesitemexporter">
<h5>JsonLinesItemExporter<a class="headerlink" href="#jsonlinesitemexporter" title="永久链接至标题">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.exporter.JsonLinesItemExporter">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.exporter.</tt><tt class="descname">JsonLinesItemExporter</tt><big>(</big><em>file</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.JsonLinesItemExporter" title="永久链接至目标">¶</a></dt>
<dd><p>输出 JSON 文件格式, 每行写一个 JSON-encoded 项. 此构造器额外的关键字参数将传给 <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter" title="scrapy.contrib.exporter.BaseItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></tt></a> 构造器, 其余的将传给 <a class="reference external" href="http://docs.python.org/library/json.html#json.JSONEncoder">JSONEncoder</a> 构造器, 以此来定制 exporter.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">参数:</th><td class="field-body"><strong>file</strong> &#8211; 文件类.</td>
</tr>
</tbody>
</table>
<p>一个典型的 exporter 实例:</p>
<div class="highlight-none"><div class="highlight"><pre>{&quot;name&quot;: &quot;Color TV&quot;, &quot;price&quot;: &quot;1200&quot;}
{&quot;name&quot;: &quot;DVD player&quot;, &quot;price&quot;: &quot;200&quot;}
</pre></div>
</div>
<p>这个类能很好的处理大量数据.</p>
</dd></dl>

</div>
</div>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-topics/commands"><em>命令行工具(Command line tools)</em></a></dt>
<dd>学习命令行工具及所有 <a class="reference internal" href="index.html#topics-commands-ref"><em>可用的命令</em></a> 。</dd>
<dt><a class="reference internal" href="index.html#document-topics/request-response"><em>Requests and Responses</em></a></dt>
<dd>了解代表HTTP请求和回复的request,response类</dd>
<dt><a class="reference internal" href="index.html#document-topics/settings"><em>Settings</em></a></dt>
<dd>了解如何配置Scrapy及所有 <a class="reference internal" href="index.html#topics-settings-ref"><em>可用的设置</em></a> 。</dd>
<dt><a class="reference internal" href="index.html#document-topics/signals"><em>信号(Signals)</em></a></dt>
<dd>查看如何使用及所有可用的信号</dd>
<dt><a class="reference internal" href="index.html#document-topics/exceptions"><em>异常(Exceptions)</em></a></dt>
<dd>查看所有可用的exception以及相应的意义。</dd>
<dt><a class="reference internal" href="index.html#document-topics/exporters"><em>Item Exporters</em></a></dt>
<dd>快速将您爬取到的item导出到文件中(XML, CSV等格式)</dd>
</dl>
</div>
<div class="section" id="id8">
<h2>其他<a class="headerlink" href="#id8" title="永久链接至标题">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-news"></span><div class="section" id="release-notes">
<span id="news"></span><h3>Release notes<a class="headerlink" href="#release-notes" title="永久链接至标题">¶</a></h3>
<div class="section" id="id1">
<h4>0.24.0 (2014-06-26)<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h4>
<div class="section" id="enhancements">
<h5>Enhancements<a class="headerlink" href="#enhancements" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>Improve Scrapy top-level namespace (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/494">issue 494</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/684">issue 684</a>)</li>
<li>Add selector shortcuts to responses (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/554">issue 554</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/690">issue 690</a>)</li>
<li>Add new lxml based LinkExtractor to replace unmantained SgmlLinkExtractor
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/559">issue 559</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/761">issue 761</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/763">issue 763</a>)</li>
<li>Cleanup settings API - part of per-spider settings <strong>GSoC project</strong> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/737">issue 737</a>)</li>
<li>Add UTF8 encoding header to templates (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/688">issue 688</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/762">issue 762</a>)</li>
<li>Telnet console now binds to 127.0.0.1 by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/699">issue 699</a>)</li>
<li>Update debian/ubuntu install instructions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/509">issue 509</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/549">issue 549</a>)</li>
<li>Disable smart strings in lxml XPath evaluations (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/535">issue 535</a>)</li>
<li>Restore filesystem based cache as default for http
cache middleware (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/541">issue 541</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/500">issue 500</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/571">issue 571</a>)</li>
<li>Expose current crawler in Scrapy shell (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/557">issue 557</a>)</li>
<li>Improve testsuite comparing CSV and XML exporters (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/570">issue 570</a>)</li>
<li>New <cite>offsite/filtered</cite> and <cite>offsite/domains</cite> stats (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/566">issue 566</a>)</li>
<li>Support process_links as generator in CrawlSpider (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/555">issue 555</a>)</li>
<li>Verbose logging and new stats counters for DupeFilter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/553">issue 553</a>)</li>
<li>Add a mimetype parameter to <cite>MailSender.send()</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/602">issue 602</a>)</li>
<li>Generalize file pipeline log messages (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/622">issue 622</a>)</li>
<li>Replace unencodeable codepoints with html entities in SGMLLinkExtractor (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/565">issue 565</a>)</li>
<li>Converted SEP documents to rst format (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/629">issue 629</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/630">issue 630</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/638">issue 638</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/632">issue 632</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/636">issue 636</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/640">issue 640</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/635">issue 635</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/634">issue 634</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/639">issue 639</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/637">issue 637</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/631">issue 631</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/633">issue 633</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/641">issue 641</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/642">issue 642</a>)</li>
<li>Tests and docs for clickdata&#8217;s nr index in FormRequest (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/646">issue 646</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/645">issue 645</a>)</li>
<li>Allow to disable a downloader handler just like any other component (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/650">issue 650</a>)</li>
<li>Log when a request is discarded after too many redirections (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/654">issue 654</a>)</li>
<li>Log error responses if they are not handled by spider callbacks
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/612">issue 612</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/656">issue 656</a>)</li>
<li>Add content-type check to http compression mw (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/193">issue 193</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/660">issue 660</a>)</li>
<li>Run pypy tests using latest pypi from ppa (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/674">issue 674</a>)</li>
<li>Run test suite using pytest instead of trial (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/679">issue 679</a>)</li>
<li>Build docs and check for dead links in tox environment (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/687">issue 687</a>)</li>
<li>Make scrapy.version_info a tuple of integers (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/681">issue 681</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/692">issue 692</a>)</li>
<li>Infer exporter&#8217;s output format from filename extensions
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/546">issue 546</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/659">issue 659</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/760">issue 760</a>)</li>
<li>Support case-insensitive domains in <cite>url_is_from_any_domain()</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/693">issue 693</a>)</li>
<li>Remove pep8 warnings in project and spider templates (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/698">issue 698</a>)</li>
<li>Tests and docs for <cite>request_fingerprint</cite> function (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/597">issue 597</a>)</li>
<li>Update SEP-19 for GSoC project <cite>per-spider settings</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/705">issue 705</a>)</li>
<li>Set exit code to non-zero when contracts fails (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/727">issue 727</a>)</li>
<li>Add a setting to control what class is instanciated as Downloader component
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/738">issue 738</a>)</li>
<li>Pass response in <cite>item_dropped</cite> signal (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/724">issue 724</a>)</li>
<li>Improve <cite>scrapy check</cite> contracts command (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/733">issue 733</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/752">issue 752</a>)</li>
<li>Document <cite>spider.closed()</cite> shortcut (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/719">issue 719</a>)</li>
<li>Document <cite>request_scheduled</cite> signal (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/746">issue 746</a>)</li>
<li>Add a note about reporting security issues (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/697">issue 697</a>)</li>
<li>Add LevelDB http cache storage backend (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/626">issue 626</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/500">issue 500</a>)</li>
<li>Sort spider list output of <cite>scrapy list</cite> command (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/742">issue 742</a>)</li>
<li>Multiple documentation enhancemens and fixes
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/575">issue 575</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/587">issue 587</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/590">issue 590</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/596">issue 596</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/610">issue 610</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/617">issue 617</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/618">issue 618</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/627">issue 627</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/613">issue 613</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/643">issue 643</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/654">issue 654</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/675">issue 675</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/663">issue 663</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/711">issue 711</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/714">issue 714</a>)</li>
</ul>
</div>
<div class="section" id="bugfixes">
<h5>Bugfixes<a class="headerlink" href="#bugfixes" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>Encode unicode URL value when creating Links in RegexLinkExtractor (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/561">issue 561</a>)</li>
<li>Ignore None values in ItemLoader processors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/556">issue 556</a>)</li>
<li>Fix link text when there is an inner tag in SGMLLinkExtractor and
HtmlParserLinkExtractor (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/485">issue 485</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/574">issue 574</a>)</li>
<li>Fix wrong checks on subclassing of deprecated classes
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/581">issue 581</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/584">issue 584</a>)</li>
<li>Handle errors caused by inspect.stack() failures (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/582">issue 582</a>)</li>
<li>Fix a reference to unexistent engine attribute (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/593">issue 593</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/594">issue 594</a>)</li>
<li>Fix dynamic itemclass example usage of type() (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/603">issue 603</a>)</li>
<li>Use lucasdemarchi/codespell to fix typos (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/628">issue 628</a>)</li>
<li>Fix default value of attrs argument in SgmlLinkExtractor to be tuple (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/661">issue 661</a>)</li>
<li>Fix XXE flaw in sitemap reader (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/676">issue 676</a>)</li>
<li>Fix engine to support filtered start requests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/707">issue 707</a>)</li>
<li>Fix offsite middleware case on urls with no hostnames (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/745">issue 745</a>)</li>
<li>Testsuite doesn&#8217;t require PIL anymore (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/585">issue 585</a>)</li>
</ul>
</div>
</div>
<div class="section" id="released-2014-02-14">
<h4>0.22.2 (released 2014-02-14)<a class="headerlink" href="#released-2014-02-14" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>fix a reference to unexistent engine.slots. closes #593 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/13c099a">commit 13c099a</a>)</li>
<li>downloaderMW doc typo (spiderMW doc copy remnant) (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8ae11bf">commit 8ae11bf</a>)</li>
<li>Correct typos (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1346037">commit 1346037</a>)</li>
</ul>
</div>
<div class="section" id="released-2014-02-08">
<h4>0.22.1 (released 2014-02-08)<a class="headerlink" href="#released-2014-02-08" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>localhost666 can resolve under certain circumstances (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2ec2279">commit 2ec2279</a>)</li>
<li>test inspect.stack failure (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cc3eda3">commit cc3eda3</a>)</li>
<li>Handle cases when inspect.stack() fails (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8cb44f9">commit 8cb44f9</a>)</li>
<li>Fix wrong checks on subclassing of deprecated classes. closes #581 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/46d98d6">commit 46d98d6</a>)</li>
<li>Docs: 4-space indent for final spider example (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/13846de">commit 13846de</a>)</li>
<li>Fix HtmlParserLinkExtractor and tests after #485 merge (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/368a946">commit 368a946</a>)</li>
<li>BaseSgmlLinkExtractor: Fixed the missing space when the link has an inner tag (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b566388">commit b566388</a>)</li>
<li>BaseSgmlLinkExtractor: Added unit test of a link with an inner tag (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c1cb418">commit c1cb418</a>)</li>
<li>BaseSgmlLinkExtractor: Fixed unknown_endtag() so that it only set current_link=None when the end tag match the opening tag (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7e4d627">commit 7e4d627</a>)</li>
<li>Fix tests for Travis-CI build (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/76c7e20">commit 76c7e20</a>)</li>
<li>replace unencodeable codepoints with html entities. fixes #562 and #285 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5f87b17">commit 5f87b17</a>)</li>
<li>RegexLinkExtractor: encode URL unicode value when creating Links (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d0ee545">commit d0ee545</a>)</li>
<li>Updated the tutorial crawl output with latest output. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8da65de">commit 8da65de</a>)</li>
<li>Updated shell docs with the crawler reference and fixed the actual shell output. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/875b9ab">commit 875b9ab</a>)</li>
<li>PEP8 minor edits. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/f89efaf">commit f89efaf</a>)</li>
<li>Expose current crawler in the scrapy shell. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5349cec">commit 5349cec</a>)</li>
<li>Unused re import and PEP8 minor edits. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/387f414">commit 387f414</a>)</li>
<li>Ignore None&#8217;s values when using the ItemLoader. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0632546">commit 0632546</a>)</li>
<li>DOC Fixed HTTPCACHE_STORAGE typo in the default value which is now Filesystem instead Dbm. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cde9a8c">commit cde9a8c</a>)</li>
<li>show ubuntu setup instructions as literal code (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fb5c9c5">commit fb5c9c5</a>)</li>
<li>Update Ubuntu installation instructions (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/70fb105">commit 70fb105</a>)</li>
<li>Merge pull request #550 from stray-leone/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6f70b6a">commit 6f70b6a</a>)</li>
<li>modify the version of scrapy ubuntu package (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/725900d">commit 725900d</a>)</li>
<li>fix 0.22.0 release date (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/af0219a">commit af0219a</a>)</li>
<li>fix typos in news.rst and remove (not released yet) header (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b7f58f4">commit b7f58f4</a>)</li>
</ul>
</div>
<div class="section" id="released-2014-01-17">
<h4>0.22.0 (released 2014-01-17)<a class="headerlink" href="#released-2014-01-17" title="永久链接至标题">¶</a></h4>
<div class="section" id="id2">
<h5>Enhancements<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>[<strong>Backwards incompatible</strong>] Switched HTTPCacheMiddleware backend to filesystem (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/541">issue 541</a>)
To restore old backend set <cite>HTTPCACHE_STORAGE</cite> to <cite>scrapy.contrib.httpcache.DbmCacheStorage</cite></li>
<li>Proxy https:// urls using CONNECT method (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/392">issue 392</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/397">issue 397</a>)</li>
<li>Add a middleware to crawl ajax crawleable pages as defined by google (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/343">issue 343</a>)</li>
<li>Rename scrapy.spider.BaseSpider to scrapy.spider.Spider (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/510">issue 510</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/519">issue 519</a>)</li>
<li>Selectors register EXSLT namespaces by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/472">issue 472</a>)</li>
<li>Unify item loaders similar to selectors renaming (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/461">issue 461</a>)</li>
<li>Make <cite>RFPDupeFilter</cite> class easily subclassable (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/533">issue 533</a>)</li>
<li>Improve test coverage and forthcoming Python 3 support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/525">issue 525</a>)</li>
<li>Promote startup info on settings and middleware to INFO level (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/520">issue 520</a>)</li>
<li>Support partials in <cite>get_func_args</cite> util (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/506">issue 506</a>, issue:<cite>504</cite>)</li>
<li>Allow running indiviual tests via tox (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/503">issue 503</a>)</li>
<li>Update extensions ignored by link extractors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/498">issue 498</a>)</li>
<li>Add middleware methods to get files/images/thumbs paths (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/490">issue 490</a>)</li>
<li>Improve offsite middleware tests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/478">issue 478</a>)</li>
<li>Add a way to skip default Referer header set by RefererMiddleware (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/475">issue 475</a>)</li>
<li>Do not send <cite>x-gzip</cite> in default <cite>Accept-Encoding</cite> header (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/469">issue 469</a>)</li>
<li>Support defining http error handling using settings (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/466">issue 466</a>)</li>
<li>Use modern python idioms wherever you find legacies (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/497">issue 497</a>)</li>
<li>Improve and correct documentation
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/527">issue 527</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/524">issue 524</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/521">issue 521</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/517">issue 517</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/512">issue 512</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/505">issue 505</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/502">issue 502</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/489">issue 489</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/465">issue 465</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/460">issue 460</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/425">issue 425</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/536">issue 536</a>)</li>
</ul>
</div>
<div class="section" id="fixes">
<h5>Fixes<a class="headerlink" href="#fixes" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>Update Selector class imports in CrawlSpider template (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/484">issue 484</a>)</li>
<li>Fix unexistent reference to <cite>engine.slots</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/464">issue 464</a>)</li>
<li>Do not try to call <cite>body_as_unicode()</cite> on a non-TextResponse instance (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/462">issue 462</a>)</li>
<li>Warn when subclassing XPathItemLoader, previously it only warned on
instantiation. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/523">issue 523</a>)</li>
<li>Warn when subclassing XPathSelector, previously it only warned on
instantiation. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/537">issue 537</a>)</li>
<li>Multiple fixes to memory stats (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/531">issue 531</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/530">issue 530</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/529">issue 529</a>)</li>
<li>Fix overriding url in <cite>FormRequest.from_response()</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/507">issue 507</a>)</li>
<li>Fix tests runner under pip 1.5 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/513">issue 513</a>)</li>
<li>Fix logging error when spider name is unicode (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/479">issue 479</a>)</li>
</ul>
</div>
</div>
<div class="section" id="released-2013-12-09">
<h4>0.20.2 (released 2013-12-09)<a class="headerlink" href="#released-2013-12-09" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>Update CrawlSpider Template with Selector changes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6d1457d">commit 6d1457d</a>)</li>
<li>fix method name in tutorial. closes GH-480 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b4fc359">commit b4fc359</a></li>
</ul>
</div>
<div class="section" id="released-2013-11-28">
<h4>0.20.1 (released 2013-11-28)<a class="headerlink" href="#released-2013-11-28" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>include_package_data is required to build wheels from published sources (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5ba1ad5">commit 5ba1ad5</a>)</li>
<li>process_parallel was leaking the failures on its internal deferreds.  closes #458 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/419a780">commit 419a780</a>)</li>
</ul>
</div>
<div class="section" id="released-2013-11-08">
<h4>0.20.0 (released 2013-11-08)<a class="headerlink" href="#released-2013-11-08" title="永久链接至标题">¶</a></h4>
<div class="section" id="id3">
<h5>Enhancements<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>New Selector&#8217;s API including CSS selectors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/395">issue 395</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues/426">issue 426</a>),</li>
<li>Request/Response url/body attributes are now immutable
(modifying them had been deprecated for a long time)</li>
<li><a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES"><tt class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></tt></a> is now defined as a dict (instead of a list)</li>
<li>Sitemap spider can fetch alternate URLs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/360">issue 360</a>)</li>
<li><cite>Selector.remove_namespaces()</cite> now remove namespaces from element&#8217;s attributes. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/416">issue 416</a>)</li>
<li>Paved the road for Python 3.3+ (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/435">issue 435</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/436">issue 436</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/431">issue 431</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/452">issue 452</a>)</li>
<li>New item exporter using native python types with nesting support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/366">issue 366</a>)</li>
<li>Tune HTTP1.1 pool size so it matches concurrency defined by settings (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b43b5f575">commit b43b5f575</a>)</li>
<li>scrapy.mail.MailSender now can connect over TLS or upgrade using STARTTLS (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/327">issue 327</a>)</li>
<li>New FilesPipeline with functionality factored out from ImagesPipeline (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/370">issue 370</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/409">issue 409</a>)</li>
<li>Recommend Pillow instead of PIL for image handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/317">issue 317</a>)</li>
<li>Added debian packages for Ubuntu quantal and raring (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/86230c0">commit 86230c0</a>)</li>
<li>Mock server (used for tests) can listen for HTTPS requests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/410">issue 410</a>)</li>
<li>Remove multi spider support from multiple core components
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/422">issue 422</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/421">issue 421</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/420">issue 420</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/419">issue 419</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/423">issue 423</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/418">issue 418</a>)</li>
<li>Travis-CI now tests Scrapy changes against development versions of <cite>w3lib</cite> and <cite>queuelib</cite> python packages.</li>
<li>Add pypy 2.1 to continuous integration tests (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ecfa7431">commit ecfa7431</a>)</li>
<li>Pylinted, pep8 and removed old-style exceptions from source (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/430">issue 430</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/432">issue 432</a>)</li>
<li>Use importlib for parametric imports (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/445">issue 445</a>)</li>
<li>Handle a regression introduced in Python 2.7.5 that affects XmlItemExporter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/372">issue 372</a>)</li>
<li>Bugfix crawling shutdown on SIGINT (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/450">issue 450</a>)</li>
<li>Do not submit <cite>reset</cite> type inputs in FormRequest.from_response (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b326b87">commit b326b87</a>)</li>
<li>Do not silence download errors when request errback raises an exception (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/684cfc0">commit 684cfc0</a>)</li>
</ul>
</div>
<div class="section" id="id4">
<h5>Bugfixes<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>Fix tests under Django 1.6 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b6bed44c">commit b6bed44c</a>)</li>
<li>Lot of bugfixes to retry middleware under disconnections using HTTP 1.1 download handler</li>
<li>Fix inconsistencies among Twisted releases (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/406">issue 406</a>)</li>
<li>Fix scrapy shell bugs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/418">issue 418</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/407">issue 407</a>)</li>
<li>Fix invalid variable name in setup.py (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/429">issue 429</a>)</li>
<li>Fix tutorial references (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/387">issue 387</a>)</li>
<li>Improve request-response docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/391">issue 391</a>)</li>
<li>Improve best practices docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/399">issue 399</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/400">issue 400</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/401">issue 401</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/402">issue 402</a>)</li>
<li>Improve django integration docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/404">issue 404</a>)</li>
<li>Document <cite>bindaddress</cite> request meta (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/37c24e01d7">commit 37c24e01d7</a>)</li>
<li>Improve <cite>Request</cite> class documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/226">issue 226</a>)</li>
</ul>
</div>
<div class="section" id="other">
<h5>Other<a class="headerlink" href="#other" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>Dropped Python 2.6 support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/448">issue 448</a>)</li>
<li>Add <a class="reference external" href="https://github.com/SimonSapin/cssselect">cssselect</a> python package as install dependency</li>
<li>Drop libxml2 and multi selector&#8217;s backend support, <a class="reference external" href="http://lxml.de/">lxml</a> is required from now on.</li>
<li>Minimum Twisted version increased to 10.0.0, dropped Twisted 8.0 support.</li>
<li>Running test suite now requires <cite>mock</cite> python library (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/390">issue 390</a>)</li>
</ul>
</div>
<div class="section" id="thanks">
<h5>Thanks<a class="headerlink" href="#thanks" title="永久链接至标题">¶</a></h5>
<p>Thanks to everyone who contribute to this release!</p>
<p>List of contributors sorted by number of commits:</p>
<div class="highlight-none"><div class="highlight"><pre>69 Daniel Graña &lt;dangra@...&gt;
37 Pablo Hoffman &lt;pablo@...&gt;
13 Mikhail Korobov &lt;kmike84@...&gt;
 9 Alex Cepoi &lt;alex.cepoi@...&gt;
 9 alexanderlukanin13 &lt;alexander.lukanin.13@...&gt;
 8 Rolando Espinoza La fuente &lt;darkrho@...&gt;
 8 Lukasz Biedrycki &lt;lukasz.biedrycki@...&gt;
 6 Nicolas Ramirez &lt;nramirez.uy@...&gt;
 3 Paul Tremberth &lt;paul.tremberth@...&gt;
 2 Martin Olveyra &lt;molveyra@...&gt;
 2 Stefan &lt;misc@...&gt;
 2 Rolando Espinoza &lt;darkrho@...&gt;
 2 Loren Davie &lt;loren@...&gt;
 2 irgmedeiros &lt;irgmedeiros@...&gt;
 1 Stefan Koch &lt;taikano@...&gt;
 1 Stefan &lt;cct@...&gt;
 1 scraperdragon &lt;dragon@...&gt;
 1 Kumara Tharmalingam &lt;ktharmal@...&gt;
 1 Francesco Piccinno &lt;stack.box@...&gt;
 1 Marcos Campal &lt;duendex@...&gt;
 1 Dragon Dave &lt;dragon@...&gt;
 1 Capi Etheriel &lt;barraponto@...&gt;
 1 cacovsky &lt;amarquesferraz@...&gt;
 1 Berend Iwema &lt;berend@...&gt;
</pre></div>
</div>
</div>
</div>
<div class="section" id="released-2013-10-10">
<h4>0.18.4 (released 2013-10-10)<a class="headerlink" href="#released-2013-10-10" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>IPython refuses to update the namespace. fix #396 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3d32c4f">commit 3d32c4f</a>)</li>
<li>Fix AlreadyCalledError replacing a request in shell command. closes #407 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b1d8919">commit b1d8919</a>)</li>
<li>Fix start_requests laziness and early hangs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/89faf52">commit 89faf52</a>)</li>
</ul>
</div>
<div class="section" id="released-2013-10-03">
<h4>0.18.3 (released 2013-10-03)<a class="headerlink" href="#released-2013-10-03" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>fix regression on lazy evaluation of start requests (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/12693a5">commit 12693a5</a>)</li>
<li>forms: do not submit reset inputs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e429f63">commit e429f63</a>)</li>
<li>increase unittest timeouts to decrease travis false positive failures (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/912202e">commit 912202e</a>)</li>
<li>backport master fixes to json exporter (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cfc2d46">commit cfc2d46</a>)</li>
<li>Fix permission and set umask before generating sdist tarball (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/06149e0">commit 06149e0</a>)</li>
</ul>
</div>
<div class="section" id="released-2013-09-03">
<h4>0.18.2 (released 2013-09-03)<a class="headerlink" href="#released-2013-09-03" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>Backport <cite>scrapy check</cite> command fixes and backward compatible multi
crawler process(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/339">issue 339</a>)</li>
</ul>
</div>
<div class="section" id="released-2013-08-27">
<h4>0.18.1 (released 2013-08-27)<a class="headerlink" href="#released-2013-08-27" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>remove extra import added by cherry picked changes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d20304e">commit d20304e</a>)</li>
<li>fix crawling tests under twisted pre 11.0.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1994f38">commit 1994f38</a>)</li>
<li>py26 can not format zero length fields {} (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/abf756f">commit abf756f</a>)</li>
<li>test PotentiaDataLoss errors on unbound responses (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b15470d">commit b15470d</a>)</li>
<li>Treat responses without content-length or Transfer-Encoding as good responses (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c4bf324">commit c4bf324</a>)</li>
<li>do no include ResponseFailed if http11 handler is not enabled (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6cbe684">commit 6cbe684</a>)</li>
<li>New HTTP client wraps connection losts in ResponseFailed exception. fix #373 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1a20bba">commit 1a20bba</a>)</li>
<li>limit travis-ci build matrix (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3b01bb8">commit 3b01bb8</a>)</li>
<li>Merge pull request #375 from peterarenot/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fa766d7">commit fa766d7</a>)</li>
<li>Fixed so it refers to the correct folder (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3283809">commit 3283809</a>)</li>
<li>added quantal &amp; raring to support ubuntu releases (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1411923">commit 1411923</a>)</li>
<li>fix retry middleware which didn&#8217;t retry certain connection errors after the upgrade to http1 client, closes GH-373 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bb35ed0">commit bb35ed0</a>)</li>
<li>fix XmlItemExporter in Python 2.7.4 and 2.7.5 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/de3e451">commit de3e451</a>)</li>
<li>minor updates to 0.18 release notes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c45e5f1">commit c45e5f1</a>)</li>
<li>fix contributters list format (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0b60031">commit 0b60031</a>)</li>
</ul>
</div>
<div class="section" id="released-2013-08-09">
<h4>0.18.0 (released 2013-08-09)<a class="headerlink" href="#released-2013-08-09" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>Lot of improvements to testsuite run using Tox, including a way to test on pypi</li>
<li>Handle GET parameters for AJAX crawleable urls (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3fe2a32">commit 3fe2a32</a>)</li>
<li>Use lxml recover option to parse sitemaps (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/347">issue 347</a>)</li>
<li>Bugfix cookie merging by hostname and not by netloc (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/352">issue 352</a>)</li>
<li>Support disabling <cite>HttpCompressionMiddleware</cite> using a flag setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/359">issue 359</a>)</li>
<li>Support xml namespaces using <cite>iternodes</cite> parser in <cite>XMLFeedSpider</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/12">issue 12</a>)</li>
<li>Support <cite>dont_cache</cite> request meta flag (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/19">issue 19</a>)</li>
<li>Bugfix <cite>scrapy.utils.gz.gunzip</cite> broken by changes in python 2.7.4 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/4dc76e">commit 4dc76e</a>)</li>
<li>Bugfix url encoding on <cite>SgmlLinkExtractor</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/24">issue 24</a>)</li>
<li>Bugfix <cite>TakeFirst</cite> processor shouldn&#8217;t discard zero (0) value (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/59">issue 59</a>)</li>
<li>Support nested items in xml exporter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/66">issue 66</a>)</li>
<li>Improve cookies handling performance (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/77">issue 77</a>)</li>
<li>Log dupe filtered requests once (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/105">issue 105</a>)</li>
<li>Split redirection middleware into status and meta based middlewares (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/78">issue 78</a>)</li>
<li>Use HTTP1.1 as default downloader handler (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/109">issue 109</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues/318">issue 318</a>)</li>
<li>Support xpath form selection on <cite>FormRequest.from_response</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/185">issue 185</a>)</li>
<li>Bugfix unicode decoding error on <cite>SgmlLinkExtractor</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/199">issue 199</a>)</li>
<li>Bugfix signal dispatching on pypi interpreter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/205">issue 205</a>)</li>
<li>Improve request delay and concurrency handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/206">issue 206</a>)</li>
<li>Add RFC2616 cache policy to <cite>HttpCacheMiddleware</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/212">issue 212</a>)</li>
<li>Allow customization of messages logged by engine (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/214">issue 214</a>)</li>
<li>Multiples improvements to <cite>DjangoItem</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/217">issue 217</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/218">issue 218</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/221">issue 221</a>)</li>
<li>Extend Scrapy commands using setuptools entry points (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/260">issue 260</a>)</li>
<li>Allow spider <cite>allowed_domains</cite> value to be set/tuple (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/261">issue 261</a>)</li>
<li>Support <cite>settings.getdict</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/269">issue 269</a>)</li>
<li>Simplify internal <cite>scrapy.core.scraper</cite> slot handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/271">issue 271</a>)</li>
<li>Added <cite>Item.copy</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/290">issue 290</a>)</li>
<li>Collect idle downloader slots (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/297">issue 297</a>)</li>
<li>Add <cite>ftp://</cite> scheme downloader handler (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/329">issue 329</a>)</li>
<li>Added downloader benchmark webserver and spider tools <a class="reference internal" href="index.html#benchmarking"><em>Benchmarking</em></a></li>
<li>Moved persistent (on disk) queues to a separate project (<a class="reference external" href="https://github.com/scrapy/queuelib">queuelib</a>) which scrapy now depends on</li>
<li>Add scrapy commands using external libraries (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/260">issue 260</a>)</li>
<li>Added <tt class="docutils literal"><span class="pre">--pdb</span></tt> option to <tt class="docutils literal"><span class="pre">scrapy</span></tt> command line tool</li>
<li>Added <tt class="xref py py-meth docutils literal"><span class="pre">XPathSelector.remove_namespaces()</span></tt> which allows to remove all namespaces from XML documents for convenience (to work with namespace-less XPaths). Documented in <a class="reference internal" href="index.html#topics-selectors"><em>选择器(Selectors)</em></a>.</li>
<li>Several improvements to spider contracts</li>
<li>New default middleware named MetaRefreshMiddldeware that handles meta-refresh html tag redirections,</li>
<li>MetaRefreshMiddldeware and RedirectMiddleware have different priorities to address #62</li>
<li>added from_crawler method to spiders</li>
<li>added system tests with mock server</li>
<li>more improvements to Mac OS compatibility (thanks Alex Cepoi)</li>
<li>several more cleanups to singletons and multi-spider support (thanks Nicolas Ramirez)</li>
<li>support custom download slots</li>
<li>added &#8211;spider option to &#8220;shell&#8221; command.</li>
<li>log overridden settings when scrapy starts</li>
</ul>
<p>Thanks to everyone who contribute to this release. Here is a list of
contributors sorted by number of commits:</p>
<div class="highlight-none"><div class="highlight"><pre>130 Pablo Hoffman &lt;pablo@...&gt;
 97 Daniel Graña &lt;dangra@...&gt;
 20 Nicolás Ramírez &lt;nramirez.uy@...&gt;
 13 Mikhail Korobov &lt;kmike84@...&gt;
 12 Pedro Faustino &lt;pedrobandim@...&gt;
 11 Steven Almeroth &lt;sroth77@...&gt;
  5 Rolando Espinoza La fuente &lt;darkrho@...&gt;
  4 Michal Danilak &lt;mimino.coder@...&gt;
  4 Alex Cepoi &lt;alex.cepoi@...&gt;
  4 Alexandr N Zamaraev (aka tonal) &lt;tonal@...&gt;
  3 paul &lt;paul.tremberth@...&gt;
  3 Martin Olveyra &lt;molveyra@...&gt;
  3 Jordi Llonch &lt;llonchj@...&gt;
  3 arijitchakraborty &lt;myself.arijit@...&gt;
  2 Shane Evans &lt;shane.evans@...&gt;
  2 joehillen &lt;joehillen@...&gt;
  2 Hart &lt;HartSimha@...&gt;
  2 Dan &lt;ellisd23@...&gt;
  1 Zuhao Wan &lt;wanzuhao@...&gt;
  1 whodatninja &lt;blake@...&gt;
  1 vkrest &lt;v.krestiannykov@...&gt;
  1 tpeng &lt;pengtaoo@...&gt;
  1 Tom Mortimer-Jones &lt;tom@...&gt;
  1 Rocio Aramberri &lt;roschegel@...&gt;
  1 Pedro &lt;pedro@...&gt;
  1 notsobad &lt;wangxiaohugg@...&gt;
  1 Natan L &lt;kuyanatan.nlao@...&gt;
  1 Mark Grey &lt;mark.grey@...&gt;
  1 Luan &lt;luanpab@...&gt;
  1 Libor Nenadál &lt;libor.nenadal@...&gt;
  1 Juan M Uys &lt;opyate@...&gt;
  1 Jonas Brunsgaard &lt;jonas.brunsgaard@...&gt;
  1 Ilya Baryshev &lt;baryshev@...&gt;
  1 Hasnain Lakhani &lt;m.hasnain.lakhani@...&gt;
  1 Emanuel Schorsch &lt;emschorsch@...&gt;
  1 Chris Tilden &lt;chris.tilden@...&gt;
  1 Capi Etheriel &lt;barraponto@...&gt;
  1 cacovsky &lt;amarquesferraz@...&gt;
  1 Berend Iwema &lt;berend@...&gt;
</pre></div>
</div>
</div>
<div class="section" id="released-2013-05-30">
<h4>0.16.5 (released 2013-05-30)<a class="headerlink" href="#released-2013-05-30" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>obey request method when scrapy deploy is redirected to a new endpoint (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8c4fcee">commit 8c4fcee</a>)</li>
<li>fix inaccurate downloader middleware documentation. refs #280 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/40667cb">commit 40667cb</a>)</li>
<li>doc: remove links to diveintopython.org, which is no longer available. closes #246 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bd58bfa">commit bd58bfa</a>)</li>
<li>Find form nodes in invalid html5 documents (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e3d6945">commit e3d6945</a>)</li>
<li>Fix typo labeling attrs type bool instead of list (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a274276">commit a274276</a>)</li>
</ul>
</div>
<div class="section" id="released-2013-01-23">
<h4>0.16.4 (released 2013-01-23)<a class="headerlink" href="#released-2013-01-23" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>fixes spelling errors in documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6d2b3aa">commit 6d2b3aa</a>)</li>
<li>add doc about disabling an extension. refs #132 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c90de33">commit c90de33</a>)</li>
<li>Fixed error message formatting. log.err() doesn&#8217;t support cool formatting and when error occurred, the message was:    &#8220;ERROR: Error processing %(item)s&#8221; (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c16150c">commit c16150c</a>)</li>
<li>lint and improve images pipeline error logging (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/56b45fc">commit 56b45fc</a>)</li>
<li>fixed doc typos (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/243be84">commit 243be84</a>)</li>
<li>add documentation topics: Broad Crawls &amp; Common Practies (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1fbb715">commit 1fbb715</a>)</li>
<li>fix bug in scrapy parse command when spider is not specified explicitly. closes #209 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c72e682">commit c72e682</a>)</li>
<li>Update docs/topics/commands.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/28eac7a">commit 28eac7a</a>)</li>
</ul>
</div>
<div class="section" id="released-2012-12-07">
<h4>0.16.3 (released 2012-12-07)<a class="headerlink" href="#released-2012-12-07" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>Remove concurrency limitation when using download delays and still ensure inter-request delays are enforced (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/487b9b5">commit 487b9b5</a>)</li>
<li>add error details when image pipeline fails (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8232569">commit 8232569</a>)</li>
<li>improve mac os compatibility (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8dcf8aa">commit 8dcf8aa</a>)</li>
<li>setup.py: use README.rst to populate long_description (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7b5310d">commit 7b5310d</a>)</li>
<li>doc: removed obsolete references to ClientForm (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/80f9bb6">commit 80f9bb6</a>)</li>
<li>correct docs for default storage backend (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2aa491b">commit 2aa491b</a>)</li>
<li>doc: removed broken proxyhub link from FAQ (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bdf61c4">commit bdf61c4</a>)</li>
<li>Fixed docs typo in SpiderOpenCloseLogging example (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7184094">commit 7184094</a>)</li>
</ul>
</div>
<div class="section" id="released-2012-11-09">
<h4>0.16.2 (released 2012-11-09)<a class="headerlink" href="#released-2012-11-09" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>scrapy contracts: python2.6 compat (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a4a9199">commit a4a9199</a>)</li>
<li>scrapy contracts verbose option (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ec41673">commit ec41673</a>)</li>
<li>proper unittest-like output for scrapy contracts (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/86635e4">commit 86635e4</a>)</li>
<li>added open_in_browser to debugging doc (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c9b690d">commit c9b690d</a>)</li>
<li>removed reference to global scrapy stats from settings doc (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/dd55067">commit dd55067</a>)</li>
<li>Fix SpiderState bug in Windows platforms (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/58998f4">commit 58998f4</a>)</li>
</ul>
</div>
<div class="section" id="released-2012-10-26">
<h4>0.16.1 (released 2012-10-26)<a class="headerlink" href="#released-2012-10-26" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>fixed LogStats extension, which got broken after a wrong merge before the 0.16 release (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8c780fd">commit 8c780fd</a>)</li>
<li>better backwards compatibility for scrapy.conf.settings (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3403089">commit 3403089</a>)</li>
<li>extended documentation on how to access crawler stats from extensions (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c4da0b5">commit c4da0b5</a>)</li>
<li>removed .hgtags (no longer needed now that scrapy uses git) (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d52c188">commit d52c188</a>)</li>
<li>fix dashes under rst headers (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fa4f7f9">commit fa4f7f9</a>)</li>
<li>set release date for 0.16.0 in news (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e292246">commit e292246</a>)</li>
</ul>
</div>
<div class="section" id="released-2012-10-18">
<h4>0.16.0 (released 2012-10-18)<a class="headerlink" href="#released-2012-10-18" title="永久链接至标题">¶</a></h4>
<p>Scrapy changes:</p>
<ul class="simple">
<li>added <a class="reference internal" href="index.html#topics-contracts"><em>Spiders Contracts</em></a>, a mechanism for testing spiders in a formal/reproducible way</li>
<li>added options <tt class="docutils literal"><span class="pre">-o</span></tt> and <tt class="docutils literal"><span class="pre">-t</span></tt> to the <a class="reference internal" href="index.html#std:command-runspider"><tt class="xref std std-command docutils literal"><span class="pre">runspider</span></tt></a> command</li>
<li>documented <a class="reference internal" href="index.html#document-topics/autothrottle"><em>自动限速(AutoThrottle)扩展</em></a> and added to extensions installed by default. You still need to enable it with <a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_ENABLED</span></tt></a></li>
<li>major Stats Collection refactoring: removed separation of global/per-spider stats, removed stats-related signals (<tt class="docutils literal"><span class="pre">stats_spider_opened</span></tt>, etc). Stats are much simpler now, backwards compatibility is kept on the Stats Collector API and signals.</li>
<li>added <a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_start_requests" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_start_requests"><tt class="xref py py-meth docutils literal"><span class="pre">process_start_requests()</span></tt></a> method to spider middlewares</li>
<li>dropped Signals singleton. Signals should now be accesed through the Crawler.signals attribute. See the signals documentation for more info.</li>
<li>dropped Signals singleton. Signals should now be accesed through the Crawler.signals attribute. See the signals documentation for more info.</li>
<li>dropped Stats Collector singleton. Stats can now be accessed through the Crawler.stats attribute. See the stats collection documentation for more info.</li>
<li>documented <a class="reference internal" href="index.html#topics-api"><em>核心API</em></a></li>
<li><cite>lxml</cite> is now the default selectors backend instead of <cite>libxml2</cite></li>
<li>ported FormRequest.from_response() to use <a class="reference external" href="http://lxml.de/">lxml</a> instead of <a class="reference external" href="http://wwwsearch.sourceforge.net/old/ClientForm/">ClientForm</a></li>
<li>removed modules: <tt class="docutils literal"><span class="pre">scrapy.xlib.BeautifulSoup</span></tt> and <tt class="docutils literal"><span class="pre">scrapy.xlib.ClientForm</span></tt></li>
<li>SitemapSpider: added support for sitemap urls ending in .xml and .xml.gz, even if they advertise a wrong content type (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/10ed28b">commit 10ed28b</a>)</li>
<li>StackTraceDump extension: also dump trackref live references (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fe2ce93">commit fe2ce93</a>)</li>
<li>nested items now fully supported in JSON and JSONLines exporters</li>
<li>added <a class="reference internal" href="index.html#std:reqmeta-cookiejar"><tt class="xref std std-reqmeta docutils literal"><span class="pre">cookiejar</span></tt></a> Request meta key to support multiple cookie sessions per spider</li>
<li>decoupled encoding detection code to <a class="reference external" href="https://github.com/scrapy/w3lib/blob/master/w3lib/encoding.py">w3lib.encoding</a>, and ported Scrapy code to use that mdule</li>
<li>dropped support for Python 2.5. See <a class="reference external" href="http://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/">http://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/</a></li>
<li>dropped support for Twisted 2.5</li>
<li>added <a class="reference internal" href="index.html#std:setting-REFERER_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">REFERER_ENABLED</span></tt></a> setting, to control referer middleware</li>
<li>changed default user agent to: <tt class="docutils literal"><span class="pre">Scrapy/VERSION</span> <span class="pre">(+http://scrapy.org)</span></tt></li>
<li>removed (undocumented) <tt class="docutils literal"><span class="pre">HTMLImageLinkExtractor</span></tt> class from <tt class="docutils literal"><span class="pre">scrapy.contrib.linkextractors.image</span></tt></li>
<li>removed per-spider settings (to be replaced by instantiating multiple crawler objects)</li>
<li><tt class="docutils literal"><span class="pre">USER_AGENT</span></tt> spider attribute will no longer work, use <tt class="docutils literal"><span class="pre">user_agent</span></tt> attribute instead</li>
<li><tt class="docutils literal"><span class="pre">DOWNLOAD_TIMEOUT</span></tt> spider attribute will no longer work, use <tt class="docutils literal"><span class="pre">download_timeout</span></tt> attribute instead</li>
<li>removed <tt class="docutils literal"><span class="pre">ENCODING_ALIASES</span></tt> setting, as encoding auto-detection has been moved to the <a class="reference external" href="https://github.com/scrapy/w3lib">w3lib</a> library</li>
<li>promoted <a class="reference internal" href="index.html#topics-djangoitem"><em>DjangoItem</em></a> to main contrib</li>
<li>LogFormatter method now return dicts(instead of strings) to support lazy formatting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/164">issue 164</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/commit/dcef7b0">commit dcef7b0</a>)</li>
<li>downloader handlers (<a class="reference internal" href="index.html#std:setting-DOWNLOAD_HANDLERS"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_HANDLERS</span></tt></a> setting) now receive settings as the first argument of the constructor</li>
<li>replaced memory usage acounting with (more portable) <a class="reference external" href="http://docs.python.org/library/resource.html">resource</a> module, removed <tt class="docutils literal"><span class="pre">scrapy.utils.memory</span></tt> module</li>
<li>removed signal: <tt class="docutils literal"><span class="pre">scrapy.mail.mail_sent</span></tt></li>
<li>removed <tt class="docutils literal"><span class="pre">TRACK_REFS</span></tt> setting, now <a class="reference internal" href="index.html#topics-leaks-trackrefs"><em>trackrefs</em></a> is always enabled</li>
<li>DBM is now the default storage backend for HTTP cache middleware</li>
<li>number of log messages (per level) are now tracked through Scrapy stats (stat name: <tt class="docutils literal"><span class="pre">log_count/LEVEL</span></tt>)</li>
<li>number received responses are now tracked through Scrapy stats (stat name: <tt class="docutils literal"><span class="pre">response_received_count</span></tt>)</li>
<li>removed <tt class="docutils literal"><span class="pre">scrapy.log.started</span></tt> attribute</li>
</ul>
</div>
<div class="section" id="id5">
<h4>0.14.4<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>added precise to supported ubuntu distros (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b7e46df">commit b7e46df</a>)</li>
<li>fixed bug in json-rpc webservice reported in <a class="reference external" href="https://groups.google.com/d/topic/scrapy-users/qgVBmFybNAQ/discussion">https://groups.google.com/d/topic/scrapy-users/qgVBmFybNAQ/discussion</a>. also removed no longer supported &#8216;run&#8217; command from extras/scrapy-ws.py (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/340fbdb">commit 340fbdb</a>)</li>
<li>meta tag attributes for content-type http equiv can be in any order. #123 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0cb68af">commit 0cb68af</a>)</li>
<li>replace &#8220;import Image&#8221; by more standard &#8220;from PIL import Image&#8221;. closes #88 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/4d17048">commit 4d17048</a>)</li>
<li>return trial status as bin/runtests.sh exit value. #118 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b7b2e7f">commit b7b2e7f</a>)</li>
</ul>
</div>
<div class="section" id="id6">
<h4>0.14.3<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>forgot to include pydispatch license. #118 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fd85f9c">commit fd85f9c</a>)</li>
<li>include egg files used by testsuite in source distribution. #118 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c897793">commit c897793</a>)</li>
<li>update docstring in project template to avoid confusion with genspider command, which may be considered as an advanced feature. refs #107 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2548dcc">commit 2548dcc</a>)</li>
<li>added note to docs/topics/firebug.rst about google directory being shut down (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/668e352">commit 668e352</a>)</li>
<li>dont discard slot when empty, just save in another dict in order to recycle if needed again. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8e9f607">commit 8e9f607</a>)</li>
<li>do not fail handling unicode xpaths in libxml2 backed selectors (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b830e95">commit b830e95</a>)</li>
<li>fixed minor mistake in Request objects documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bf3c9ee">commit bf3c9ee</a>)</li>
<li>fixed minor defect in link extractors documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ba14f38">commit ba14f38</a>)</li>
<li>removed some obsolete remaining code related to sqlite support in scrapy (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0665175">commit 0665175</a>)</li>
</ul>
</div>
<div class="section" id="id7">
<h4>0.14.2<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>move buffer pointing to start of file before computing checksum. refs #92 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6a5bef2">commit 6a5bef2</a>)</li>
<li>Compute image checksum before persisting images. closes #92 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/9817df1">commit 9817df1</a>)</li>
<li>remove leaking references in cached failures (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/673a120">commit 673a120</a>)</li>
<li>fixed bug in MemoryUsage extension: get_engine_status() takes exactly 1 argument (0 given) (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/11133e9">commit 11133e9</a>)</li>
<li>fixed struct.error on http compression middleware. closes #87 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1423140">commit 1423140</a>)</li>
<li>ajax crawling wasn&#8217;t expanding for unicode urls (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0de3fb4">commit 0de3fb4</a>)</li>
<li>Catch start_requests iterator errors. refs #83 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/454a21d">commit 454a21d</a>)</li>
<li>Speed-up libxml2 XPathSelector (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2fbd662">commit 2fbd662</a>)</li>
<li>updated versioning doc according to recent changes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0a070f5">commit 0a070f5</a>)</li>
<li>scrapyd: fixed documentation link (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2b4e4c3">commit 2b4e4c3</a>)</li>
<li>extras/makedeb.py: no longer obtaining version from git (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/caffe0e">commit caffe0e</a>)</li>
</ul>
</div>
<div class="section" id="id8">
<h4>0.14.1<a class="headerlink" href="#id8" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>extras/makedeb.py: no longer obtaining version from git (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/caffe0e">commit caffe0e</a>)</li>
<li>bumped version to 0.14.1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6cb9e1c">commit 6cb9e1c</a>)</li>
<li>fixed reference to tutorial directory (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/4b86bd6">commit 4b86bd6</a>)</li>
<li>doc: removed duplicated callback argument from Request.replace() (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1aeccdd">commit 1aeccdd</a>)</li>
<li>fixed formatting of scrapyd doc (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8bf19e6">commit 8bf19e6</a>)</li>
<li>Dump stacks for all running threads and fix engine status dumped by StackTraceDump extension (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/14a8e6e">commit 14a8e6e</a>)</li>
<li>added comment about why we disable ssl on boto images upload (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5223575">commit 5223575</a>)</li>
<li>SSL handshaking hangs when doing too many parallel connections to S3 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/63d583d">commit 63d583d</a>)</li>
<li>change tutorial to follow changes on dmoz site (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bcb3198">commit bcb3198</a>)</li>
<li>Avoid _disconnectedDeferred AttributeError exception in Twisted&gt;=11.1.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/98f3f87">commit 98f3f87</a>)</li>
<li>allow spider to set autothrottle max concurrency (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/175a4b5">commit 175a4b5</a>)</li>
</ul>
</div>
<div class="section" id="id9">
<h4>0.14<a class="headerlink" href="#id9" title="永久链接至标题">¶</a></h4>
<div class="section" id="new-features-and-settings">
<h5>New features and settings<a class="headerlink" href="#new-features-and-settings" title="永久链接至标题">¶</a></h5>
<ul>
<li><p class="first">Support for <a class="reference external" href="http://code.google.com/web/ajaxcrawling/docs/getting-started.html">AJAX crawleable urls</a></p>
</li>
<li><p class="first">New persistent scheduler that stores requests on disk, allowing to suspend and resume crawls (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2737">r2737</a>)</p>
</li>
<li><p class="first">added <tt class="docutils literal"><span class="pre">-o</span></tt> option to <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">crawl</span></tt>, a shortcut for dumping scraped items into a file (or standard output using <tt class="docutils literal"><span class="pre">-</span></tt>)</p>
</li>
<li><p class="first">Added support for passing custom settings to Scrapyd <tt class="docutils literal"><span class="pre">schedule.json</span></tt> api (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2779">r2779</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2783">r2783</a>)</p>
</li>
<li><p class="first">New <tt class="docutils literal"><span class="pre">ChunkedTransferMiddleware</span></tt> (enabled by default) to support <a class="reference external" href="http://en.wikipedia.org/wiki/Chunked_transfer_encoding">chunked transfer encoding</a> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2769">r2769</a>)</p>
</li>
<li><p class="first">Add boto 2.0 support for S3 downloader handler (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2763">r2763</a>)</p>
</li>
<li><p class="first">Added <a class="reference external" href="http://docs.python.org/library/marshal.html">marshal</a> to formats supported by feed exports (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2744">r2744</a>)</p>
</li>
<li><p class="first">In request errbacks, offending requests are now received in <cite>failure.request</cite> attribute (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2738">r2738</a>)</p>
</li>
<li><dl class="first docutils">
<dt>Big downloader refactoring to support per domain/ip concurrency limits (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2732">r2732</a>)</dt>
<dd><ul class="first last">
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_SPIDER</span></tt> setting has been deprecated and replaced by:</dt>
<dd><ul class="first last simple">
<li><a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS"><tt class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS</span></tt></a>, <a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><tt class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></tt></a>, <a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><tt class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></tt></a></li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">check the documentation for more details</p>
</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">Added builtin caching DNS resolver (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2728">r2728</a>)</p>
</li>
<li><p class="first">Moved Amazon AWS-related components/extensions (SQS spider queue, SimpleDB stats collector) to a separate project: [scaws](<a class="reference external" href="https://github.com/scrapinghub/scaws">https://github.com/scrapinghub/scaws</a>) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2706">r2706</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2714">r2714</a>)</p>
</li>
<li><p class="first">Moved spider queues to scrapyd: <cite>scrapy.spiderqueue</cite> -&gt; <cite>scrapyd.spiderqueue</cite> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2708">r2708</a>)</p>
</li>
<li><p class="first">Moved sqlite utils to scrapyd: <cite>scrapy.utils.sqlite</cite> -&gt; <cite>scrapyd.sqlite</cite> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2781">r2781</a>)</p>
</li>
<li><p class="first">Real support for returning iterators on <cite>start_requests()</cite> method. The iterator is now consumed during the crawl when the spider is getting idle (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2704">r2704</a>)</p>
</li>
<li><p class="first">Added <a class="reference internal" href="index.html#std:setting-REDIRECT_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">REDIRECT_ENABLED</span></tt></a> setting to quickly enable/disable the redirect middleware (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2697">r2697</a>)</p>
</li>
<li><p class="first">Added <a class="reference internal" href="index.html#std:setting-RETRY_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">RETRY_ENABLED</span></tt></a> setting to quickly enable/disable the retry middleware (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2694">r2694</a>)</p>
</li>
<li><p class="first">Added <tt class="docutils literal"><span class="pre">CloseSpider</span></tt> exception to manually close spiders (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2691">r2691</a>)</p>
</li>
<li><p class="first">Improved encoding detection by adding support for HTML5 meta charset declaration (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2690">r2690</a>)</p>
</li>
<li><p class="first">Refactored close spider behavior to wait for all downloads to finish and be processed by spiders, before closing the spider (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2688">r2688</a>)</p>
</li>
<li><p class="first">Added <tt class="docutils literal"><span class="pre">SitemapSpider</span></tt> (see documentation in Spiders page) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2658">r2658</a>)</p>
</li>
<li><p class="first">Added <tt class="docutils literal"><span class="pre">LogStats</span></tt> extension for periodically logging basic stats (like crawled pages and scraped items) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2657">r2657</a>)</p>
</li>
<li><p class="first">Make handling of gzipped responses more robust (#319, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2643">r2643</a>). Now Scrapy will try and decompress as much as possible from a gzipped response, instead of failing with an <cite>IOError</cite>.</p>
</li>
<li><p class="first">Simplified !MemoryDebugger extension to use stats for dumping memory debugging info (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2639">r2639</a>)</p>
</li>
<li><p class="first">Added new command to edit spiders: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">edit</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2636">r2636</a>) and <cite>-e</cite> flag to <cite>genspider</cite> command that uses it (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2653">r2653</a>)</p>
</li>
<li><p class="first">Changed default representation of items to pretty-printed dicts. (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2631">r2631</a>). This improves default logging by making log more readable in the default case, for both Scraped and Dropped lines.</p>
</li>
<li><p class="first">Added <a class="reference internal" href="index.html#std:signal-spider_error"><tt class="xref std std-signal docutils literal"><span class="pre">spider_error</span></tt></a> signal (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2628">r2628</a>)</p>
</li>
<li><p class="first">Added <a class="reference internal" href="index.html#std:setting-COOKIES_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">COOKIES_ENABLED</span></tt></a> setting (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2625">r2625</a>)</p>
</li>
<li><p class="first">Stats are now dumped to Scrapy log (default value of <a class="reference internal" href="index.html#std:setting-STATS_DUMP"><tt class="xref std std-setting docutils literal"><span class="pre">STATS_DUMP</span></tt></a> setting has been changed to <cite>True</cite>). This is to make Scrapy users more aware of Scrapy stats and the data that is collected there.</p>
</li>
<li><p class="first">Added support for dynamically adjusting download delay and maximum concurrent requests (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2599">r2599</a>)</p>
</li>
<li><p class="first">Added new DBM HTTP cache storage backend (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2576">r2576</a>)</p>
</li>
<li><p class="first">Added <tt class="docutils literal"><span class="pre">listjobs.json</span></tt> API to Scrapyd (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2571">r2571</a>)</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">CsvItemExporter</span></tt>: added <tt class="docutils literal"><span class="pre">join_multivalued</span></tt> parameter (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2578">r2578</a>)</p>
</li>
<li><p class="first">Added namespace support to <tt class="docutils literal"><span class="pre">xmliter_lxml</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2552">r2552</a>)</p>
</li>
<li><p class="first">Improved cookies middleware by making <cite>COOKIES_DEBUG</cite> nicer and documenting it (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2579">r2579</a>)</p>
</li>
<li><p class="first">Several improvements to Scrapyd and Link extractors</p>
</li>
</ul>
</div>
<div class="section" id="code-rearranged-and-removed">
<h5>Code rearranged and removed<a class="headerlink" href="#code-rearranged-and-removed" title="永久链接至标题">¶</a></h5>
<ul>
<li><dl class="first docutils">
<dt>Merged item passed and item scraped concepts, as they have often proved confusing in the past. This means: (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2630">r2630</a>)</dt>
<dd><ul class="first last simple">
<li>original item_scraped signal was removed</li>
<li>original item_passed signal was renamed to item_scraped</li>
<li>old log lines <tt class="docutils literal"><span class="pre">Scraped</span> <span class="pre">Item...</span></tt> were removed</li>
<li>old log lines <tt class="docutils literal"><span class="pre">Passed</span> <span class="pre">Item...</span></tt> were renamed to <tt class="docutils literal"><span class="pre">Scraped</span> <span class="pre">Item...</span></tt> lines and downgraded to <tt class="docutils literal"><span class="pre">DEBUG</span></tt> level</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Reduced Scrapy codebase by striping part of Scrapy code into two new libraries:</dt>
<dd><ul class="first last simple">
<li><a class="reference external" href="https://github.com/scrapy/w3lib">w3lib</a> (several functions from <tt class="docutils literal"><span class="pre">scrapy.utils.{http,markup,multipart,response,url}</span></tt>, done in <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2584">r2584</a>)</li>
<li><a class="reference external" href="https://github.com/scrapy/scrapely">scrapely</a> (was <tt class="docutils literal"><span class="pre">scrapy.contrib.ibl</span></tt>, done in <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2586">r2586</a>)</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">Removed unused function: <cite>scrapy.utils.request.request_info()</cite> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2577">r2577</a>)</p>
</li>
<li><p class="first">Removed googledir project from <cite>examples/googledir</cite>. There&#8217;s now a new example project called <cite>dirbot</cite> available on github: <a class="reference external" href="https://github.com/scrapy/dirbot">https://github.com/scrapy/dirbot</a></p>
</li>
<li><p class="first">Removed support for default field values in Scrapy items (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2616">r2616</a>)</p>
</li>
<li><p class="first">Removed experimental crawlspider v2 (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2632">r2632</a>)</p>
</li>
<li><p class="first">Removed scheduler middleware to simplify architecture. Duplicates filter is now done in the scheduler itself, using the same dupe fltering class as before (<cite>DUPEFILTER_CLASS</cite> setting) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2640">r2640</a>)</p>
</li>
<li><p class="first">Removed support for passing urls to <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">crawl</span></tt> command (use <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">parse</span></tt> instead) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2704">r2704</a>)</p>
</li>
<li><p class="first">Removed deprecated Execution Queue (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2704">r2704</a>)</p>
</li>
<li><p class="first">Removed (undocumented) spider context extension (from scrapy.contrib.spidercontext) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2780">r2780</a>)</p>
</li>
<li><p class="first">removed <tt class="docutils literal"><span class="pre">CONCURRENT_SPIDERS</span></tt> setting (use scrapyd maxproc instead) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2789">r2789</a>)</p>
</li>
<li><p class="first">Renamed attributes of core components: downloader.sites -&gt; downloader.slots, scraper.sites -&gt; scraper.slots (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2717">r2717</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2718">r2718</a>)</p>
</li>
<li><p class="first">Renamed setting <tt class="docutils literal"><span class="pre">CLOSESPIDER_ITEMPASSED</span></tt> to <a class="reference internal" href="index.html#std:setting-CLOSESPIDER_ITEMCOUNT"><tt class="xref std std-setting docutils literal"><span class="pre">CLOSESPIDER_ITEMCOUNT</span></tt></a> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2655">r2655</a>). Backwards compatibility kept.</p>
</li>
</ul>
</div>
</div>
<div class="section" id="id10">
<h4>0.12<a class="headerlink" href="#id10" title="永久链接至标题">¶</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<div class="section" id="new-features-and-improvements">
<h5>New features and improvements<a class="headerlink" href="#new-features-and-improvements" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>Passed item is now sent in the <tt class="docutils literal"><span class="pre">item</span></tt> argument of the <tt class="xref std std-signal docutils literal"><span class="pre">item_passed</span></tt> (#273)</li>
<li>Added verbose option to <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">version</span></tt> command, useful for bug reports (#298)</li>
<li>HTTP cache now stored by default in the project data dir (#279)</li>
<li>Added project data storage directory (#276, #277)</li>
<li>Documented file structure of Scrapy projects (see command-line tool doc)</li>
<li>New lxml backend for XPath selectors (#147)</li>
<li>Per-spider settings (#245)</li>
<li>Support exit codes to signal errors in Scrapy commands (#248)</li>
<li>Added <tt class="docutils literal"><span class="pre">-c</span></tt> argument to <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">shell</span></tt> command</li>
<li>Made <tt class="docutils literal"><span class="pre">libxml2</span></tt> optional (#260)</li>
<li>New <tt class="docutils literal"><span class="pre">deploy</span></tt> command (#261)</li>
<li>Added <a class="reference internal" href="index.html#std:setting-CLOSESPIDER_PAGECOUNT"><tt class="xref std std-setting docutils literal"><span class="pre">CLOSESPIDER_PAGECOUNT</span></tt></a> setting (#253)</li>
<li>Added <a class="reference internal" href="index.html#std:setting-CLOSESPIDER_ERRORCOUNT"><tt class="xref std std-setting docutils literal"><span class="pre">CLOSESPIDER_ERRORCOUNT</span></tt></a> setting (#254)</li>
</ul>
</div>
<div class="section" id="scrapyd-changes">
<h5>Scrapyd changes<a class="headerlink" href="#scrapyd-changes" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>Scrapyd now uses one process per spider</li>
<li>It stores one log file per spider run, and rotate them keeping the lastest 5 logs per spider (by default)</li>
<li>A minimal web ui was added, available at <a class="reference external" href="http://localhost:6800">http://localhost:6800</a> by default</li>
<li>There is now a <cite>scrapy server</cite> command to start a Scrapyd server of the current project</li>
</ul>
</div>
<div class="section" id="changes-to-settings">
<h5>Changes to settings<a class="headerlink" href="#changes-to-settings" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>added <cite>HTTPCACHE_ENABLED</cite> setting (False by default) to enable HTTP cache middleware</li>
<li>changed <cite>HTTPCACHE_EXPIRATION_SECS</cite> semantics: now zero means &#8220;never expire&#8221;.</li>
</ul>
</div>
<div class="section" id="deprecated-obsoleted-functionality">
<h5>Deprecated/obsoleted functionality<a class="headerlink" href="#deprecated-obsoleted-functionality" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>Deprecated <tt class="docutils literal"><span class="pre">runserver</span></tt> command in favor of <tt class="docutils literal"><span class="pre">server</span></tt> command which starts a Scrapyd server. See also: Scrapyd changes</li>
<li>Deprecated <tt class="docutils literal"><span class="pre">queue</span></tt> command in favor of using Scrapyd <tt class="docutils literal"><span class="pre">schedule.json</span></tt> API. See also: Scrapyd changes</li>
<li>Removed the !LxmlItemLoader (experimental contrib which never graduated to main contrib)</li>
</ul>
</div>
</div>
<div class="section" id="id11">
<h4>0.10<a class="headerlink" href="#id11" title="永久链接至标题">¶</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<div class="section" id="id12">
<h5>New features and improvements<a class="headerlink" href="#id12" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>New Scrapy service called <tt class="docutils literal"><span class="pre">scrapyd</span></tt> for deploying Scrapy crawlers in production (#218) (documentation available)</li>
<li>Simplified Images pipeline usage which doesn&#8217;t require subclassing your own images pipeline now (#217)</li>
<li>Scrapy shell now shows the Scrapy log by default (#206)</li>
<li>Refactored execution queue in a common base code and pluggable backends called &#8220;spider queues&#8221; (#220)</li>
<li>New persistent spider queue (based on SQLite) (#198), available by default, which allows to start Scrapy in server mode and then schedule spiders to run.</li>
<li>Added documentation for Scrapy command-line tool and all its available sub-commands. (documentation available)</li>
<li>Feed exporters with pluggable backends (#197) (documentation available)</li>
<li>Deferred signals (#193)</li>
<li>Added two new methods to item pipeline open_spider(), close_spider() with deferred support (#195)</li>
<li>Support for overriding default request headers per spider (#181)</li>
<li>Replaced default Spider Manager with one with similar functionality but not depending on Twisted Plugins (#186)</li>
<li>Splitted Debian package into two packages - the library and the service (#187)</li>
<li>Scrapy log refactoring (#188)</li>
<li>New extension for keeping persistent spider contexts among different runs (#203)</li>
<li>Added <cite>dont_redirect</cite> request.meta key for avoiding redirects (#233)</li>
<li>Added <cite>dont_retry</cite> request.meta key for avoiding retries (#234)</li>
</ul>
</div>
<div class="section" id="command-line-tool-changes">
<h5>Command-line tool changes<a class="headerlink" href="#command-line-tool-changes" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>New <cite>scrapy</cite> command which replaces the old <cite>scrapy-ctl.py</cite> (#199)
- there is only one global <cite>scrapy</cite> command now, instead of one <cite>scrapy-ctl.py</cite> per project
- Added <cite>scrapy.bat</cite> script for running more conveniently from Windows</li>
<li>Added bash completion to command-line tool (#210)</li>
<li>Renamed command <cite>start</cite> to <cite>runserver</cite> (#209)</li>
</ul>
</div>
<div class="section" id="api-changes">
<h5>API changes<a class="headerlink" href="#api-changes" title="永久链接至标题">¶</a></h5>
<ul>
<li><p class="first"><tt class="docutils literal"><span class="pre">url</span></tt> and <tt class="docutils literal"><span class="pre">body</span></tt> attributes of Request objects are now read-only (#230)</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">Request.copy()</span></tt> and <tt class="docutils literal"><span class="pre">Request.replace()</span></tt> now also copies their <tt class="docutils literal"><span class="pre">callback</span></tt> and <tt class="docutils literal"><span class="pre">errback</span></tt> attributes (#231)</p>
</li>
<li><p class="first">Removed <tt class="docutils literal"><span class="pre">UrlFilterMiddleware</span></tt> from <tt class="docutils literal"><span class="pre">scrapy.contrib</span></tt> (already disabled by default)</p>
</li>
<li><p class="first">Offsite middelware doesn&#8217;t filter out any request coming from a spider that doesn&#8217;t have a allowed_domains attribute (#225)</p>
</li>
<li><p class="first">Removed Spider Manager <tt class="docutils literal"><span class="pre">load()</span></tt> method. Now spiders are loaded in the constructor itself.</p>
</li>
<li><dl class="first docutils">
<dt>Changes to Scrapy Manager (now called &#8220;Crawler&#8221;):</dt>
<dd><ul class="first last simple">
<li><tt class="docutils literal"><span class="pre">scrapy.core.manager.ScrapyManager</span></tt> class renamed to <tt class="docutils literal"><span class="pre">scrapy.crawler.Crawler</span></tt></li>
<li><tt class="docutils literal"><span class="pre">scrapy.core.manager.scrapymanager</span></tt> singleton moved to <tt class="docutils literal"><span class="pre">scrapy.project.crawler</span></tt></li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">Moved module: <tt class="docutils literal"><span class="pre">scrapy.contrib.spidermanager</span></tt> to <tt class="docutils literal"><span class="pre">scrapy.spidermanager</span></tt></p>
</li>
<li><p class="first">Spider Manager singleton moved from <tt class="docutils literal"><span class="pre">scrapy.spider.spiders</span></tt> to the <tt class="docutils literal"><span class="pre">spiders`</span> <span class="pre">attribute</span> <span class="pre">of</span> <span class="pre">``scrapy.project.crawler</span></tt> singleton.</p>
</li>
<li><dl class="first docutils">
<dt>moved Stats Collector classes: (#204)</dt>
<dd><ul class="first last simple">
<li><tt class="docutils literal"><span class="pre">scrapy.stats.collector.StatsCollector</span></tt> to <tt class="docutils literal"><span class="pre">scrapy.statscol.StatsCollector</span></tt></li>
<li><tt class="docutils literal"><span class="pre">scrapy.stats.collector.SimpledbStatsCollector</span></tt> to <tt class="docutils literal"><span class="pre">scrapy.contrib.statscol.SimpledbStatsCollector</span></tt></li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">default per-command settings are now specified in the <tt class="docutils literal"><span class="pre">default_settings</span></tt> attribute of command object class (#201)</p>
</li>
<li><dl class="first docutils">
<dt>changed arguments of Item pipeline <tt class="docutils literal"><span class="pre">process_item()</span></tt> method from <tt class="docutils literal"><span class="pre">(spider,</span> <span class="pre">item)</span></tt> to <tt class="docutils literal"><span class="pre">(item,</span> <span class="pre">spider)</span></tt></dt>
<dd><ul class="first last simple">
<li>backwards compatibility kept (with deprecation warning)</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>moved <tt class="docutils literal"><span class="pre">scrapy.core.signals</span></tt> module to <tt class="docutils literal"><span class="pre">scrapy.signals</span></tt></dt>
<dd><ul class="first last simple">
<li>backwards compatibility kept (with deprecation warning)</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>moved <tt class="docutils literal"><span class="pre">scrapy.core.exceptions</span></tt> module to <tt class="docutils literal"><span class="pre">scrapy.exceptions</span></tt></dt>
<dd><ul class="first last simple">
<li>backwards compatibility kept (with deprecation warning)</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">added <tt class="docutils literal"><span class="pre">handles_request()</span></tt> class method to <tt class="docutils literal"><span class="pre">BaseSpider</span></tt></p>
</li>
<li><p class="first">dropped <tt class="docutils literal"><span class="pre">scrapy.log.exc()</span></tt> function (use <tt class="docutils literal"><span class="pre">scrapy.log.err()</span></tt> instead)</p>
</li>
<li><p class="first">dropped <tt class="docutils literal"><span class="pre">component</span></tt> argument of <tt class="docutils literal"><span class="pre">scrapy.log.msg()</span></tt> function</p>
</li>
<li><p class="first">dropped <tt class="docutils literal"><span class="pre">scrapy.log.log_level</span></tt> attribute</p>
</li>
<li><p class="first">Added <tt class="docutils literal"><span class="pre">from_settings()</span></tt> class methods to Spider Manager, and Item Pipeline Manager</p>
</li>
</ul>
</div>
<div class="section" id="id13">
<h5>Changes to settings<a class="headerlink" href="#id13" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>Added <tt class="docutils literal"><span class="pre">HTTPCACHE_IGNORE_SCHEMES</span></tt> setting to ignore certain schemes on !HttpCacheMiddleware (#225)</li>
<li>Added <tt class="docutils literal"><span class="pre">SPIDER_QUEUE_CLASS</span></tt> setting which defines the spider queue to use (#220)</li>
<li>Added <tt class="docutils literal"><span class="pre">KEEP_ALIVE</span></tt> setting (#220)</li>
<li>Removed <tt class="docutils literal"><span class="pre">SERVICE_QUEUE</span></tt> setting (#220)</li>
<li>Removed <tt class="docutils literal"><span class="pre">COMMANDS_SETTINGS_MODULE</span></tt> setting (#201)</li>
<li>Renamed <tt class="docutils literal"><span class="pre">REQUEST_HANDLERS</span></tt> to <tt class="docutils literal"><span class="pre">DOWNLOAD_HANDLERS</span></tt> and make download handlers classes (instead of functions)</li>
</ul>
</div>
</div>
<div class="section" id="id14">
<h4>0.9<a class="headerlink" href="#id14" title="永久链接至标题">¶</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<div class="section" id="id15">
<h5>New features and improvements<a class="headerlink" href="#id15" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>Added SMTP-AUTH support to scrapy.mail</li>
<li>New settings added: <tt class="docutils literal"><span class="pre">MAIL_USER</span></tt>, <tt class="docutils literal"><span class="pre">MAIL_PASS</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2065">r2065</a> | #149)</li>
<li>Added new scrapy-ctl view command - To view URL in the browser, as seen by Scrapy (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2039">r2039</a>)</li>
<li>Added web service for controlling Scrapy process (this also deprecates the web console. (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2053">r2053</a> | #167)</li>
<li>Support for running Scrapy as a service, for production systems (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1988">r1988</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2054">r2054</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2055">r2055</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2056">r2056</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2057">r2057</a> | #168)</li>
<li>Added wrapper induction library (documentation only available in source code for now). (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2011">r2011</a>)</li>
<li>Simplified and improved response encoding support (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1961">r1961</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1969">r1969</a>)</li>
<li>Added <tt class="docutils literal"><span class="pre">LOG_ENCODING</span></tt> setting (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1956">r1956</a>, documentation available)</li>
<li>Added <tt class="docutils literal"><span class="pre">RANDOMIZE_DOWNLOAD_DELAY</span></tt> setting (enabled by default) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1923">r1923</a>, doc available)</li>
<li><tt class="docutils literal"><span class="pre">MailSender</span></tt> is no longer IO-blocking (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1955">r1955</a> | #146)</li>
<li>Linkextractors and new Crawlspider now handle relative base tag urls (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1960">r1960</a> | #148)</li>
<li>Several improvements to Item Loaders and processors (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2022">r2022</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2023">r2023</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2024">r2024</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2025">r2025</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2026">r2026</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2027">r2027</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2028">r2028</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2029">r2029</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2030">r2030</a>)</li>
<li>Added support for adding variables to telnet console (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2047">r2047</a> | #165)</li>
<li>Support for requests without callbacks (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2050">r2050</a> | #166)</li>
</ul>
</div>
<div class="section" id="id16">
<h5>API changes<a class="headerlink" href="#id16" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>Change <tt class="docutils literal"><span class="pre">Spider.domain_name</span></tt> to <tt class="docutils literal"><span class="pre">Spider.name</span></tt> (SEP-012, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1975">r1975</a>)</li>
<li><tt class="docutils literal"><span class="pre">Response.encoding</span></tt> is now the detected encoding (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1961">r1961</a>)</li>
<li><tt class="docutils literal"><span class="pre">HttpErrorMiddleware</span></tt> now returns None or raises an exception (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2006">r2006</a> | #157)</li>
<li><tt class="docutils literal"><span class="pre">scrapy.command</span></tt> modules relocation (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2035">r2035</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2036">r2036</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2037">r2037</a>)</li>
<li>Added <tt class="docutils literal"><span class="pre">ExecutionQueue</span></tt> for feeding spiders to scrape (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2034">r2034</a>)</li>
<li>Removed <tt class="docutils literal"><span class="pre">ExecutionEngine</span></tt> singleton (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2039">r2039</a>)</li>
<li>Ported <tt class="docutils literal"><span class="pre">S3ImagesStore</span></tt> (images pipeline) to use boto and threads (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2033">r2033</a>)</li>
<li>Moved module: <tt class="docutils literal"><span class="pre">scrapy.management.telnet</span></tt> to <tt class="docutils literal"><span class="pre">scrapy.telnet</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2047">r2047</a>)</li>
</ul>
</div>
<div class="section" id="changes-to-default-settings">
<h5>Changes to default settings<a class="headerlink" href="#changes-to-default-settings" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>Changed default <tt class="docutils literal"><span class="pre">SCHEDULER_ORDER</span></tt> to <tt class="docutils literal"><span class="pre">DFO</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1939">r1939</a>)</li>
</ul>
</div>
</div>
<div class="section" id="id17">
<h4>0.8<a class="headerlink" href="#id17" title="永久链接至标题">¶</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<div class="section" id="new-features">
<h5>New features<a class="headerlink" href="#new-features" title="永久链接至标题">¶</a></h5>
<ul class="simple">
<li>Added DEFAULT_RESPONSE_ENCODING setting (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1809">r1809</a>)</li>
<li>Added <tt class="docutils literal"><span class="pre">dont_click</span></tt> argument to <tt class="docutils literal"><span class="pre">FormRequest.from_response()</span></tt> method (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1813">r1813</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1816">r1816</a>)</li>
<li>Added <tt class="docutils literal"><span class="pre">clickdata</span></tt> argument to <tt class="docutils literal"><span class="pre">FormRequest.from_response()</span></tt> method (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1802">r1802</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1803">r1803</a>)</li>
<li>Added support for HTTP proxies (<tt class="docutils literal"><span class="pre">HttpProxyMiddleware</span></tt>) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1781">r1781</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1785">r1785</a>)</li>
<li>Offiste spider middleware now logs messages when filtering out requests (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1841">r1841</a>)</li>
</ul>
</div>
<div class="section" id="backwards-incompatible-changes">
<h5>Backwards-incompatible changes<a class="headerlink" href="#backwards-incompatible-changes" title="永久链接至标题">¶</a></h5>
<ul>
<li><p class="first">Changed <tt class="docutils literal"><span class="pre">scrapy.utils.response.get_meta_refresh()</span></tt> signature (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1804">r1804</a>)</p>
</li>
<li><p class="first">Removed deprecated <tt class="docutils literal"><span class="pre">scrapy.item.ScrapedItem</span></tt> class - use <tt class="docutils literal"><span class="pre">scrapy.item.Item</span> <span class="pre">instead</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1838">r1838</a>)</p>
</li>
<li><p class="first">Removed deprecated <tt class="docutils literal"><span class="pre">scrapy.xpath</span></tt> module - use <tt class="docutils literal"><span class="pre">scrapy.selector</span></tt> instead. (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1836">r1836</a>)</p>
</li>
<li><p class="first">Removed deprecated <tt class="docutils literal"><span class="pre">core.signals.domain_open</span></tt> signal - use <tt class="docutils literal"><span class="pre">core.signals.domain_opened</span></tt> instead (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1822">r1822</a>)</p>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">log.msg()</span></tt> now receives a <tt class="docutils literal"><span class="pre">spider</span></tt> argument (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1822">r1822</a>)</dt>
<dd><ul class="first last simple">
<li>Old domain argument has been deprecated and will be removed in 0.9. For spiders, you should always use the <tt class="docutils literal"><span class="pre">spider</span></tt> argument and pass spider references. If you really want to pass a string, use the <tt class="docutils literal"><span class="pre">component</span></tt> argument instead.</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">Changed core signals <tt class="docutils literal"><span class="pre">domain_opened</span></tt>, <tt class="docutils literal"><span class="pre">domain_closed</span></tt>, <tt class="docutils literal"><span class="pre">domain_idle</span></tt></p>
</li>
<li><dl class="first docutils">
<dt>Changed Item pipeline to use spiders instead of domains</dt>
<dd><ul class="first last simple">
<li>The <tt class="docutils literal"><span class="pre">domain</span></tt> argument of  <tt class="docutils literal"><span class="pre">process_item()</span></tt> item pipeline method was changed to  <tt class="docutils literal"><span class="pre">spider</span></tt>, the new signature is: <tt class="docutils literal"><span class="pre">process_item(spider,</span> <span class="pre">item)</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1827">r1827</a> | #105)</li>
<li>To quickly port your code (to work with Scrapy 0.8) just use <tt class="docutils literal"><span class="pre">spider.domain_name</span></tt> where you previously used <tt class="docutils literal"><span class="pre">domain</span></tt>.</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Changed Stats API to use spiders instead of domains (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1849">r1849</a> | #113)</dt>
<dd><ul class="first last simple">
<li><tt class="docutils literal"><span class="pre">StatsCollector</span></tt> was changed to receive spider references (instead of domains) in its methods (<tt class="docutils literal"><span class="pre">set_value</span></tt>, <tt class="docutils literal"><span class="pre">inc_value</span></tt>, etc).</li>
<li>added <tt class="docutils literal"><span class="pre">StatsCollector.iter_spider_stats()</span></tt> method</li>
<li>removed <tt class="docutils literal"><span class="pre">StatsCollector.list_domains()</span></tt> method</li>
<li>Also, Stats signals were renamed and now pass around spider references (instead of domains). Here&#8217;s a summary of the changes:</li>
<li>To quickly port your code (to work with Scrapy 0.8) just use <tt class="docutils literal"><span class="pre">spider.domain_name</span></tt> where you previously used <tt class="docutils literal"><span class="pre">domain</span></tt>. <tt class="docutils literal"><span class="pre">spider_stats</span></tt> contains exactly the same data as <tt class="docutils literal"><span class="pre">domain_stats</span></tt>.</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">CloseDomain</span></tt> extension moved to <tt class="docutils literal"><span class="pre">scrapy.contrib.closespider.CloseSpider</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1833">r1833</a>)</dt>
<dd><ul class="first last">
<li><dl class="first docutils">
<dt>Its settings were also renamed:</dt>
<dd><ul class="first last simple">
<li><tt class="docutils literal"><span class="pre">CLOSEDOMAIN_TIMEOUT</span></tt> to <tt class="docutils literal"><span class="pre">CLOSESPIDER_TIMEOUT</span></tt></li>
<li><tt class="docutils literal"><span class="pre">CLOSEDOMAIN_ITEMCOUNT</span></tt> to <tt class="docutils literal"><span class="pre">CLOSESPIDER_ITEMCOUNT</span></tt></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">Removed deprecated <tt class="docutils literal"><span class="pre">SCRAPYSETTINGS_MODULE</span></tt> environment variable - use <tt class="docutils literal"><span class="pre">SCRAPY_SETTINGS_MODULE</span></tt> instead (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1840">r1840</a>)</p>
</li>
<li><p class="first">Renamed setting: <tt class="docutils literal"><span class="pre">REQUESTS_PER_DOMAIN</span></tt> to <tt class="docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_SPIDER</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1830">r1830</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1844">r1844</a>)</p>
</li>
<li><p class="first">Renamed setting: <tt class="docutils literal"><span class="pre">CONCURRENT_DOMAINS</span></tt> to <tt class="docutils literal"><span class="pre">CONCURRENT_SPIDERS</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1830">r1830</a>)</p>
</li>
<li><p class="first">Refactored HTTP Cache middleware</p>
</li>
<li><p class="first">HTTP Cache middleware has been heavilty refactored, retaining the same functionality except for the domain sectorization which was removed. (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1843">r1843</a> )</p>
</li>
<li><p class="first">Renamed exception: <tt class="docutils literal"><span class="pre">DontCloseDomain</span></tt> to <tt class="docutils literal"><span class="pre">DontCloseSpider</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1859">r1859</a> | #120)</p>
</li>
<li><p class="first">Renamed extension: <tt class="docutils literal"><span class="pre">DelayedCloseDomain</span></tt> to <tt class="docutils literal"><span class="pre">SpiderCloseDelay</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1861">r1861</a> | #121)</p>
</li>
<li><p class="first">Removed obsolete <tt class="docutils literal"><span class="pre">scrapy.utils.markup.remove_escape_chars</span></tt> function - use <tt class="docutils literal"><span class="pre">scrapy.utils.markup.replace_escape_chars</span></tt> instead (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1865">r1865</a>)</p>
</li>
</ul>
</div>
</div>
<div class="section" id="id18">
<h4>0.7<a class="headerlink" href="#id18" title="永久链接至标题">¶</a></h4>
<p>First release of Scrapy.</p>
</div>
</div>
<span id="document-contributing"></span><div class="section" id="contributing-to-scrapy">
<span id="topics-contributing"></span><h3>Contributing to Scrapy<a class="headerlink" href="#contributing-to-scrapy" title="永久链接至标题">¶</a></h3>
<p>There are many ways to contribute to Scrapy. Here are some of them:</p>
<ul class="simple">
<li>Blog about Scrapy. Tell the world how you&#8217;re using Scrapy. This will help
newcomers with more examples and the Scrapy project to increase its
visibility.</li>
<li>Report bugs and request features in the <a class="reference external" href="https://github.com/scrapy/scrapy/issues">issue tracker</a>, trying to follow
the guidelines detailed in <a class="reference internal" href="#reporting-bugs">Reporting bugs</a> below.</li>
<li>Submit patches for new functionality and/or bug fixes. Please read
<a class="reference internal" href="#writing-patches">Writing patches</a> and <a class="reference internal" href="#submitting-patches">Submitting patches</a> below for details on how to
write and submit a patch.</li>
<li>Join the <a class="reference external" href="http://groups.google.com/group/scrapy-users">scrapy-users</a> mailing list and share your ideas on how to
improve Scrapy. We&#8217;re always open to suggestions.</li>
</ul>
<div class="section" id="reporting-bugs">
<h4>Reporting bugs<a class="headerlink" href="#reporting-bugs" title="永久链接至标题">¶</a></h4>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">Please report security issues <strong>only</strong> to
<a class="reference external" href="mailto:scrapy-security&#37;&#52;&#48;googlegroups&#46;com">scrapy-security<span>&#64;</span>googlegroups<span>&#46;</span>com</a>. This is a private list only open to
trusted Scrapy developers, and its archives are not public.</p>
</div>
<p>Well-written bug reports are very helpful, so keep in mind the following
guidelines when reporting a new bug.</p>
<ul class="simple">
<li>check the <a class="reference internal" href="index.html#faq"><em>FAQ</em></a> first to see if your issue is addressed in a
well-known question</li>
<li>check the <a class="reference external" href="https://github.com/scrapy/scrapy/issues">open issues</a> to see if it has already been reported. If it has,
don&#8217;t dismiss the report but check the ticket history and comments, you may
find additional useful information to contribute.</li>
<li>search the <a class="reference external" href="http://groups.google.com/group/scrapy-users">scrapy-users</a> list to see if it has been discussed there, or
if you&#8217;re not sure if what you&#8217;re seeing is a bug. You can also ask in the
<cite>#scrapy</cite> IRC channel.</li>
<li>write complete, reproducible, specific bug reports. The smaller the test
case, the better. Remember that other developers won&#8217;t have your project to
reproduce the bug, so please include all relevant files required to reproduce
it.</li>
<li>include the output of <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">version</span> <span class="pre">-v</span></tt> so developers working on your bug
know exactly which version and platform it occurred on, which is often very
helpful for reproducing it, or knowing if it was already fixed.</li>
</ul>
</div>
<div class="section" id="writing-patches">
<h4>Writing patches<a class="headerlink" href="#writing-patches" title="永久链接至标题">¶</a></h4>
<p>The better written a patch is, the higher chance that it&#8217;ll get accepted and
the sooner that will be merged.</p>
<p>Well-written patches should:</p>
<ul class="simple">
<li>contain the minimum amount of code required for the specific change. Small
patches are easier to review and merge. So, if you&#8217;re doing more than one
change (or bug fix), please consider submitting one patch per change. Do not
collapse multiple changes into a single patch. For big changes consider using
a patch queue.</li>
<li>pass all unit-tests. See <a class="reference internal" href="#running-tests">Running tests</a> below.</li>
<li>include one (or more) test cases that check the bug fixed or the new
functionality added. See <a class="reference internal" href="#writing-tests">Writing tests</a> below.</li>
<li>if you&#8217;re adding or changing a public (documented) API, please include
the documentation changes in the same patch.  See <a class="reference internal" href="#documentation-policies">Documentation policies</a>
below.</li>
</ul>
</div>
<div class="section" id="submitting-patches">
<h4>Submitting patches<a class="headerlink" href="#submitting-patches" title="永久链接至标题">¶</a></h4>
<p>The best way to submit a patch is to issue a <a class="reference external" href="http://help.github.com/send-pull-requests/">pull request</a> on Github,
optionally creating a new issue first.</p>
<p>Remember to explain what was fixed or the new functionality (what it is, why
it&#8217;s needed, etc). The more info you include, the easier will be for core
developers to understand and accept your patch.</p>
<p>You can also discuss the new functionality (or bug fix) before creating the
patch, but it&#8217;s always good to have a patch ready to illustrate your arguments
and show that you have put some additional thought into the subject. A good
starting point is to send a pull request on Github. It can be simple enough to
illustrate your idea, and leave documentation/tests for later, after the idea
has been validated and proven useful. Alternatively, you can send an email to
<a class="reference external" href="http://groups.google.com/group/scrapy-users">scrapy-users</a> to discuss your idea first.</p>
<p>Finally, try to keep aesthetic changes (<span class="target" id="index-0"></span><a class="pep reference external" href="http://www.python.org/dev/peps/pep-0008"><strong>PEP 8</strong></a> compliance, unused imports
removal, etc) in separate commits than functional changes. This will make pull
requests easier to review and more likely to get merged.</p>
</div>
<div class="section" id="coding-style">
<h4>Coding style<a class="headerlink" href="#coding-style" title="永久链接至标题">¶</a></h4>
<p>Please follow these coding conventions when writing code for inclusion in
Scrapy:</p>
<ul class="simple">
<li>Unless otherwise specified, follow <span class="target" id="index-1"></span><a class="pep reference external" href="http://www.python.org/dev/peps/pep-0008"><strong>PEP 8</strong></a>.</li>
<li>It&#8217;s OK to use lines longer than 80 chars if it improves the code
readability.</li>
<li>Don&#8217;t put your name in the code you contribute. Our policy is to keep
the contributor&#8217;s name in the <a class="reference external" href="https://github.com/scrapy/scrapy/blob/master/AUTHORS">AUTHORS</a> file distributed with Scrapy.</li>
</ul>
</div>
<div class="section" id="scrapy-contrib">
<h4>Scrapy Contrib<a class="headerlink" href="#scrapy-contrib" title="永久链接至标题">¶</a></h4>
<p>Scrapy contrib shares a similar rationale as Django contrib, which is explained
in <a class="reference external" href="http://jacobian.org/writing/what-is-django-contrib/">this post</a>. If you
are working on a new functionality, please follow that rationale to decide
whether it should be a Scrapy contrib. If unsure, you can ask in
<a class="reference external" href="http://groups.google.com/group/scrapy-users">scrapy-users</a>.</p>
</div>
<div class="section" id="documentation-policies">
<h4>Documentation policies<a class="headerlink" href="#documentation-policies" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li><strong>Don&#8217;t</strong> use docstrings for documenting classes, or methods which are
already documented in the official (sphinx) documentation. For example, the
<tt class="xref py py-meth docutils literal"><span class="pre">ItemLoader.add_value()</span></tt> method should be documented in the sphinx
documentation, not its docstring.</li>
<li><strong>Do</strong> use docstrings for documenting functions not present in the official
(sphinx) documentation, such as functions from <tt class="docutils literal"><span class="pre">scrapy.utils</span></tt> package and
its sub-modules.</li>
</ul>
</div>
<div class="section" id="tests">
<h4>Tests<a class="headerlink" href="#tests" title="永久链接至标题">¶</a></h4>
<p>Tests are implemented using the <a class="reference external" href="http://twistedmatrix.com/documents/current/core/development/policy/test-standard.html">Twisted unit-testing framework</a> called
<tt class="docutils literal"><span class="pre">trial</span></tt>.</p>
<div class="section" id="running-tests">
<h5>Running tests<a class="headerlink" href="#running-tests" title="永久链接至标题">¶</a></h5>
<p>To run all tests go to the root directory of Scrapy source code and run:</p>
<blockquote>
<div><p><tt class="docutils literal"><span class="pre">bin/runtests.sh</span></tt> (on unix)</p>
<p><tt class="docutils literal"><span class="pre">bin\runtests.bat</span></tt> (on windows)</p>
</div></blockquote>
<p>To run a specific test (say <tt class="docutils literal"><span class="pre">scrapy.tests.test_contrib_loader</span></tt>) use:</p>
<blockquote>
<div><p><tt class="docutils literal"><span class="pre">bin/runtests.sh</span> <span class="pre">scrapy.tests.test_contrib_loader</span></tt> (on unix)</p>
<p><tt class="docutils literal"><span class="pre">bin\runtests.bat</span> <span class="pre">scrapy.tests.test_contrib_loader</span></tt> (on windows)</p>
</div></blockquote>
</div>
<div class="section" id="writing-tests">
<h5>Writing tests<a class="headerlink" href="#writing-tests" title="永久链接至标题">¶</a></h5>
<p>All functionality (including new features and bug fixes) must include a test
case to check that it works as expected, so please include tests for your
patches if you want them to get accepted sooner.</p>
<p>Scrapy uses unit-tests, which are located in the <tt class="docutils literal"><span class="pre">scrapy.tests</span></tt> package
(<a class="reference external" href="https://github.com/scrapy/scrapy/tree/master/scrapy/tests">scrapy/tests</a> directory). Their module name typically resembles the full
path of the module they&#8217;re testing. For example, the item loaders code is in:</p>
<div class="highlight-none"><div class="highlight"><pre>scrapy.contrib.loader
</pre></div>
</div>
<p>And their unit-tests are in:</p>
<div class="highlight-none"><div class="highlight"><pre>scrapy.tests.test_contrib_loader
</pre></div>
</div>
</div>
</div>
</div>
<span id="document-versioning"></span><div class="section" id="versioning-and-api-stability">
<span id="versioning"></span><h3>Versioning and API Stability<a class="headerlink" href="#versioning-and-api-stability" title="永久链接至标题">¶</a></h3>
<div class="section" id="id1">
<h4>Versioning<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h4>
<p>Scrapy uses the <a class="reference external" href="http://en.wikipedia.org/wiki/Software_versioning#Odd-numbered_versions_for_development_releases">odd-numbered versions for development releases</a>.</p>
<p>There are 3 numbers in a Scrapy version: <em>A.B.C</em></p>
<ul class="simple">
<li><em>A</em> is the major version. This will rarely change and will signify very
large changes. So far, only zero is available for <em>A</em> as Scrapy hasn&#8217;t yet
reached 1.0.</li>
<li><em>B</em> is the release number. This will include many changes including features
and things that possibly break backwards compatibility. Even Bs will be
stable branches, and odd Bs will be development.</li>
<li><em>C</em> is the bugfix release number.</li>
</ul>
<p>For example:</p>
<ul class="simple">
<li><em>0.14.1</em> is the first bugfix release of the <em>0.14</em> series (safe to use in
production)</li>
</ul>
</div>
<div class="section" id="api-stability">
<h4>API Stability<a class="headerlink" href="#api-stability" title="永久链接至标题">¶</a></h4>
<p>API stability is one of Scrapy major goals for the <em>1.0</em> release, which doesn&#8217;t
have a due date scheduled yet.</p>
<p>Methods or functions that start with a single dash (<tt class="docutils literal"><span class="pre">_</span></tt>) are private and
should never be relied as stable. Besides those, the plan is to stabilize and
document the entire API, as we approach the 1.0 release.</p>
<p>Also, keep in mind that stable doesn&#8217;t mean complete: stable APIs could grow
new methods or functionality but the existing methods should keep working the
same way.</p>
</div>
</div>
<span id="document-experimental/index"></span><div class="section" id="experimental">
<span id="id1"></span><h3>试验阶段特性<a class="headerlink" href="#experimental" title="永久链接至标题">¶</a></h3>
<p>这部分介绍一些正处于试验阶段的Scrapy特性，
这些特性所涉及到的函数接口等还不够稳定，
但会在以后的发布版中趋于完善。所以在使用这些特性过程中需更谨慎，
并且最好订阅我们的 <a class="reference external" href="http://scrapy.org/community/">邮件列表</a> 以便接收任何有关特性改变的通知。</p>
<p>虽然这些特性不会频繁的被修改，但是这部分文档仍有可能是过时的、
不完整的或是与已经稳定的特性文档重复。所以你需要自行承担使用风险。</p>
<div class="admonition warning">
<p class="first admonition-title">警告</p>
<p class="last">本部分文档一直处于修改中。请自行承担使用风险。</p>
</div>
<div class="section" id="id3">
<h4>使用外部库插入命令<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h4>
<p>你可以使用外部库通过增加 <cite>scrapy.commands</cite> 部分到 <cite>setup.py</cite> 的entry_points中来插入Scrapy命令。</p>
<p>增加 <cite>my_command</cite> 命令的例子:</p>
<div class="highlight-none"><div class="highlight"><pre>from setuptools import setup, find_packages

setup(name=&#39;scrapy-mymodule&#39;,
  entry_points={
    &#39;scrapy.commands&#39;: [
      &#39;my_command=my_scrapy_module.commands:MyCommand&#39;,
    ],
  },
 )
</pre></div>
</div>
</div>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-news"><em>Release notes</em></a></dt>
<dd>了解最近的Scrapy版本的修改。</dd>
<dt><a class="reference internal" href="index.html#document-contributing"><em>Contributing to Scrapy</em></a></dt>
<dd>了解如何为Scrapy项目做出贡献。</dd>
<dt><a class="reference internal" href="index.html#document-versioning"><em>Versioning and API Stability</em></a></dt>
<dd>了解Scrapy如何命名版本以及API的稳定性。</dd>
<dt><a class="reference internal" href="index.html#document-experimental/index"><em>试验阶段特性</em></a></dt>
<dd>了解最新的特性</dd>
</dl>
</div>
</div>


    
        <h2>
            讨论
            <a class="headerlink" href="#discuss" title="永久链接至标题">¶</a>
        </h2>

        <div id="disqus_thread"></div>
    

          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; 版权所有 2008-2014, written by Scrapy developers, translated by Summer&amp;Friends.
      最后更新于 Jun 27, 2014.
    </p>
  </div>

  <a href="https://github.com/snide/sphinx_rtd_theme">Sphinx theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>
</footer>
        </div>
      </div>

    </section>

  </div>
  

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Read the Docs</span>
      v: 0.24
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      <dl>
        <dt>Versions</dt>
        
          <dd><a href="/zh_CN/master/">master</a></dd>
        
          <dd><a href="/zh_CN/latest/">latest</a></dd>
        
          <dd><a href="/zh_CN/0.24/">0.24</a></dd>
        
          <dd><a href="/zh_CN/0.22/">0.22</a></dd>
        
      </dl>
      <dl>
        <dt>Downloads</dt>
        
      </dl>
      <dl>
        <dt>On Read the Docs</dt>
          <dd>
            <a href="//readthedocs.org/projects/scrapy-chs/?fromdocs=scrapy-chs">Project Home</a>
          </dd>
          <dd>
            <a href="//readthedocs.org/builds/scrapy-chs/?fromdocs=scrapy-chs">Builds</a>
          </dd>
      </dl>
      <hr/>
      Free document hosting provided by <a href="http://www.readthedocs.org">Read the Docs</a>.

    </div>
  </div>



  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.24.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="_static/translations.js"></script>

  

  
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
  
     
    
    <script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'scrapychs'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
     var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
     dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
     (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();

    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
         (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
           m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
             })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-50189694-1', 'readthedocs.org');
      ga('send', 'pageview');

    </script>
    


</body>
</html>